{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp weights\n",
    "#%nbdev_default_export weights\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utilities.ipynb_docgen import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights\n",
    "> Load weighted data, combine with photon data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the full-sky, catalog analysis model to evaluate the predicted flux from a source of interest with respect to the\n",
    "background, the combined fluxes from all other sources. We choose the following binning:\n",
    "\n",
    "* energy:  4/decade from 100 MeV to 1 TeV \n",
    "* event type: Front and Back  \n",
    "* Angular position: HEALPix, currently nside=64, for 1 degree-square pixels,\n",
    "\n",
    "We plan to increase nside to 128 in the near future.\n",
    "\n",
    "A procedure, currently only for pointlike, packs this table with the source name and position, into a pickled dict.\n",
    "\n",
    "This table is used with the data, as a simple lookup: A weight is assigned to each photon according to which energy, event type or HEALPix pixel it lands in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys,  pickle, healpy\n",
    "import numpy as np\n",
    "from wtlike.config import *\n",
    "#from wtlike.photon_data import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_weights(config, source):\n",
    "    \"\"\"\n",
    "    Check that weights for the source are available: if so, return the weight file name\n",
    "    \n",
    "    - source -- A PointSource object with information on source location\n",
    "    \n",
    "    Returns the filepath to the file if successful, otherwise, print a message abount available files\n",
    "    \"\"\"\n",
    "    weight_files = config.wtlike_data/'weight_files' \n",
    "    assert weight_files.is_dir(), f'Expect {weight_files} to be a directory'\n",
    "    weight_file = weight_files/ (source.filename+'_weights.pkl')\n",
    "    if not weight_file.exists():\n",
    "        available = np.array(list(map(lambda p: p.name[:p.name.find('_weights')], \n",
    "                          weight_files.glob('*_weights.pkl'))))\n",
    "        print(f'{source} not found in list of weight files at\\n\\t {weight_files}.\\n Available:\\n{available}',\n",
    "             file = sys.stderr)\n",
    "        return None\n",
    "    return weight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"check_weights\" class=\"doc_header\"><code>check_weights</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>check_weights</code>(**`config`**, **`source`**)\n",
       "\n",
       "Check that weights for the source are available: if so, return the weight file name\n",
       "\n",
       "- source -- A PointSource object with information on source location\n",
       "\n",
       "Returns the filepath to the file if successful, otherwise, print a message abount available files"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source \"test\" at: (l,b)=(0.000,0.000) not found in list of weight files at\n",
      "\t /home/burnett/wtlike_data/weight_files.\n",
      " Available:\n",
      "['3C454.3' '3C_279' '4FGL_J1257.0-6339' 'B2_1520p31' 'BL_Lac' 'Eta_car'\n",
      " 'Geminga' 'PSR_B1259-63' 'PSR_J0633p1746' 'PSR_J0835-4510'\n",
      " 'PSR_J1302-6350' 'PSR_J1836p5925' 'PSR_J1909-3744' 'PSR_J1913p1011'\n",
      " 'PSR_J2022p3842' 'PSR_J2032p4127']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source \"Geminga\" at: (l,b)=(195.134,4.266) Should be found: file at /home/burnett/wtlike_data/weight_files/Geminga_weights.pkl \n"
     ]
    }
   ],
   "source": [
    "show_doc(check_weights)\n",
    "config = Config()\n",
    "if config.valid:\n",
    "    print('Check not found')\n",
    "    test_source = PointSource('test', (0,0))\n",
    "    check_weights(config, test_source)\n",
    "    good_source = PointSource('Geminga')\n",
    "    print(f'{good_source} Should be found: file at {check_weights(config, good_source)} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _load_weights(config, filename, ):\n",
    "    \"\"\"Load the weight informaton\n",
    "\n",
    "    filename: pickled dict with map info\n",
    "\n",
    "    \"\"\"\n",
    "    # load a pickle containing weights, generated by pointlike\n",
    "    assert os.path.exists(filename),f'File {filename} not found.'\n",
    "    with open(filename, 'rb') as file:\n",
    "        wtd = pickle.load(file, encoding='latin1')\n",
    "    assert type(wtd)==dict, 'Expect a dictionary'\n",
    "    test_elements = 'energy_bins pixels weights nside model_name radius order roi_name'.split()\n",
    "    assert np.all([x in wtd.keys() for x in test_elements]),f'Dict missing one of the keys {test_elements}'\n",
    "    if config.verbose>0:\n",
    "        print(f'Load weights from file {os.path.realpath(filename)}')\n",
    "        pos = wtd['source_lb']\n",
    "        print(f'\\tFound: {wtd[\"source_name\"]} at ({pos[0]:.2f}, {pos[1]:.2f})')\n",
    "    # extract pixel ids and nside used\n",
    "    wt_pix   = wtd['pixels']\n",
    "    nside_wt = wtd['nside']\n",
    "\n",
    "    # merge the weights into a table, with default nans\n",
    "    # indexing is band id rows by weight pixel columns\n",
    "    # append one empty column for photons not in a weight pixel\n",
    "    # calculated weights are in a dict with band id keys\n",
    "    wts = np.full((32, len(wt_pix)+1), np.nan, dtype=np.float32)\n",
    "    weight_dict = wtd['weights']\n",
    "    for k in weight_dict.keys():\n",
    "        t = weight_dict[k]\n",
    "        if len(t.shape)==2:\n",
    "            t = t.T[0] #???\n",
    "        wts[k,:-1] = t\n",
    "    return wts , wt_pix , nside_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _add_weights(config, wts, wt_pix, nside_wt, photon_data):\n",
    "    \"\"\" get the photon pixel ids, convert to NEST (if not already) and right shift them\n",
    "        add 'weight', remove 'band', 'pixel'\n",
    "    \"\"\"\n",
    "    if not config.nest:\n",
    "        # data are RING\n",
    "        photon_pix = healpy.ring2nest(config.nside, photon_data.pixel.values)\n",
    "    else:\n",
    "        photon_pix = photon_data.pixel.values\n",
    "    to_shift = 2*int(np.log2(config.nside/nside_wt));\n",
    "    shifted_pix =   np.right_shift(photon_pix, to_shift)\n",
    "    bad = np.logical_not(np.isin(shifted_pix, wt_pix))\n",
    "    if config.verbose>0 & sum(bad)>0:\n",
    "        print(f'\\tApplying weights: {sum(bad)} / {len(bad)} photon pixels are outside weight region')\n",
    "    if sum(bad)==len(bad):\n",
    "        a = np.array(healpy.pix2ang(nside_wt, wt_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "        b = np.array(healpy.pix2ang(nside_wt, shifted_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "\n",
    "        raise Exception(f'There was no overlap of the photon data at {b} and the weights at {a}')\n",
    "    shifted_pix[bad] = 12*nside_wt**2 # set index to be beyond pixel indices\n",
    "\n",
    "    # find indices with search and add a \"weights\" column\n",
    "    # (expect that wt_pix are NEST ordering and sorted)\n",
    "    weight_index = np.searchsorted(wt_pix,shifted_pix)\n",
    "    band_index = np.fmin(31, photon_data.band.values) #all above 1 TeV into last bin\n",
    "\n",
    "    # final grand lookup -- isn't numpy wonderful!\n",
    "    photon_data.loc[:,'weight'] = wts[tuple([band_index, weight_index])]\n",
    "    \n",
    "    # don't need these columns now (add flag to config to control??)\n",
    "    photon_data.drop(['band', 'pixel'], axis=1)\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        print(f'\\t{sum(np.isnan(photon_data.weight.values))} events without weight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def add_weights(config,  photon_data, source, nbins=50):\n",
    "    \"\"\" add weights for the source to the photon data\n",
    "    \n",
    "    - photon_data -- DataFrame with photon data\n",
    "    \n",
    "    - source -- `PointSource` object\n",
    "    \n",
    "    Return the weight value histogram\n",
    "    \"\"\"\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    if weight_file is None:\n",
    "        raise Exception(f'Weight file not found for {source}')\n",
    " \n",
    "    wts, wt_pix, nside_wt = _load_weights(config, weight_file)\n",
    "    _add_weights(config, wts, wt_pix, nside_wt, photon_data)\n",
    "\n",
    "    return np.histogram(photon_data.weight.values, np.linspace(0,1,nbins+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_weight_hist(config,  source, nbins=50, key=''):\n",
    "    \"\"\" return a weight distribution\n",
    "        \n",
    "    - photon_data -- DataFrame with photon data    \n",
    "    - source -- `PointSource` object\n",
    "    \n",
    "    Uses `add_weights`.\n",
    "    \"\"\"\n",
    "    def doit(nbins):\n",
    "        weight_file =  check_weights(config,  source)\n",
    "        if weight_file is None:\n",
    "            raise Exception(f'Weight file not found for {source}')\n",
    "        photon_data = get_photon_data(config,  source )\n",
    "        return add_weights(config, photon_data, source, nbins=nbins)\n",
    "    \n",
    "    key = f'weight_hist_{source.name}' if key=='' else key\n",
    "    description = f'Weight histogram for {source.name}' if config.verbose>1 else ''\n",
    "    return config.cache(key, doit, nbins, description=description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config(wtlike_data='~/wtlike_data')\n",
    "# if config.valid:\n",
    "#     source = PointSource('Geminga')\n",
    "#     h = get_weight_hist(config, source, key='')\n",
    "#     print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %nbdev_collapse_input\n",
    "# config = Config() \n",
    "\n",
    "# if config.valid:\n",
    "#     source = PointSource('Geminga')\n",
    "#     print(f'Loading photon data for source {source.name} to test adding weights')\n",
    "#     photon_data = get_photon_data(config,  source )\n",
    "#     h = add_weights(config, photon_data, source)\n",
    "#     print(f'Head of modified photon_data\\n {photon_data.head()}')\n",
    "#     plt.rc('font',size=14)\n",
    "#     fig, ax =plt.subplots(figsize=(4,3))\n",
    "#     n = len(h)\n",
    "#     ax.step( np.linspace(0,1, n+1) , np.concatenate([[h[0]], h])); \n",
    "#     ax.grid();\n",
    "#     ax.set(xlabel='weight', xlim=(0,1), ylim=(0,None))\n",
    "#     #plt.hist(photon_data.weight.values, np.linspace(0,1,51), histtype='stepfilled', lw=2)\n",
    "#     plt.title(f'Weights for source {source.name}')\n",
    "# else:\n",
    "#     print('Not testing since no files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90-main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sun May 16 05:39:57 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
