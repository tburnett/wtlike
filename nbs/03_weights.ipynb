{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp weights\n",
    "#%nbdev_default_export weights\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utilities.ipynb_docgen import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights\n",
    "> Load weighted data, combine with photon data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the full-sky catalog analysis model to evaluate the predicted flux from a source of interest with respect to the\n",
    "background, the combined fluxes from all other sources. We choose the following binning:\n",
    "\n",
    "* energy:  4/decade from 100 MeV to 1 TeV \n",
    "* event type: Front and Back  \n",
    "* Angular position: HEALPix, currently nside=64, for 1 degree-square pixels,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointlike generation \n",
    "A procedure, currently only for pointlike, [see source_weighsts](https://github.com/tburnett/pointlike/blob/master/python/uw/like2/source_weights.py) packs this table with the source name and position, into a pickled dict.\n",
    "\n",
    "#### Original format -- fixed 1-degree pixel\n",
    "```\n",
    "        outdict = dict(\n",
    "            model_name = '/'.join(os.getcwd().split('/')[-2:]),\n",
    "            radius=self.radius,\n",
    "            nside=self.nside,\n",
    "            order='NEST',\n",
    "            energy_bins=self.energy_bins,\n",
    "            source_name= self.source.name,\n",
    "            source_lb=galactic(self.source.skydir),\n",
    "            roi_lb  = galactic(self.roi.roi_dir),\n",
    "            roi_name=self.roi.name,\n",
    "            pixels= pixels,\n",
    "            weights = weights,\n",
    "        )\n",
    "\n",
    "```\n",
    "#### New format -- varying pixel size depending on PSF width\n",
    "\n",
    "\n",
    "This is unpacked by `load_weights`\n",
    "\n",
    "This table is used with the data, as a simple lookup: A weight is assigned to each photon according to which energy, event type or HEALPix pixel it lands in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for variations from neighboring sources\n",
    "\n",
    "Consider the case where sources $S_1$ and $S_2$ have overlapping pixels. For a given pixel the corresponding weights are\n",
    "$w_1$ and $w_2$, and we investigate the effect on $S_1$ from a fractional variation $\\alpha_2 \\ne 0$ of $S_2$, such that\n",
    "its flux for that pixel, $s_2$, becomes $(1+\\alpha )\\ s_2$. With the background $b$, the flux of all\n",
    "sources besides $S_1$ and $S_2$, we have for the $S_1$ weight,\n",
    "$$ w_1 = \\frac{s_1}{s_1+s_2+b}\\ \\ ,$$ and similarly for $S_2$.\n",
    "\n",
    "Replacing $s_2$ with $(1+\\alpha ) s_2$, we have for the modified weight $w_1'$ that we should use for  $S_1$,\n",
    "$$w'_1 = \\frac{w_1}{1+\\alpha_2\\ w_2}\\ \\ .   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys,  pickle, healpy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from wtlike.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_weights(config, source):\n",
    "    \"\"\"\n",
    "    Check that weights for the source are available: if so, return the weight file name\n",
    "    \n",
    "    - source -- A PointSource object with information on source location\n",
    "    \n",
    "    Returns the filepath to the file if successful, otherwise, print a message abount available files\n",
    "    \"\"\"\n",
    "    weight_files = config.wtlike_data/'weight_files' \n",
    "    assert weight_files.is_dir(), f'Expect {weight_files} to be a directory'\n",
    "    weight_file = weight_files/ (source.filename+'_weights.pkl')\n",
    "    if not weight_file.exists():\n",
    "        available = np.array(list(map(lambda p: p.name[:p.name.find('_weights')], \n",
    "                          weight_files.glob('*_weights.pkl'))))\n",
    "        print(f'{source} not found in list of weight files at\\n\\t {weight_files}.\\n Available:\\n{available}',\n",
    "             file = sys.stderr)\n",
    "        return None\n",
    "    return weight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"check_weights\" class=\"doc_header\"><code>check_weights</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>check_weights</code>(**`config`**, **`source`**)\n",
       "\n",
       "Check that weights for the source are available: if so, return the weight file name\n",
       "\n",
       "- source -- A PointSource object with information on source location\n",
       "\n",
       "Returns the filepath to the file if successful, otherwise, print a message abount available files"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source \"test\" at: (l,b)=(0.000,0.000) not found in list of weight files at\n",
      "\t /home/burnett/wtlike_data/weight_files.\n",
      " Available:\n",
      "['3C454.3' '3C_273' '3C_279' '4FGL_J1257.0-6339' 'B2_1520p31' 'BL_Lac'\n",
      " 'Eta_car' 'Geminga' 'HESS_J1303-631' 'J1257_ring' 'J1257_test2'\n",
      " 'J1257_test' 'J1257' 'PSR_B1259-63' 'PSR_J0633p1746' 'PSR_J0835-4510'\n",
      " 'PSR_J1302-6350' 'PSR_J1836p5925' 'PSR_J1909-3744' 'PSR_J1913p1011'\n",
      " 'PSR_J2022p3842' 'PSR_J2032p4127']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source \"Geminga\" at: (l,b)=(195.134,4.266) Should be found: file at /home/burnett/wtlike_data/weight_files/Geminga_weights.pkl \n"
     ]
    }
   ],
   "source": [
    "show_doc(check_weights)\n",
    "config = Config()\n",
    "if config.valid:\n",
    "    print('Check not found')\n",
    "    test_source = PointSource('test', (0,0))\n",
    "    check_weights(config, test_source)\n",
    "    good_source = PointSource('Geminga')\n",
    "    print(f'{good_source} Should be found: file at {check_weights(config, good_source)} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_weights(config, filename, ):\n",
    "    \"\"\"Load the weight informaton\n",
    "\n",
    "    filename: pickled dict with map info\n",
    "\n",
    "    \"\"\"\n",
    "    # load a pickle containing weights, generated by pointlike\n",
    "    assert os.path.exists(filename),f'File {filename} not found.'\n",
    "    with open(filename, 'rb') as file:\n",
    "        wtd = pickle.load(file, encoding='latin1')\n",
    "    assert type(wtd)==dict, 'Expect a dictionary'\n",
    "    test_elements = 'energy_bins pixels weights nside model_name radius order roi_name'.split()\n",
    "    assert np.all([x in wtd.keys() for x in test_elements]),f'Dict missing one of the keys {test_elements}'\n",
    "    if config.verbose>0:\n",
    "        print(f'Load weights from file {os.path.realpath(filename)}')\n",
    "        pos = wtd['source_lb']\n",
    "        print(f'\\tFound: {wtd[\"source_name\"]} at ({pos[0]:.2f}, {pos[1]:.2f})')\n",
    "    # extract pixel ids and nside used\n",
    "    wt_pix   = wtd['pixels']\n",
    "    nside_wt = wtd['nside']\n",
    "\n",
    "    # merge the weights into a table, with default nans\n",
    "    # indexing is band id rows by weight pixel columns\n",
    "    # append one empty column for photons not in a weight pixel\n",
    "    # calculated weights are in a dict with band id keys\n",
    "    wts = np.full((32, len(wt_pix)+1), np.nan, dtype=np.float32)\n",
    "    weight_dict = wtd['weights']\n",
    "    for k in weight_dict.keys():\n",
    "        t = weight_dict[k]\n",
    "        if len(t.shape)==2:\n",
    "            t = t.T[0] #???\n",
    "        wts[k,:-1] = t\n",
    "    return wts , wt_pix , nside_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _add_weights(config, wts, wt_pix, nside_wt, photon_data):\n",
    "    \"\"\" get the photon pixel ids, convert to NEST (if not already) and right shift them\n",
    "        add 'weight', remove 'band', 'pixel'\n",
    "    \"\"\"\n",
    "    if not config.nest:\n",
    "        # data are RING\n",
    "        photon_pix = healpy.ring2nest(config.nside, photon_data.pixel.values)\n",
    "    else:\n",
    "        photon_pix = photon_data.pixel.values\n",
    "    to_shift = 2*int(np.log2(config.nside/nside_wt));\n",
    "    shifted_pix =   np.right_shift(photon_pix, to_shift)\n",
    "    bad = np.logical_not(np.isin(shifted_pix, wt_pix))\n",
    "    if config.verbose>0 & sum(bad)>0:\n",
    "        print(f'\\tApplying weights: {sum(bad)} / {len(bad)} photon pixels are outside weight region')\n",
    "    if sum(bad)==len(bad):\n",
    "        a = np.array(healpy.pix2ang(nside_wt, wt_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "        b = np.array(healpy.pix2ang(nside_wt, shifted_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "\n",
    "        raise Exception(f'There was no overlap of the photon data at {b} and the weights at {a}')\n",
    "    shifted_pix[bad] = 12*nside_wt**2 # set index to be beyond pixel indices\n",
    "\n",
    "    # find indices with search and add a \"weights\" column\n",
    "    # (expect that wt_pix are NEST ordering and sorted)\n",
    "    weight_index = np.searchsorted(wt_pix,shifted_pix)\n",
    "    band_index = np.fmin(31, photon_data.band.values) #all above 1 TeV into last bin\n",
    "\n",
    "    # final grand lookup -- isn't numpy wonderful!\n",
    "    photon_data.loc[:,'weight'] = self.wts[tuple([band_index, weight_index])]\n",
    "    \n",
    "    # don't need these columns now (add flag to config to control??)\n",
    "#     photon_data.drop(['band', 'pixel'], axis=1)\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        print(f'\\t{sum(np.isnan(photon_data.weight.values))} events without weight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def add_weights(config,  photon_data, source): # nbins=50):\n",
    "    \"\"\" add weights for the source to the photon data\n",
    "    \n",
    "    - photon_data -- DataFrame with photon data\n",
    "    \n",
    "    - source -- `PointSource` object\n",
    "    \n",
    "    \"\"\"\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    if weight_file is None:\n",
    "        raise Exception(f'Weight file not found for {source}')\n",
    " \n",
    "    ## NEW\n",
    "    wtman = WeightMan(config, filename=weight_file)\n",
    "    photon_data = wtman.add_weights(photon_data)\n",
    "    \n",
    "    ## OLD\n",
    "#     wts, wt_pix, nside_wt = load_weights(config, weight_file)\n",
    "#     _add_weights(config, wts, wt_pix, nside_wt, photon_data)\n",
    "\n",
    "    #return np.histogram(photon_data.weight.values, np.linspace(0,1,nbins+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class WeightMan(dict):\n",
    "    \"\"\" Weight Management\n",
    "    \n",
    "    * Load weight tables\n",
    "    * Assign weights to photons\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config,  source=None, filename=None,):\n",
    "        \"\"\"\n",
    "        TODO: find filename given source\n",
    "        \"\"\"\n",
    "        # load a pickle containing weights, generated by pointlike\n",
    "        assert source is not None or filename is not None, 'Expect source or filename'\n",
    "        wtpath =Path(config.wtlike_data)/'weight_files'\n",
    "        assert wtpath.is_dir(), f' {wtpath} not an existing file path' \n",
    "        assert (wtpath/filename).is_file(),f'File {filename} not found at {wtpath}'\n",
    "        with open(wtpath/filename, 'rb') as file:\n",
    "            wtd = pickle.load(file, encoding='latin1')\n",
    "        assert type(wtd)==dict, 'Expect a dictionary'\n",
    "        self.update(wtd)\n",
    "        self.__dict__.update(wtd)\n",
    "        self.filename=filename\n",
    "        self.config = config\n",
    "#         pos = self['source_lb']\n",
    "#         print(f'\\tSource is {self[\"source_name\"]} at ({pos[0]:.2f}, {pos[1]:.2f})')\n",
    "        \n",
    "        # check format--old has pixels, weights at tome\n",
    "        if hasattr(self, 'nside'):\n",
    "            self.format=0\n",
    "            if config.verbose>0:\n",
    "                print(f'WeightMan: file \"{self.filename}\": old format, nside={self.nside}')\n",
    "            \n",
    "            test_elements = 'energy_bins pixels weights nside model_name radius order roi_name'.split()\n",
    "            assert np.all([x in wtd.keys() for x in test_elements]),f'Dict missing one of the keys {test_elements}'\n",
    "            if config.verbose>0:\n",
    "                print(f'Load weights from file {os.path.realpath(filename)}')\n",
    "                pos = self['source_lb']\n",
    "                print(f'\\tFound: {self[\"source_name\"]} at ({pos[0]:.2f}, {pos[1]:.2f})')\n",
    "            # extract pixel ids and nside used\n",
    "            self.wt_pix   = self['pixels']\n",
    "            self.nside_wt = self['nside']\n",
    "\n",
    "            # merge the weights into a table, with default nans\n",
    "            # indexing is band id rows by weight pixel columns\n",
    "            # append one empty column for photons not in a weight pixel\n",
    "            # calculated weights are in a dict with band id keys\n",
    "            self.wts = np.full((32, len(self.wt_pix)+1), np.nan, dtype=np.float32)\n",
    "            weight_dict = self['weights']\n",
    "            for k in weight_dict.keys():\n",
    "                t = weight_dict[k]\n",
    "                if len(t.shape)==2:\n",
    "                    t = t.T[0] #???\n",
    "                self.wts[k,:-1] = t\n",
    "            \n",
    "        else:\n",
    "            self.format=1\n",
    "            wtdict = self.wt_dict\n",
    "            nsides = [v['nside'] for v in wtdict.values() ];\n",
    "            if config.verbose>1:\n",
    "                print(f'WeightMan: file \"{self.filename}\": new format, {len(nsides)} bamds'\\\n",
    "                      f' with nsides {nsides[0]} to {nsides[-1]}')\n",
    "     \n",
    "    def _old_format(self, photons):\n",
    "        if not self.config.nest:\n",
    "            # data are RING\n",
    "            photon_pix = healpy.ring2nest(config.nside, photons.pixel.values)\n",
    "        else:\n",
    "            photon_pix = photons.pixel.values\n",
    "        nside = self.nside_wt\n",
    "        to_shift = 2*int(np.log2(self.config.nside//self.nside_wt));\n",
    "        shifted_pix =   np.right_shift(photon_pix, to_shift)\n",
    "        bad = np.logical_not(np.isin(shifted_pix, self.wt_pix))\n",
    "        if self.config.verbose>0 & sum(bad)>0:\n",
    "            print(f'\\tApplying weights: {sum(bad)} / {len(bad)} photon pixels are outside weight region')\n",
    "        if sum(bad)==len(bad):\n",
    "            a = np.array(healpy.pix2ang(nside, self.wt_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "            b = np.array(healpy.pix2ang(nside, shifted_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "\n",
    "            raise Exception(f'There was no overlap of the photon data at {b} and the weights at {a}')\n",
    "        shifted_pix[bad] = 12*nside**2 # set index to be beyond pixel indices\n",
    "\n",
    "        # find indices with search and add a \"weights\" column\n",
    "        # (expect that wt_pix are NEST ordering and sorted)\n",
    "        weight_index = np.searchsorted(self.wt_pix,shifted_pix)\n",
    "        band_index = np.fmin(31, photons.band.values) #all above 1 TeV into last bin\n",
    "\n",
    "        # final grand lookup -- isn't numpy wonderful!\n",
    "        photons.loc[:,'weight'] = self.wts[tuple([band_index, weight_index])]\n",
    "        \n",
    "        \n",
    "    def _new_format(self, photons):\n",
    "\n",
    "        wt_tables =self.wt_dict\n",
    "        data_nside=1024\n",
    "        #photons = photons.rename(columns=dict(weight='old_wt'))\n",
    "        photons.loc[:,'weight'] = np.nan\n",
    "\n",
    "        if self.config.verbose>1:\n",
    "            print(f'WeightMan: processing {len(photons):,} photons')\n",
    "\n",
    "        def load_data( band_id):\n",
    "            \"\"\" fetch pixels and weights for the band;\n",
    "                adjust pixels to the band nside\n",
    "                generate mask for pixels, weights\n",
    "            \"\"\"\n",
    "            band = photons[photons.band==band_id] #.query('band== @band_id')\n",
    "            wt_table = wt_tables[band_id]\n",
    "            nside =  wt_table['nside'] \n",
    "            new_weights = wt_table['wts']\n",
    "            to_shift = int(2*np.log2(data_nside//nside))\n",
    "            data_pixels = np.right_shift(band.pixel, to_shift) \n",
    "            wt_pixels=wt_table['pixels']\n",
    "            good = np.isin( data_pixels, wt_pixels)\n",
    "            if self.config.verbose>1:\n",
    "                print(f'\\t {band_id:2}: {len(band):8,} -> {sum(good ):8,}')\n",
    "            return data_pixels, new_weights, good\n",
    "\n",
    "        def set_weights(band_id):\n",
    "            if band_id not in wt_tables.keys(): return\n",
    "\n",
    "            data_pixels, new_weights, good = load_data(band_id)\n",
    "            wt_pixels = wt_tables[band_id]['pixels']\n",
    "            indicies = np.searchsorted( wt_pixels, data_pixels[good])\n",
    "            new_wts = new_weights[indicies]\n",
    "            # get subset of photons in this band, with new weights\n",
    "            these_photons = photons[photons.band==band_id][good]\n",
    "            these_photons.loc[:,'weight']=new_wts\n",
    "            photons.loc[photons.band==band_id,'weight'] = these_photons.weight\n",
    "    #         if self.config.verbose>1:\n",
    "    #             print(f' -> {len(new_wts):8,}')\n",
    "\n",
    "        for band_id in range(16):\n",
    "            set_weights(band_id)\n",
    "\n",
    "        return photons\n",
    "        \n",
    "    def add_weights(self, photons):\n",
    "        \"\"\"\n",
    "        get the photon pixel ids, convert to NEST (if not already) and right shift them\n",
    "        add 'weight', remove 'band', 'pixel'\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.format==0:\n",
    "            self._old_format(photons)\n",
    "        else:\n",
    "            photons = self._new_format(photons)\n",
    "\n",
    "        # don't need these columns now (add flag to config to control??)\n",
    "        photons.drop(['pixel'], axis=1, inplace=True)\n",
    "\n",
    "        if self.config.verbose>1:\n",
    "            print(f'\\t{sum(np.isnan(photons.weight.values)):,} events without weight')\n",
    "        return photons\n",
    "\n",
    "def weight_radius_plots(photons):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axx = plt.subplots(2,8, figsize=(16,5), sharex=True, sharey=True)\n",
    "    plt.subplots_adjust(hspace=0.02, wspace=0)\n",
    "    for id,ax in enumerate(axx.flatten()):\n",
    "        subset = photons.query('band==@id & weight>0')\n",
    "        ax.semilogy(subset.radius, subset.weight, '.', label=f'{id}');\n",
    "        ax.legend(loc='upper right', fontsize=10)\n",
    "        ax.grid(alpha=0.5)\n",
    "    ax.set(ylim=(8e-4, 1.2), xlim=(0,4.9))\n",
    "    plt.suptitle('Weights vs. radius per band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"WeightMan\" class=\"doc_header\"><code>class</code> <code>WeightMan</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>WeightMan</code>(**`config`**, **`source`**=*`None`*, **`filename`**=*`None`*) :: `dict`\n",
       "\n",
       "Weight Management\n",
       "\n",
       "* Load weight tables\n",
       "* Assign weights to photons"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(WeightMan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightMan: file \"4FGL_J1257.0-6339_weights.pkl\": old format, nside=64\n",
      "Load weights from file /mnt/c/Users/thbur/OneDrive/work/wtlike/nbs/4FGL_J1257.0-6339_weights.pkl\n",
      "\tFound: P88Y3250 at (303.56, -0.78)\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "config.verbose=2\n",
    "wtman = WeightMan(config, filename='4FGL_J1257.0-6339_weights.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Fri May 28 11:41:58 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
