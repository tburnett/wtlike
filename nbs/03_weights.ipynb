{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp weights\n",
    "#%nbdev_default_export weights\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utilities.ipynb_docgen import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights\n",
    "> Load weighted data, combine with photon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys,  pickle, healpy\n",
    "import numpy as np\n",
    "from wtlike.config import *\n",
    "#from wtlike.photon_data import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_weights(config, source):\n",
    "    \"\"\"\n",
    "    Check that weights for the source are available: if so, return the weight file name\n",
    "    \n",
    "    - source -- A PointSource object with information on source location\n",
    "    \n",
    "    Returns the filepath to the file if successful, otherwise, print a message abount available files\n",
    "    \"\"\"\n",
    "    weight_files = config.wtlike_data/'weight_files' \n",
    "    assert weight_files.is_dir(), f'Expect {weight_files} to be a directory'\n",
    "    weight_file = weight_files/ (source.filename+'_weights.pkl')\n",
    "    if not weight_file.exists():\n",
    "        available = np.array(list(map(lambda p: p.name[:p.name.find('_weights')], \n",
    "                          weight_files.glob('*_weights.pkl'))))\n",
    "        print(f'{source} not found in list of weight files at\\n\\t {weight_files}.\\n Available:\\n{available}',\n",
    "             file = sys.stderr)\n",
    "        return None\n",
    "    return weight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"check_weights\" class=\"doc_header\"><code>check_weights</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>check_weights</code>(**`config`**, **`source`**)\n",
       "\n",
       "Check that weights for the source are available: if so, return the weight file name\n",
       "\n",
       "- source -- A PointSource object with information on source location\n",
       "\n",
       "Returns the filepath to the file if successful, otherwise, print a message abount available files"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source \"test\" at: (l,b)=(0.000,0.000) not found in list of weight files at\n",
      "\t /home/burnett/wtlike_data/weight_files.\n",
      " Available:\n",
      "['3C454.3' '3C_279' '4FGL_J1257.0-6339' 'B2_1520p31' 'BL_Lac' 'Eta_car'\n",
      " 'Geminga' 'PSR_B1259-63' 'PSR_J0633p1746' 'PSR_J0835-4510'\n",
      " 'PSR_J1302-6350' 'PSR_J1836p5925' 'PSR_J1909-3744' 'PSR_J1913p1011'\n",
      " 'PSR_J2022p3842' 'PSR_J2032p4127']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source \"Geminga\" at: (l,b)=(195.134,4.266) Should be found: file at /home/burnett/wtlike_data/weight_files/Geminga_weights.pkl \n"
     ]
    }
   ],
   "source": [
    "show_doc(check_weights)\n",
    "config = Config()\n",
    "if config.valid:\n",
    "    print('Check not found')\n",
    "    test_source = PointSource('test', (0,0))\n",
    "    check_weights(config, test_source)\n",
    "    good_source = PointSource('Geminga')\n",
    "    print(f'{good_source} Should be found: file at {check_weights(config, good_source)} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _load_weights(config, filename, ):\n",
    "    \"\"\"Load the weight informaton\n",
    "\n",
    "    filename: pickled dict with map info\n",
    "\n",
    "    \"\"\"\n",
    "    # load a pickle containing weights, generated by pointlike\n",
    "    assert os.path.exists(filename),f'File {filename} not found.'\n",
    "    with open(filename, 'rb') as file:\n",
    "        wtd = pickle.load(file, encoding='latin1')\n",
    "    assert type(wtd)==dict, 'Expect a dictionary'\n",
    "    test_elements = 'energy_bins pixels weights nside model_name radius order roi_name'.split()\n",
    "    assert np.all([x in wtd.keys() for x in test_elements]),f'Dict missing one of the keys {test_elements}'\n",
    "    if config.verbose>0:\n",
    "        print(f'Load weights from file {os.path.realpath(filename)}')\n",
    "        pos = wtd['source_lb']\n",
    "        print(f'\\tFound: {wtd[\"source_name\"]} at ({pos[0]:.2f}, {pos[1]:.2f})')\n",
    "    # extract pixel ids and nside used\n",
    "    wt_pix   = wtd['pixels']\n",
    "    nside_wt = wtd['nside']\n",
    "\n",
    "    # merge the weights into a table, with default nans\n",
    "    # indexing is band id rows by weight pixel columns\n",
    "    # append one empty column for photons not in a weight pixel\n",
    "    # calculated weights are in a dict with band id keys\n",
    "    wts = np.full((32, len(wt_pix)+1), np.nan, dtype=np.float32)\n",
    "    weight_dict = wtd['weights']\n",
    "    for k in weight_dict.keys():\n",
    "        t = weight_dict[k]\n",
    "        if len(t.shape)==2:\n",
    "            t = t.T[0] #???\n",
    "        wts[k,:-1] = t\n",
    "    return wts , wt_pix , nside_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _add_weights(config, wts, wt_pix, nside_wt, photon_data):\n",
    "    \"\"\" get the photon pixel ids, convert to NEST (if not already) and right shift them\n",
    "        add 'weight', remove 'band', 'pixel'\n",
    "    \"\"\"\n",
    "    if not config.nest:\n",
    "        # data are RING\n",
    "        photon_pix = healpy.ring2nest(config.nside, photon_data.pixel.values)\n",
    "    else:\n",
    "        photon_pix = photon_data.pixel.values\n",
    "    to_shift = 2*int(np.log2(config.nside/nside_wt));\n",
    "    shifted_pix =   np.right_shift(photon_pix, to_shift)\n",
    "    bad = np.logical_not(np.isin(shifted_pix, wt_pix))\n",
    "    if config.verbose>0 & sum(bad)>0:\n",
    "        print(f'\\tApplying weights: {sum(bad)} / {len(bad)} photon pixels are outside weight region')\n",
    "    if sum(bad)==len(bad):\n",
    "        a = np.array(healpy.pix2ang(nside_wt, wt_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "        b = np.array(healpy.pix2ang(nside_wt, shifted_pix, nest=True, lonlat=True)).mean(axis=1).round(1)\n",
    "\n",
    "        raise Exception(f'There was no overlap of the photon data at {b} and the weights at {a}')\n",
    "    shifted_pix[bad] = 12*nside_wt**2 # set index to be beyond pixel indices\n",
    "\n",
    "    # find indices with search and add a \"weights\" column\n",
    "    # (expect that wt_pix are NEST ordering and sorted)\n",
    "    weight_index = np.searchsorted(wt_pix,shifted_pix)\n",
    "    band_index = np.fmin(31, photon_data.band.values) #all above 1 TeV into last bin\n",
    "\n",
    "    # final grand lookup -- isn't numpy wonderful!\n",
    "    photon_data.loc[:,'weight'] = wts[tuple([band_index, weight_index])]\n",
    "    \n",
    "    # don't need these columns now (add flag to config to control??)\n",
    "    photon_data.drop(['band', 'pixel'], axis=1)\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        print(f'\\t{sum(np.isnan(photon_data.weight.values))} events without weight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def add_weights(config,  photon_data, source, nbins=50):\n",
    "    \"\"\" add weights for the source to the photon data\n",
    "    \n",
    "    - photon_data -- DataFrame with photon data\n",
    "    \n",
    "    - source -- `PointSource` object\n",
    "    \n",
    "    Return the weight value histogram\n",
    "    \"\"\"\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    if weight_file is None:\n",
    "        raise Exception(f'Weight file not found for {source}')\n",
    " \n",
    "    wts, wt_pix, nside_wt = _load_weights(config, weight_file)\n",
    "    _add_weights(config, wts, wt_pix, nside_wt, photon_data)\n",
    "\n",
    "    return np.histogram(photon_data.weight.values, np.linspace(0,1,nbins+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_weight_hist(config,  source, nbins=50, key=''):\n",
    "    \"\"\" return a weight distribution\n",
    "        \n",
    "    - photon_data -- DataFrame with photon data    \n",
    "    - source -- `PointSource` object\n",
    "    \n",
    "    Uses `add_weights`.\n",
    "    \"\"\"\n",
    "    def doit(nbins):\n",
    "        weight_file =  check_weights(config,  source)\n",
    "        if weight_file is None:\n",
    "            raise Exception(f'Weight file not found for {source}')\n",
    "        photon_data = get_photon_data(config,  source )\n",
    "        return add_weights(config, photon_data, source, nbins=nbins)\n",
    "    \n",
    "    key = f'weight_hist_{source.name}' if key=='' else key\n",
    "    description = f'Weight histogram for {source.name}' if config.verbose>1 else ''\n",
    "    return config.cache(key, doit, nbins, description=description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config(wtlike_data='~/wtlike_data')\n",
    "# if config.valid:\n",
    "#     source = PointSource('Geminga')\n",
    "#     h = get_weight_hist(config, source, key='')\n",
    "#     print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %nbdev_collapse_input\n",
    "# config = Config() \n",
    "\n",
    "# if config.valid:\n",
    "#     source = PointSource('Geminga')\n",
    "#     print(f'Loading photon data for source {source.name} to test adding weights')\n",
    "#     photon_data = get_photon_data(config,  source )\n",
    "#     h = add_weights(config, photon_data, source)\n",
    "#     print(f'Head of modified photon_data\\n {photon_data.head()}')\n",
    "#     plt.rc('font',size=14)\n",
    "#     fig, ax =plt.subplots(figsize=(4,3))\n",
    "#     n = len(h)\n",
    "#     ax.step( np.linspace(0,1, n+1) , np.concatenate([[h[0]], h])); \n",
    "#     ax.grid();\n",
    "#     ax.set(xlabel='weight', xlim=(0,1), ylim=(0,None))\n",
    "#     #plt.hist(photon_data.weight.values, np.linspace(0,1,51), histtype='stepfilled', lw=2)\n",
    "#     plt.title(f'Weights for source {source.name}')\n",
    "# else:\n",
    "#     print('Not testing since no files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90-main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sun May 16 05:39:57 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
