{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc39fe8-a0ae-4dfb-9be6-7dc13bb74e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug  1 08:58:28 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "# default_exp load_data\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "from nbdev.showdoc import show_doc\n",
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45f94e-3c5e-445e-8a5c-ead33b19baec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data\n",
    "> Prepare photons and exposure, given a source "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b20c9f-4a07-44cc-a52d-651a287e73e1",
   "metadata": {},
   "source": [
    "This module defines the function `load_source_data`.\n",
    "\n",
    "Depends on module Exposure for SC data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6128e-1002-4981-8cb2-402843cf88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import pickle, healpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wtlike.config import (Config, UTC, MJD)\n",
    "from wtlike.exposure import  binned_exposure, sc_data_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d1114-9f1d-4e34-ba77-6b9ff0eff95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_photons_near_source(config, source, week): #tzero, photon_df):\n",
    "    \"\"\"\n",
    "    Select the photons near a source\n",
    "\n",
    "    - source : a PointSource object\n",
    "    - week : dict with\n",
    "        - tzero : start time for the photon\n",
    "        - photon_df : DataFrame with photon data\n",
    "\n",
    "    Returns a DF with\n",
    "    - `band` index,\n",
    "    - `time` in MJD (added tstart and converted from MET)\n",
    "    - `pixel` index, nest indexing\n",
    "    - `radius` distance in deg from source direction\n",
    "    \"\"\"\n",
    "\n",
    "    def _cone(config, source, nest=True):\n",
    "        # cone geometry stuff: get corresponding pixels and center vector\n",
    "        l,b,radius = source.l, source.b, config.radius\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "        conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "        center = healpy.dir2vec(l,b, lonlat=True)\n",
    "        return center, conepix\n",
    "\n",
    "    center, conepix = _cone(config,source)\n",
    "\n",
    "    df = week['photons']\n",
    "    tstart = week['tstart']\n",
    "    allpix = df.nest_index.values\n",
    "\n",
    "    # select by comparing high-order pixels (faster)\n",
    "    shift=11\n",
    "    a = np.right_shift(allpix, shift)\n",
    "    c = np.unique(np.right_shift(conepix, shift))\n",
    "    incone = np.isin(a,c)\n",
    "\n",
    "    if sum(incone)<2:\n",
    "        if config.verbose>1:\n",
    "            print(f'\\nWeek starting {UTC(MJD(tstart))} has 0 or 1 photons')\n",
    "        return\n",
    "\n",
    "    if config.verbose>2:\n",
    "        a, b = sum(incone), len(allpix)\n",
    "        print(f'Select photons for source {source.name}:\\n\\tPixel cone cut: select {a} from {b} ({100*a/b:.1f}%)')\n",
    "\n",
    "    # cut df to entries in the cone\n",
    "    dfc = df[incone].copy()\n",
    "\n",
    "    if 'trun' in dfc:\n",
    "        time = dfc.run_id.astype(float) + dfc.trun * config.offset_size\n",
    "    else:\n",
    "        # old: convert  to float, add tstart, convert to MJD\n",
    "        time = np.array(dfc.time, float)+tstart\n",
    "    dfc.loc[:,'time'] = MJD(time)\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = dfc\n",
    "\n",
    "    # make sure times are monotonic by sorting (needed since runs not in order in most\n",
    "    #  week-files after March 2018)\n",
    "    out_df = dfc.sort_values(by='time')\n",
    "\n",
    "    if config.verbose>2:\n",
    "        print(f'selected photons:\\n{out_df.head()}')\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70f37d-639e-497c-9524-a2ca639b5e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class ProcessWeek(object):\n",
    "    \"\"\"\n",
    "    Process a week    \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, config, source, week_file): #, carry_in=np.zeros(32)): \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        with open(week_file, 'rb') as inp:\n",
    "            week = pickle.load(inp)\n",
    "        pdf = week['photons']\n",
    "        sc_data = edf = week['sc_data']\n",
    "        self.start = MJD(week['tstart'])\n",
    "        self.config = config\n",
    "\n",
    "        if config.verbose>1:\n",
    "            print(f'Opened week file \"{week_file.name}\" of {UTC(self.start)}')\n",
    "            print(f'\\tFound {len(pdf):,} photons, {len(edf):,} SC entries)')\n",
    "        \n",
    "        self.sc_df = sc_df = sc_data_selection(config, source, sc_data)\n",
    "        \n",
    "        # interleaved start/stop \n",
    "        self.stime = np.empty(2*len(sc_df.start))\n",
    "        self.stime[0::2]=sc_df.start.values\n",
    "        self.stime[1::2]=sc_df.stop.values\n",
    "        assert np.all(np.diff(self.stime)>=0), 'Time-ordering failure'\n",
    "\n",
    "        self.lt = sc_df.livetime.values\n",
    "        self.ct = sc_df.cos_theta.values\n",
    "        \n",
    "\n",
    "        pdf = _get_photons_near_source(config,source, week)\n",
    "        if pdf is None or len(pdf)<3 :\n",
    "            self.photons = None\n",
    "        else:\n",
    "            assert pdf is not None and len(pdf)>0\n",
    "            \n",
    "            # set weights from the weight table, removing those with no weight            \n",
    "            pdf = source.wtman.add_weights(pdf)\n",
    "            \n",
    "            # finally set the time and the exposure per remaining photons\n",
    "            self.photons = self.photon_times( pdf )\n",
    "            \n",
    "    def __repl__(self):\n",
    "        return f'Data for week of {UTF(self.start)}: {len(self.photons):,} photons'\n",
    "    \n",
    "    def __str__(self): return self.__repl__()\n",
    "        \n",
    "    def photon_times(self, pdf):\n",
    "        \n",
    "        # construct the time from the run number and offset\n",
    "        ptime = MJD(pdf.run_id.astype(float) + pdf.trun * self.config.offset_size)\n",
    "        pdf.loc[:,'time'] = ptime\n",
    "\n",
    "        # select the subset with exposure info\n",
    "        tk = np.searchsorted(self.stime, ptime)\n",
    "        good_exp = np.mod(tk,2)==1\n",
    "        pdfg = pdf[good_exp].copy()\n",
    "        if len(pdfg)==0:\n",
    "            return None\n",
    "        pdfg.drop(columns=['trun'], inplace=True)\n",
    "        # time edges-- same for each band\n",
    "        #xp = np.append(self.stime[0::2],self.stime[-1]) \n",
    "        \n",
    "        return pdfg\n",
    "    \n",
    "    def hist_spacecraft(self):\n",
    "        self.sc_df.hist('livetime cos_theta exp'.split(), bins=100, layout=(1,3), figsize=(12,3));\n",
    "        \n",
    "    def hist_photons(self):\n",
    "        self.photons.hist('band time'.split(), bins=100, log=True, figsize=(12,3), layout=(1,3));\n",
    "        \n",
    "       \n",
    "    def __call__(self):\n",
    "        return dict(\n",
    "            start= self.start,\n",
    "            photons=self.photons, \n",
    "            exposure=self.sc_df,\n",
    "\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190dd838-2f14-41ba-9d5e-53e42f24a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_week_files(config, week_range=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_folder = config.datapath/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    weeks = week_range or  config.week_range\n",
    "    if week_range is not None:\n",
    "\n",
    "        slc = slice(*week_range) if type(week_range)==tuple else slice(week_range,week_range)\n",
    "        wk_table = pd.Series(data=[df for df in data_files],\n",
    "                     index= [ int(df.name[-7:-4]) for df in  data_files],\n",
    "                    )\n",
    "        data_files = wk_table.loc[slc].values\n",
    "\n",
    "        if config.verbose>0:\n",
    "            q = lambda x: x if x is not None else \"\"\n",
    "            print(f'LoadData: Loading weeks[{q(slc.start)}:{q(slc.stop)}:{q(slc.step)}]', end='' if config.verbose<2 else '\\n')\n",
    "    else:\n",
    "        if config.verbose>0: print(f'LoadData: loading all {len(data_files)} weekly files')\n",
    "    assert len(data_files)>0, f'Specified week_range {week_range} produced no output.'\n",
    "\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095aff0-aad7-4da7-a175-6542ef80860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_source_data(config, source, week_range=None, key='', clear=False):\n",
    "    \"\"\"\n",
    "    This is a client of SourceData. \n",
    "    \n",
    "    - week_range [None] -- if None, select all weeks\n",
    "    - key ['']   -- key to use for cache, construct from name if not set      \n",
    "    - clear [False]\n",
    "    \n",
    "    Returns a tuple of\n",
    "    - photons\n",
    "    - exposure\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if config.datapath/'data_files' is None and key not in config.cache:\n",
    "        raise Exception(f'Data for {source.name} is not cached, and config.datapath/\"data_files\" is not set')\n",
    "\n",
    "    def load_from_weekly_data(config, source, week_range=None):\n",
    "\n",
    "        week_files = get_week_files(config, week_range) \n",
    "        pp = []\n",
    "        ee = []\n",
    "\n",
    "        for week_file in week_files:\n",
    "            if config.verbose<2: print('.', end='')\n",
    "            elif config.verbose>=2:\n",
    "                print(f'Loading file {week_file}-----')\n",
    "\n",
    "            wk = ProcessWeek(config, source, week_file)\n",
    "\n",
    "            # append week data to photons, weighted exposure, band exposure\n",
    "            pdf = wk.photons\n",
    "            edf = wk.sc_df\n",
    "            if pdf is not None and len(pdf)>2:\n",
    "                pp.append(pdf)\n",
    "            if len(edf)>0:\n",
    "                ee.append(edf)\n",
    "\n",
    "        print('');\n",
    "\n",
    "        # concatenate the two lists of DataFrames\n",
    "        p_df = pd.concat(pp, ignore_index=True)\n",
    "        p_df.loc[:,'run_id'] = pd.Categorical(p_df.run_id)\n",
    "        e_df = pd.concat(ee, ignore_index=True)\n",
    "\n",
    "        return p_df, e_df\n",
    "    description=f'SourceData:  {source.name}'\n",
    "    \n",
    "    if week_range is not None:\n",
    "        # always load directly if weeks specified\n",
    "        print(description)\n",
    "        r = load_from_weekly_data(config, source, week_range=week_range)\n",
    "    else:\n",
    "        # use the cache\n",
    "        key = f'{source.filename}_data' if key=='' else key\n",
    "        r = config.cache(key,\n",
    "                    load_from_weekly_data, config, source, week_range=None,\n",
    "                    overwrite=clear,\n",
    "                    description=description)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b12e5-688d-4dbe-82ec-b437a0d6f5a8",
   "metadata": {},
   "source": [
    "### Develop incremental update, using weeks\n",
    "Also try multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c9857-fe43-4331-a1ac-f727c81adf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "class TWeek():\n",
    "    def __init__(self, config, source):\n",
    "        self.config=config\n",
    "        self.source=source\n",
    "        \n",
    "    def __call__(self, wkf):\n",
    "        print('.', end='')\n",
    "        eman = ProcessWeek( self.config, self.source, wkf)\n",
    "        return (eman.photons, eman.sc_df)\n",
    "    \n",
    "def get_week_data(config, source, week_range, processes=1):\n",
    "    \n",
    "    from multiprocessing import Pool\n",
    "    \n",
    "    week_files = get_week_files(config,  week_range) \n",
    "    txt = f', with multiprocessing, {processes} processes' if processes>1 else ''\n",
    "    print(f'Processing {len(week_files)} files{txt}')\n",
    "    \n",
    "    process_week = TWeek(config, source)\n",
    "    \n",
    "    if processes>1:\n",
    "        with Pool(processes=processes) as pool:\n",
    "            week_data = pool.map(process_week, week_files)\n",
    "    else:\n",
    "        week_data = map(process_week,  week_files)\n",
    "    print('\\n')\n",
    "    \n",
    "    pp = []\n",
    "    ee = []\n",
    "\n",
    "    for wk in week_data:\n",
    "        # append week data to photons, weighted exposure, band exposure\n",
    "        pdf,edf = wk\n",
    "        if pdf is not None and len(pdf)>2:\n",
    "            pp.append(pdf)\n",
    "        if len(edf)>0:\n",
    "            ee.append(edf)\n",
    "\n",
    "    return pp,ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26b143-563d-4993-9ae9-26a78f28310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# config = Config()\n",
    "# from wtlike.sources import PointSource\n",
    "# week_range = (9,508)\n",
    "# source = PointSource('Geminga')\n",
    "# config.verbose=0\n",
    "# !date\n",
    "# pp,ee = get_week_data(config, source, week_range)\n",
    "# !date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be674d-88cf-471b-b6f9-277cc8cd17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !date\n",
    "# pp,ee = get_week_data(config, source, week_range, processes=8)\n",
    "# !date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4745312-54d9-4e8d-853b-9cfcba4ad89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# config=Config(); config.verbose=1\n",
    "# source =PointSource('Geminga', config=config) \n",
    "# t = load_source_data(config, source, week_range=None, key='Geminga_test', clear=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c98f507-7000-42f9-bc87-cfa43b541bd4",
   "metadata": {},
   "source": [
    "### Test with a Geminga week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fc93a1-2f37-42f9-9009-05ac5937030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightMan: file from source \"Geminga\"_weights.pkl : 16 bamds with nsides 64 to 512\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'datapath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c8890ed77807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mwtlike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPointSource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mPointSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Geminga'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mweek_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_week_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m665\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m665\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mweek_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweek_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessWeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweek_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-41e3dc1be187>\u001b[0m in \u001b[0;36mget_week_files\u001b[0;34m(config, week_range)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'data_files'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mweeks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweek_range\u001b[0m \u001b[0;32mor\u001b[0m  \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweek_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'datapath'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "config=Config(); config.verbose=2\n",
    "if config.valid:\n",
    "    # testing\n",
    "    from wtlike.sources import PointSource\n",
    "    source =PointSource('Geminga', config=config) \n",
    "    week_files = get_week_files(config, (665,665)); \n",
    "    week_file = week_files[0]\n",
    "    self = ProcessWeek(config, source, week_file)\n",
    "\n",
    "    self.hist_photons()\n",
    "    self.hist_spacecraft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25065484-8067-4598-b9fc-be8025c310b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82867e23-0a89-4f87-bce5-f4a8c82fe78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
