{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exposure\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exposure\n",
    "> calculate exposure for a point source from a GTI and FT2 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main interface is `get_binned_exposure`, which returns arrays with the (default) bins and exposure, needed for making cells.\n",
    "\n",
    "Also manage default bins, see `get_default_bins`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import simps\n",
    "\n",
    "from wtlike.config import *\n",
    "from wtlike.load_gti import get_gti\n",
    "from wtlike.effective_area import EffectiveArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_default_bins(config, exposure):\n",
    "    \"\"\"set up default bins from exposure; and config.mjd_range if set.\n",
    "    \n",
    "    adjust stop to come out even,    round to whole day\n",
    "    \"\"\"\n",
    "\n",
    "    start = np.round(exposure.start.values[0])\n",
    "    stop =  np.round(exposure.stop.values[-1])\n",
    "    if config.mjd_range is None:\n",
    "        config.mjd_range = (start,stop)\n",
    "\n",
    "    step = config.time_interval\n",
    "    nbins = int(round((stop-start)/step))\n",
    "    tb =time_bins = np.linspace(start, stop, nbins+1)\n",
    "    if config.verbose>0:\n",
    "        print(f'Time bins: {nbins} intervals of {step} days, '\\\n",
    "              f'in range ({time_bins[0]:.1f}, {time_bins[-1]:.1f})')\n",
    "    return time_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _process_ft2(config, source, ft2_file_path, gti, effective_area):\n",
    "    \"\"\"Process a set of FT2 files, with S/C history data\n",
    "    Parameters:\n",
    "        - config -- verbose, cos_theta_max, z_max\n",
    "        - source -- l,b for position\n",
    "        - ft2_files -- list of spacecraft files\n",
    "        - gti -- GTI object with allowed intervals\n",
    "        - effective_area -- function of energy and angle with respect to zenity\n",
    "    Generate a dataset with fields:\n",
    "        - start, stop : start  and stop times in MJD\n",
    "        - exposure    : calculated exposure using effective area\n",
    "\n",
    "     \"\"\"\n",
    "    # combine the files into a DataFrame with following fields besides START and STOP (lower case for column)\n",
    "    fields    = ['LIVETIME','RA_SCZ','DEC_SCZ', 'RA_ZENITH','DEC_ZENITH']\n",
    "    ft2_files = list(ft2_file_path.glob('*.fits'))\n",
    "    if config.verbose>1:\n",
    "        print(f'Processing {len(ft2_files)} S/C history (FT2) files')\n",
    "        print(f'  applying cuts cos(theta) < {config.cos_theta_max},  z < {config.z_max}')\n",
    "    sc_data=[]\n",
    "    for filename in ft2_files:\n",
    "        with fits.open(filename) as hdu:\n",
    "            scdata = hdu['SC_DATA'].data\n",
    "            # get times to check against MJD limits and GTI\n",
    "            start, stop = [MJD(np.array(scdata.START, float)),\n",
    "                           MJD(np.array(scdata.STOP, float))]\n",
    "            if config.mjd_range is not None:\n",
    "                a,b=  config.mjd_range\n",
    "                if stop[-1]<a:\n",
    "                    print(f'\\tfile {filename}: skip, before selected range' )\n",
    "                    continue\n",
    "                elif start[0]>b:\n",
    "                    print(f'\\tfile {filename}: quit, beyond range')\n",
    "                    break\n",
    "            # apply GTI to bin center (avoid edge effects?)\n",
    "            in_gti = gti(0.5*(start+stop))\n",
    "            if config.verbose>2:\n",
    "                print(f'\\tfile {filename}: {len(start)} entries, {sum(in_gti)} in GTI')\n",
    "            t = [('start', start[in_gti]), ('stop',stop[in_gti])]+\\\n",
    "                [(field.lower(), np.array(scdata[field][in_gti],np.float32)) for field in fields ]\n",
    "            sc_data.append( pd.DataFrame(dict(t) ) )\n",
    "    df = pd.concat(sc_data, ignore_index=True)\n",
    "\n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "    config.dfm = dfm ##############debug\n",
    "    # apply MJD range if present. note times in MJD\n",
    "    start, stop = dfm.start,dfm.stop\n",
    "    lims = slice(None)\n",
    "    if config.mjd_range is not None:\n",
    "#         a, b = config._get_limits(start)\n",
    "        a, b = np.searchsorted(start, config.mjd_range)\n",
    "        if a>0 or b<len(start):\n",
    "            if config.verbose>1:\n",
    "                print(f'\\tcut from {len(start):,} to {a} - {b}, or {b-a:,} entries after MJD range selection')\n",
    "            dfm = dfm.iloc[a:b]\n",
    "            lims = slice(a,b)\n",
    "\n",
    "\n",
    "    expose = _exposure(config, effective_area, livetime[lims], pcosines[mask][lims])\n",
    "    return pd.DataFrame(dict(start=start[lims],stop=stop[lims], exposure=expose))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _exposure(config, effective_area, livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = effective_area\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_exposure(config,  source, gti_key='gti'):\n",
    "    \"\"\"Return the exposure for the source\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    def getit():\n",
    "        files = config.files\n",
    "   \n",
    "        gti = get_gti(config, gti_key)\n",
    "        aeff = EffectiveArea(file_path = files.aeff)\n",
    "        exposure =  _process_ft2(config, source, files.ft2, gti, aeff)\n",
    "        if config.verbose>1:\n",
    "            print(f'{len(exposure)} entries, MJD {exposure.iloc[0].start:.0f}'\n",
    "                  f' - {exposure.iloc[-1].stop:.0f}')\n",
    "        return exposure\n",
    "    \n",
    "    key = f'exposure_{source.name}'\n",
    "    description = f'exposure for {source.name}' if config.verbose>1 else ''\n",
    "    return config.cache(key, getit, description=description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get, and display the exposure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a tuple of time intervals and the exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exposure for Geminga: Restoring from cache with key \"exposure_Geminga\"\n",
      "Exposure for Geminga:\n",
      "                 start          stop       exposure\n",
      "0        54682.656038  54682.656375  112998.052089\n",
      "1        54682.656375  54682.656722  116901.160578\n",
      "2        54682.656722  54682.657069  116857.143435\n",
      "3        54682.657069  54682.657416  116791.434285\n",
      "4        54682.657416  54682.657764  116876.550059\n",
      "...               ...           ...            ...\n",
      "9609753  58698.052602  58698.052949   52427.727986\n",
      "9609754  58698.052949  58698.053296   49343.687807\n",
      "9609755  58698.053296  58698.053643   44567.360241\n",
      "9609756  58698.053643  58698.053991   38779.120217\n",
      "9609757  58698.053991  58698.054338   31786.491045\n",
      "\n",
      "[2695715 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from wtlike.config import Config,  PointSource, Cache\n",
    "config = Config()\n",
    "source = PointSource('Geminga')\n",
    "\n",
    "if config.valid:\n",
    "    exposure = get_exposure(config, source)  \n",
    "    print(f'Exposure for {source.name}:\\n {exposure}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_contiguous_exposure(exposure, max_interval=10, verbose=0 ):\n",
    "    \"\"\"Combine exposure intervals from an exposure dataframe\n",
    "    return array of (start, stop) pairs\n",
    "    \"\"\"\n",
    "\n",
    "    t0s = exposure.start.values\n",
    "    t1s = exposure.stop.values\n",
    "    break_mask = (t0s[1:]-t1s[:-1])>max_interval/day \n",
    "    break_starts = t1s[:-1][break_mask]\n",
    "    break_stops = t0s[1:][break_mask]\n",
    "    # now assumble the complement\n",
    "    good_starts = np.empty(len(break_starts)+1)\n",
    "    good_stops = np.empty_like(good_starts)\n",
    "    good_starts[0] = t0s[0]\n",
    "    good_starts[1:] = break_stops\n",
    "    good_stops[-1] = t1s[-1]\n",
    "    good_stops[:-1] = break_starts\n",
    "    if verbose>1:\n",
    "        print(f'Generate list of contiguous exposures:\\n'\\\n",
    "              f'  WIth max interval {max_interval} s, combine {len(t0s):,} exposure entries to {len(good_stops):,} ')\n",
    "    return np.array([good_starts, good_stops]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_binned_exposure(config, source, time_bins=None, key='', gti_key='gti'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Calculate integrated exposures per bin\n",
    "    \n",
    "    - `source` -- a PointSource object\n",
    "    \n",
    "    - `time_bins` -- a list of bins, or None<br> If None, generate the default\n",
    "    \n",
    "    - `key` -- use cache unless set to None. \n",
    "    \n",
    "    Return two arrays: a list of integrated exposures per bin, normalized to the average per bin,\n",
    "    and the time_bin array.\n",
    "    \"\"\"\n",
    "\n",
    "    def doit(time_bins):\n",
    "        # get exposure calculation\n",
    "        exposure = get_exposure(config, source, gti_key)\n",
    "        exp   = exposure.exposure.values\n",
    "        estart= exposure.start.values\n",
    "        estop = exposure.stop.values\n",
    "\n",
    "        #use cumulative exposure to integrate over larger periods\n",
    "        cumexp = np.concatenate(([0],np.cumsum(exp)) )\n",
    "\n",
    "        # default bins depends on exposure\n",
    "        if time_bins is None:\n",
    "            time_bins = get_default_bins(config, exposure)\n",
    "\n",
    "        # get index into tstop array of the bin edges\n",
    "        edge_index = np.searchsorted(estop, time_bins)\n",
    "        # return the exposure integrated over the intervals\n",
    "        cum = cumexp[edge_index]\n",
    "\n",
    "        return np.diff(cum)/(cum[-1]-cum[0]) * (len(time_bins)-1) , time_bins\n",
    "    \n",
    "    key = f'binexp_{source.name}' if key=='' else key\n",
    "    description= f'binned exposure for source {source.name}'\n",
    "    \n",
    "    return config.cache(key, doit, time_bins, description=description)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exposure for Geminga: Restoring from cache with key \"exposure_Geminga\"\n",
      "Time bins: 4015 intervals of 1 days, in range (54683.0, 58698.0)\n",
      "Binned exposure:\n",
      "[1.26455999 1.21603611 1.18307691 ... 0.91251873 0.57874049 0.00704994], \n",
      "time bins: [54683. 54684. 54685. ... 58696. 58697. 58698.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if config.valid:\n",
    "    be, bins = get_binned_exposure(config, source,  key=None)\n",
    "    print(f'Binned exposure:\\n{be}, \\ntime bins: {bins}')\n",
    "    assert len(be)==len(bins)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate list of contiguous exposures:\n",
      "  WIth max interval 10 s, combine 2,695,715 exposure entries to 61,561 \n",
      "Check exposures in hours, gap:  0.02-751.07  delta: 0.00- 0.93\n"
     ]
    }
   ],
   "source": [
    "# #hide\n",
    "if config.valid:\n",
    "    expsub = exposure#.iloc[:1000]\n",
    "    ec = get_contiguous_exposure(expsub, verbose=2)\n",
    " \n",
    "    a, b  = ec[:,0], ec[:,1]\n",
    "    gap = b[1:]-a[:-1]\n",
    "\n",
    "    delta = (b-a)\n",
    "    print(f'Check exposures in hours, gap:  {gap.min()*24:.2f}-{gap.max()*24:.2f}'\\\n",
    "          f'  delta: {delta.min()*24:.2f}- {delta.max()*24:.2f}')\n",
    "    #plt.hist(delta*24*60, np.linspace(0,40,21));\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_effective_area.ipynb.\n",
      "Converted 02_gti.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 04_photon_data.ipynb.\n",
      "Converted 05_weights.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_cells.ipynb.\n",
      "Converted 08_loglike.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 10_simulation.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sun Dec 27 04:40:33 PST 2020\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
