{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-advertiser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to wtlike.source_data,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "from nbdev import *\n",
    "# default_exp source_data\n",
    "#%nbdev_default_export source_data\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-compatibility",
   "metadata": {},
   "source": [
    "# Source Data management\n",
    "> Extract data for a source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-freedom",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Given a point source, the class `SourceData` manages all data-oriented operations, providing all that is necessary to create a set of cells. It depends on the modules\n",
    "\n",
    "* `config` \n",
    "    This must set up the paths to the data created by `data_man`, and define paths for the effective area and weight files\n",
    "\n",
    "* effective_area\n",
    "* weights\n",
    "\n",
    "It implements `binned_exposure`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from wtlike.config import *\n",
    "from wtlike.data_man import *\n",
    "from wtlike.effective_area import *\n",
    "from wtlike.weights import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters \n",
      "  verbose         : 2\n",
      "  wtlike_data     : /home/burnett/wtlike_data\n",
      "  cachepath       : /home/burnett/wtlike_cache\n",
      "  radius          : 4\n",
      "  cos_theta_max   : 0.4\n",
      "  z_max           : 100\n",
      "  week_range      : (None, None)\n",
      "  time_bins       : (0, 0, 7)\n",
      "  use_uint8       : False\n",
      "  nside           : 1024\n",
      "  nest            : True\n",
      "  bins_per_decade : 4\n",
      "  base_spectrum   : lambda E: (E/1000)**-2.1\n",
      "  energy_range    : (100.0, 1000000.0)\n",
      "  likelihood_rep  : poisson\n",
      "\n",
      "Weekly folder \"/home/burnett/wtlike_data/data_files\" contains 665 weeks, from 9 to 674\n",
      "Most recent data to UTC 2021-05-06 00:45\n",
      "Source \"BL Lac\" at: (l,b)=(92.590,-10.441)\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "# check the weekly files\n",
    "config = Config(wtlike_data='~/wtlike_data', verbose=2)\n",
    "if config.valid:\n",
    "    ff = get_data_files(config)\n",
    "    source = PointSource('BL Lac') #Geminga')\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _exposure(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)\n",
    "\n",
    "def _calculate_exposure_for_source(config, source, week):\n",
    "    \"\"\"\n",
    "    Calcualate the exposure for the source during the given week\n",
    "    \"\"\"\n",
    "    df = week['sc_data']\n",
    "    \n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "\n",
    "    return  pd.DataFrame( \n",
    "        dict(\n",
    "            start=df.start[mask], \n",
    "            stop=df.stop[mask], \n",
    "            exp=_exposure(config, livetime, pcosines[mask]),\n",
    "            cos_theta=pcosines[mask],\n",
    "        ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-address",
   "metadata": {},
   "source": [
    "### Check exposure with last data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFound 16,968 S/C entries:  5,808 remain after zenith and theta cuts\n",
      "Calculate exposure using the energy domain 100.0-1000000.0 4 bins/decade\n",
      "       start      stop       exp  cos_theta\n",
      "46  59333.05  59333.05  59088.93       0.52\n",
      "47  59333.05  59333.05  61695.12       0.53\n",
      "48  59333.05  59333.05  64587.83       0.55\n",
      "49  59333.05  59333.05  67566.50       0.56\n",
      "50  59333.05  59333.05  70436.57       0.57\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "if config.valid:\n",
    "    filename=ff[-1]\n",
    "    week = pickle.load( open(filename,'rb') )\n",
    "    e_df = _calculate_exposure_for_source(config, source, week); \n",
    "    print(e_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _get_photons_near_source(config, source, week): #tzero, photon_df):\n",
    "    \"\"\"\n",
    "    Select the photons near a source\n",
    "    \n",
    "    - source : a PointSource object\n",
    "    - week : dict with \n",
    "        - tzero : start time for the photon\n",
    "        - photon_df : DataFrame with photon data\n",
    "    \n",
    "    Returns a DF with \n",
    "    - `band` index, \n",
    "    - `time` in MJD (added tstart and converted from MET)\n",
    "    - `pixel` index, nest indexing \n",
    "    - `radius` distance in deg from source direction\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cone(config, source, nest=True):\n",
    "        # cone geometry stuff: get corresponding pixels and center vector\n",
    "        l,b,radius = source.l, source.b, config.radius\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "        conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "        center = healpy.dir2vec(l,b, lonlat=True)\n",
    "        return center, conepix\n",
    "    \n",
    "    center, conepix = _cone(config,source)\n",
    "\n",
    "    df = week['photons']\n",
    "    tstart = week['tstart']\n",
    "    allpix = df.nest_index.values\n",
    "\n",
    "    # select by comparing high-order pixels (faster)\n",
    "    shift=11\n",
    "    a = np.right_shift(allpix, shift)\n",
    "    c = np.unique(np.right_shift(conepix, shift))\n",
    "    incone = np.isin(a,c)\n",
    "    \n",
    "    if sum(incone)<2:\n",
    "        if config.verbose>1:\n",
    "            print(f'\\nWeek at {UTC(MJD(tstart))} has 0 or 1 photons')\n",
    "        return\n",
    "    \n",
    "    if config.verbose>2:\n",
    "        a, b = sum(incone), len(allpix)\n",
    "        print(f'Select photons for source {source.name}:\\n\\tPixel cone cut: select {a} from {b} ({100*a/b:.1f}%)')\n",
    "\n",
    "    # cut df to entries in the cone\n",
    "    dfc = df[incone]\n",
    "\n",
    "    # distance from center for all accepted photons\n",
    "    ll,bb = healpy.pix2ang(config.nside, dfc.nest_index,  nest=True, lonlat=True)\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32))\n",
    "    in_cone = t2<config.radius\n",
    "\n",
    "    if config.verbose>2:\n",
    "        print(f'\\tGeometric cone cut: select {sum(in_cone)}')\n",
    "    # assume all in the GTI (should check)\n",
    "\n",
    "    # times: convert to float, add tstart, convert to MJD\n",
    "    time = MJD(np.array(dfc.time, float)+tstart)\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "        [np.array(dfc.band), time, dfc.nest_index, np.atleast_1d(t2)],\n",
    "        names='band time pixel radius'.split()))[in_cone]\n",
    "\n",
    "    # make sure times are monotonic by sorting (needed for most weeks after March 2018)\n",
    "    out_df = out_df.sort_values(by='time')\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-twelve",
   "metadata": {},
   "source": [
    "### test photon data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  hide\n",
    "# # if config.valid:\n",
    "# for i,f in enumerate(ff[300:]):\n",
    "#     week = pickle.load(open(f,'rb')); \n",
    "#     tstart = week['tstart']\n",
    "#     print(f'{i} ', end='')\n",
    "#     #print(f'TSTART: MET {tstart}, UTC {UTC(MJD(tstart))}')\n",
    "#     p_df = _get_photons_near_source(config, source, week )\n",
    "# #     print(len(p_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export  \n",
    "def contiguous_bins(exposure, min_gap=20, min_duration=600):\n",
    "    \n",
    "    \"\"\" return a dataframe with start and stop columns that \n",
    "    denote contiguous intervals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    stop = exposure.stop.values\n",
    "    start = exposure.start.values\n",
    "\n",
    "    # interleave  the starts ane stops\n",
    "    ssint = np.empty(2*len(start))\n",
    "    ssint[0::2] = start\n",
    "    ssint[1::2] = stop\n",
    "\n",
    "    # Tag the (stpp,start) pairs < 10 sec as  not adjacent\n",
    "    not_adjacent = np.diff(ssint)[1::2] > min_gap/(24*3600) ; \n",
    "    #print(f'{sum(not_adjacent)} (start,stop) pairs are not closer than {min_gap} s')\n",
    "\n",
    "    # make a mask, keep ends\n",
    "    mask = np.empty(2*len(start), bool)\n",
    "    mask[0] = mask[-1] = True\n",
    "    # \n",
    "\n",
    "    # insert into mask -- keep only the (stop,start) pairs  which are not adjacent\n",
    "    mask[1:-2:2] = not_adjacent\n",
    "    mask[2:-1:2] = not_adjacent\n",
    "    \n",
    "    # apply mask, split into start and stop\n",
    "    keep = ssint[mask]\n",
    "    return keep\n",
    "#     gstart, gstop = keep[0::2], keep[1::2]\n",
    "#     df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "    \n",
    "#     # add column with duration in sec\n",
    "#     df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "#     return df.query(f'duration>{min_duration}')\n",
    "\n",
    "def time_bin_edges(config, exposure, tbin=None):\n",
    "    \"\"\"Return an interleaved array of start/stop values\n",
    "    \n",
    "    tbin: an array (a,b,d), default config.time_bins\n",
    "    \n",
    "    interpretation of a, b:\n",
    "\n",
    "        if > 5000, interpret as MJD\n",
    "        if <0, back from stop\n",
    "        otherwise, offset from start\n",
    "        \n",
    "    d : if positive, the day bin size\n",
    "        if 0; return contiguous bins\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # nominal total range, MJD edges\n",
    "    start = np.round(exposure.start.values[0])\n",
    "    stop =  np.round(exposure.stop.values[-1])\n",
    "\n",
    "    a, b, step = tbin if tbin is not None else config.time_bins\n",
    "    \n",
    "\n",
    "    if a>50000: start=a\n",
    "    elif a<0: start = stop+a\n",
    "    else : start += a\n",
    "\n",
    "\n",
    "    if b>5000: stop=b\n",
    "    elif b>0: stop = start+b\n",
    "    else: stop += b\n",
    "    \n",
    "    if step<=0:\n",
    "        return contiguous_bins(exposure.query(f'{start}<start<{stop}'),)\n",
    "\n",
    "    # adjust stop\n",
    "    nbins = int((stop-start)/step)\n",
    "    assert nbins>1, 'Bad binning: no bins'\n",
    "    stop = start+(nbins)*step\n",
    "    u =  np.linspace(start,stop, nbins+1 )\n",
    "    \n",
    "    # make an interleaved start/stop array\n",
    "    v = np.empty(2*nbins, float)\n",
    "    v[0::2] = u[:-1]\n",
    "    v[1::2] = u[1:]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59333. 59334. 59334. 59335. 59335. 59336. 59336. 59337. 59337. 59338.\n",
      " 59338. 59339. 59339. 59340.]\n",
      "[59339.00020583 59339.01895583 59339.06414102 59339.08497435\n",
      " 59339.13015954 59339.13544294 59339.14687327 59339.15103917\n",
      " 59339.19652528 59339.20268835 59339.26248593 59339.27045451\n",
      " 59339.32885167 59339.33831331 59339.39490491 59339.40654248\n",
      " 59339.46085398 59339.47860267 59339.52735861 59339.54784472\n",
      " 59339.593435   59339.61392111 59339.65946509 59339.6799512\n",
      " 59339.7255762  59339.74606231 59339.79169889 59339.812185\n",
      " 59339.85809935 59339.87858546 59339.92411787 59339.94460398]\n"
     ]
    }
   ],
   "source": [
    "exposure = e_df\n",
    "print(time_bin_edges(config, exposure, (0,0,1)) )\n",
    "print(time_bin_edges(config, exposure, (-1,0,0)))\n",
    "# print(time_bin_edges(config, exposure, (0,0,0.5)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,0,2)) )\n",
    "# print(time_bin_edges(config, exposure, (0,1,0.25)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,-4,0.25)) )   \n",
    "# print(time_bin_edges(config, exposure, (59326, 59330,0.5)) )    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-drive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keep = contiguous_bins(exposure)\n",
    "gstart, gstop = keep[0::2], keep[1::2]\n",
    "df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "\n",
    "# add column with duration in sec\n",
    "df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "\n",
    "df.hist('duration', bins=np.linspace(0,4e3,131));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def binned_exposure(config, exposure, time_edges):\n",
    "    \"\"\"Bin the exposure\n",
    "\n",
    "    - time_bins: list of edges, as an interleaved start/stop array\n",
    "       \n",
    "        \n",
    "    returns  array of exposure integrated over each time bin, times 1e-9\n",
    "    it is interleaved, client must apply [0::2] selection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get exposure calculation\n",
    "    exp   =exposure.exp.values\n",
    "    estart= exposure.start.values\n",
    "    estop = exposure.stop.values\n",
    "\n",
    "    # determine bins,\n",
    "\n",
    "    #use cumulative exposure to integrate over larger periods\n",
    "    cumexp = np.concatenate(([0],np.cumsum(exp)) )\n",
    "\n",
    "    # get index into tstop array of the bin edges\n",
    "    edge_index = np.searchsorted(estop, time_edges)\n",
    "    \n",
    "    # return the exposure integrated over the intervals\n",
    "    cum = cumexp[edge_index]\n",
    "   \n",
    "    # difference is exposure per interval: normalize it here\n",
    "    bexp = np.diff(cum) \n",
    "#     if config.verbose>1:\n",
    "#         print(f'Relative exposure per bin:\\n{pd.Series(bexp).describe(percentiles=[])}')\n",
    "    return bexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _load_from_weekly_data(config, source, week_range=None):\n",
    "    \"\"\"\n",
    "    Generate combinded DataFrames from a list of pickled files\n",
    "    Either weekly or monthly\n",
    "    \n",
    "    kwargs:\n",
    "    - week_range\n",
    "    \"\"\"\n",
    "    \n",
    "    # check weights\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    assert weight_file is not None\n",
    "    \n",
    "    data_folder = config.wtlike_data/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    iname = data_folder.name\n",
    "    \n",
    "    if config.verbose>0:\n",
    "        print(f\"\\tAssembling photon data and exposure for source {source.name} from\"\\\n",
    "              f' folder \"{data_folder}\",\\n\\t with {len(data_files)} files,'\\\n",
    "              f' last file:  {data_files[-1].name}: ', end='')\n",
    "    \n",
    "    w1,w2 = week_range or  config.week_range\n",
    "    if w1 is not None or w2 is not None:\n",
    "        if config.verbose>0:\n",
    "            print(f'\\tLoading weeks {t}')\n",
    "        data_files= data_files[w1:w2]\n",
    "    else:\n",
    "        if config.verbose>0: print('loading all files')\n",
    "            \n",
    "            \n",
    "\n",
    "    verbose, config.verbose=config.verbose, 0\n",
    "    # list of data framees\n",
    "    pp = []\n",
    "    ee = []\n",
    "    for f in data_files:\n",
    "        print('.', end='')\n",
    "        with open(f, 'rb') as inp:\n",
    "            week = pickle.load(inp)\n",
    "\n",
    "        photons = _get_photons_near_source(config, source, week )\n",
    "        if photons is not None:\n",
    "            pp.append(photons)\n",
    "        ee.append(_calculate_exposure_for_source(config, source, week ))\n",
    "    print('');    \n",
    "    config.verbose=verbose\n",
    "    # concatenate the two lists of DataFrames\n",
    "    p_df = pd.concat(pp, ignore_index=True)\n",
    "    e_df = pd.concat(ee, ignore_index=True)\n",
    "\n",
    "    if config.verbose>1:\n",
    "        times = p_df.time.values\n",
    "        print(f'Loaded {len(p_df):,} photons from {UTC(times[0])} to  {UTC(times[-1])} ')\n",
    "        print(f'Calculated {len(e_df):,} exposure entries')\n",
    "        \n",
    "    # add weights to photon data\n",
    "    add_weights(config, p_df, source)\n",
    "        \n",
    "    return p_df, e_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  hide\n",
    "# len(ff), ff[-132]\n",
    "# _load_from_weekly_data(config, source, week_range=(-132, -100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SourceData(object):\n",
    "    \"\"\" Load the photon data near the source and associated exposure. \n",
    "    \n",
    "    Either from:\n",
    "      1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
    "      2. the cache, with key `{source.name}_data`\n",
    "    \n",
    "    * source_name : if specified, create a PointSource object\n",
    "    * `config` : basic configuration\n",
    "    * `source` : PointSource object if specified\n",
    "    * `clear` : if set, overwrite the cached results\n",
    "    \n",
    "    Calculate the values for\n",
    "    \n",
    "    * S, B : sums of w and 1-w\n",
    "    * exptot : total associated exposure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_name, config=None, source=None, clear=False, \n",
    "                 week_range=None, key=''):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        self.config = config if config else Config()\n",
    "        if not (source_name or source):\n",
    "            print('Must specify either the source name or a PointSource object', file=sys.stderr)\n",
    "            return\n",
    " \n",
    "        try:\n",
    "            self.source = PointSource(source_name) if source_name else source\n",
    "        except Exception as e:\n",
    "            print(f'{e}', file=sys.stderr)\n",
    "            return\n",
    "        \n",
    "        self.source_name = self.source.name\n",
    "        self.verbose = self.config.verbose\n",
    "        \n",
    "        key = f'{self.source.name}_data' if key=='' else key\n",
    "        self.source.data_key = key\n",
    "        \n",
    "        if self.config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "            raise Exception(f'Data for {source.name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "        \n",
    "        photons, self.exposure = self.config.cache(key, \n",
    "                        _load_from_weekly_data, self.config, self.source, week_range,\n",
    "                        overwrite=clear,\n",
    "                        description=f'SourceData: photons and exposure for {self.source.name}')\n",
    "        \n",
    "        # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "        w = photons.weight\n",
    "        good = np.logical_not(np.isnan(w))\n",
    "        self.p_df = self.photons = photons.loc[good]\n",
    "        self.exptot = self.exposure.exp.sum() \n",
    "\n",
    "        # estimates for signal and background counts in toteal exposure\n",
    "        self.S = np.sum(w)\n",
    "        self.B = np.sum(1-w)\n",
    "        \n",
    "        # normailze to exposure\n",
    "#         self.exptot = sum(self.e_df.exp)        \n",
    "#         self.S_exp = self.S/self.exptot\n",
    "#         self.B_exp = self.B/self.exptot\n",
    "        \n",
    "        if self.verbose>0:\n",
    "            print(SourceData.__repr__(self))\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        time = self.photons.time.values\n",
    "        r = f'{self.__class__.__name__}: Source {self.source.name} with:'\\\n",
    "            f'\\n\\t data:     {len(self.photons):9,} photons from   {UTC(time[0])[:10]} to {UTC(time[-1])[:10]}'\\\n",
    "            f'\\n\\t exposure: {len(self.exposure):9,} intervals from {UTC(self.exposure.iloc[0].start)[:10]}'\\\n",
    "            f' to {UTC(self.exposure.iloc[-1].stop)[:10]}'\n",
    "        return r\n",
    "    \n",
    "    def binned_exposure(self, time_edges):\n",
    "        \"\"\"Bin the exposure\n",
    "        \n",
    "        - time_bins: list of edges.  \n",
    "        \"\"\"\n",
    "        return binned_exposure(self.config, self.exposure,  time_edges)\n",
    "    \n",
    "    def binned_cos_theta(self, time_bins=None):\n",
    "        \"\"\" Calculate average cosine of angle with respect to bore axis, per time bin\n",
    "        \"\"\"\n",
    "        if time_bins is None:\n",
    "            time_bins = get_default_bins(self.config, self.exposure)\n",
    "        df = self.exposure.copy()\n",
    "        estop =df.stop.values\n",
    "        df.loc[:,'tbin'] =np.digitize(estop, time_bins)\n",
    "        ct = df.groupby('tbin').mean()['cos_theta']\n",
    "        return ct, time_bins\n",
    "    \n",
    "    def weight_histogram(self, nbins=1000, key=''):\n",
    "        \"\"\" return a weight distribution\n",
    "        \"\"\"\n",
    "        def doit(nbins):\n",
    "            return np.histogram(self.p_df.weight.values, np.linspace(0,1,nbins+1))[0]\n",
    "\n",
    "        key = f'{self.source.name}_weight_hist' if key=='' else key\n",
    "        description = f'Weight histogram for {self.source.name}' if self.config.verbose>0 else ''\n",
    "        return self.config.cache(key, doit, nbins, description=description)\n",
    "        \n",
    "    def plot_data(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, (ax1,ax2, ax3,ax4) = plt.subplots(1,4, figsize=(15,4))\n",
    "        ax1.hist(self.p_df.time.values, 500, histtype='step');\n",
    "        ax1.set(xlabel='Time (MJD)')\n",
    "        ax2.hist(self.p_df.radius.values, 500, histtype='step');\n",
    "        ax2.set(xlabel='Radius (deg)');\n",
    "        \n",
    "        ax3.hist(self.p_df.band, 32, histtype='step', log=True);\n",
    "        ax3.set(xlabel='Band index')\n",
    "        ax4.hist(self.p_df.weight, 100, histtype='step')\n",
    "        ax4.set(xlabel='weight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photons and exposure for Geminga: Restoring from cache with key \"Geminga_data\"\n",
      "SourceData: Source Geminga with:\n",
      "\t data:     1,213,841 photons from   2008-08-04 to 2021-05-05\n",
      "\t exposure: 3,117,669 intervals from 2008-08-04 to 2021-05-06\n"
     ]
    }
   ],
   "source": [
    "#sd = SourceData('3C 279', week_range=(500,None), key=None)\n",
    "sd = SourceData('Geminga');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-transportation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878470.94, 335370.2, 267899188003.90836)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd.S, sd.B, sd.exptot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-location",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SourceData\" class=\"doc_header\"><code>class</code> <code>SourceData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SourceData</code>(**`source_name`**, **`config`**=*`None`*, **`source`**=*`None`*, **`clear`**=*`False`*, **`week_range`**=*`None`*, **`key`**=*`''`*)\n",
       "\n",
       "Load the photon data near the source and associated exposure. \n",
       "\n",
       "Either from:\n",
       "  1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files with weekly or monthly data\n",
       "  2. the cache, with key `{source.name}_data`\n",
       "\n",
       "* source_name : if specified, create a PointSource object\n",
       "* [`config`](/wtlikeconfig) : basic configuration\n",
       "* `source` : PointSource object if specified\n",
       "* `clear` : if set, overwrite the cached results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SourceData.binned_exposure\" class=\"doc_header\"><code>SourceData.binned_exposure</code><a href=\"__main__.py#L73\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SourceData.binned_exposure</code>(**`time_edges`**)\n",
       "\n",
       "Bin the exposure\n",
       "\n",
       "- time_bins: list of edges.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "show_doc(SourceData)\n",
    "show_doc(SourceData.binned_exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test extracting full dataset\n",
    "# #  hide\n",
    "# sd = SourceData('Geminga') \n",
    "# sd.plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-honduras",
   "metadata": {},
   "source": [
    "Development of code to summarize cos theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  hide\n",
    "# sd.e_df.cos_theta.hist(bins=100);\n",
    "\n",
    "# plt.plot(sd.weight_histogram(200,key=None), '.')\n",
    "\n",
    "# self = sd\n",
    "# time_bins = None\n",
    "# if time_bins is None:\n",
    "#     time_bins = get_default_bins(config, self.e_df)\n",
    "\n",
    "# df = self.e_df.copy()\n",
    "# estop =df.stop.values\n",
    "# df.loc[:,'tbin'] = np.digitize(estop, time_bins)\n",
    "# ct = df.groupby('tbin').mean()['cos_theta']\n",
    "# ct, bins = sd.binned_cos_theta()\n",
    "# print(len(ct), len(time_bins))\n",
    "# print(ct)\n",
    "\n",
    "# ctx = df.groupby('tbin').agg({'cos_theta': ['count','mean', 'min', 'max']})\n",
    "# print(ctx)\n",
    "\n",
    "# ct, bins = sd.binned_cos_theta()\n",
    "# print(len(ct), len(bins))\n",
    "# plt.plot(ct.index[:100], ct[:100],'.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 10_simulation.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted index.ipynb.\n",
      "Fri May  7 16:05:44 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-cannon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
