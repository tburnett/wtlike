{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-ordering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to wtlike.source_data,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "from nbdev import *\n",
    "%nbdev_default_export source_data\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-trade",
   "metadata": {},
   "source": [
    "# Source Data management\n",
    "> Extract data for a source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-excellence",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Given a point source, the class `SourceData` manages all data-oriented operations, providing all that is necessary to create a set of cells. It depends on the modules\n",
    "\n",
    "* `config` \n",
    "    This must set up the paths to the data created by `data_man`, and define paths for the effective area and weight files\n",
    "\n",
    "* effective_area\n",
    "* weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from wtlike.config import *\n",
    "from wtlike.effective_area import *\n",
    "from wtlike.weights import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-original",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly folder \"/home/burnett/weekly\" contains 662 weeks, last=#662\n"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "# check the weekly files\n",
    "\n",
    "weekly_folder = Path('/home/burnett/weekly')\n",
    "week_files = ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:])-9, ff))\n",
    "print(f'Weekly folder \"{weekly_folder}\" contains {len(wk)} weeks, last=#{max(wk)}')\n",
    "\n",
    "filename =  ff[-1]\n",
    "source = PointSource('Geminga')\n",
    "config = Config(verbose=3, data_folder = weekly_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-thing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File paths for light curves\n",
       "  data       : /home/burnett/weekly\n",
       "  ft2        : /home/burnett/work/lat-data/ft2\n",
       "  gti        : /home/burnett/work/lat-data/binned\n",
       "  aeff       : /home/burnett/work/lat-data/aeff\n",
       "  weights    : /home/burnett/onedrive/fermi/weight_files"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def _exposure(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = EffectiveArea(file_path=config.files.aeff)\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)\n",
    "\n",
    "def _calculate_exposure_for_source(config, source, week):\n",
    "    \"\"\"\n",
    "    Calcualate the exposure for the source during the given week\n",
    "    \"\"\"\n",
    "    df = week['sc_data']\n",
    "    \n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "\n",
    "    return  pd.DataFrame( \n",
    "        dict(\n",
    "            start=df.start[mask], \n",
    "            stop=df.stop[mask], \n",
    "            exp=_exposure(config, livetime, pcosines[mask])\n",
    "        ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFound 16,880 S/C entries:  4,118 remain after zenith and theta cuts\n",
      "Calculate exposure using the energy domain 100.0-1000000.0 4 bins/decade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>59312.057903</td>\n",
       "      <td>59312.058250</td>\n",
       "      <td>36372.382362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>59312.058250</td>\n",
       "      <td>59312.058597</td>\n",
       "      <td>40561.706631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>59312.058597</td>\n",
       "      <td>59312.058944</td>\n",
       "      <td>44767.510066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>59312.058944</td>\n",
       "      <td>59312.059291</td>\n",
       "      <td>49187.904150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>59312.059291</td>\n",
       "      <td>59312.059639</td>\n",
       "      <td>53630.510640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           start          stop           exp\n",
       "78  59312.057903  59312.058250  36372.382362\n",
       "79  59312.058250  59312.058597  40561.706631\n",
       "80  59312.058597  59312.058944  44767.510066\n",
       "81  59312.058944  59312.059291  49187.904150\n",
       "82  59312.059291  59312.059639  53630.510640"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "week = pickle.load( open(filename,'rb') )\n",
    "e_df = _calculate_exposure_for_source(config, source, week); \n",
    "e_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "\n",
    "def _get_photons_near_source(config, source, week): #tzero, photon_df):\n",
    "    \"\"\"\n",
    "    Select the photons near a source\n",
    "    \n",
    "    - source : a PointSource object\n",
    "    - week : dict with \n",
    "        - tzero : start time for the photon\n",
    "        - photon_df : DataFrame with photon data\n",
    "    \n",
    "    Returns a DF with \n",
    "    - `band` index, \n",
    "    - `time` in MJD (added tstart and converted from MET)\n",
    "    - `pixel` index, nest indexing \n",
    "    - `radius` distance in deg from source direction\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cone(config, source, nest=True):\n",
    "        # cone geometry stuff: get corresponding pixels and center vector\n",
    "        l,b,radius = source.l, source.b, config.radius\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "        conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "        center = healpy.dir2vec(l,b, lonlat=True)\n",
    "        return center, conepix\n",
    "    \n",
    "    center, conepix = _cone(config,source)\n",
    "\n",
    "    df = week['photons']\n",
    "    tstart = week['tstart']\n",
    "    allpix = df.nest_index.values\n",
    "\n",
    "    # select by comparing high-order pixels (faster)\n",
    "    shift=11\n",
    "    a = np.right_shift(allpix, shift)\n",
    "    c = np.unique(np.right_shift(conepix, shift))\n",
    "    incone = np.isin(a,c)\n",
    "\n",
    "    if config.verbose>2:\n",
    "        a, b = sum(incone), len(allpix)\n",
    "        print(f'Select photons for source {source.name}:\\n\\tPixel cone cut: select {a} from {b} ({100*a/b:.1f}%)')\n",
    "\n",
    "    # cut df to entries in the cone\n",
    "    dfc = df[incone]\n",
    "\n",
    "    # distance from center for all accepted photons\n",
    "    ll,bb = healpy.pix2ang(config.nside, dfc.nest_index,  nest=True, lonlat=True)\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32))\n",
    "    in_cone = t2<config.radius\n",
    "\n",
    "    if config.verbose>2:\n",
    "        print(f'\\tGeometric cone cut: select {sum(in_cone)}')\n",
    "    # assume all in the GTI (should check)\n",
    "\n",
    "    # times: convert to float, add tstart, convert to MJD\n",
    "    time = MJD(np.array(dfc.time, float)+tstart)\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "        [np.array(dfc.band), time, dfc.nest_index, t2],\n",
    "        names='band time pixel radius'.split()))[in_cone]\n",
    "\n",
    "    # make sure times are monotonic by sorting (needed for most weeks after March 2018)\n",
    "    out_df = out_df.sort_values(by='time')\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239557417.0 2008-08-04 15:44\n",
      "Select photons for source Geminga:\n",
      "\tPixel cone cut: select 2138 from 104927 (2.0%)\n",
      "\tGeometric cone cut: select 1606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band</th>\n",
       "      <th>time</th>\n",
       "      <th>pixel</th>\n",
       "      <th>radius</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>54682.657022</td>\n",
       "      <td>6738278</td>\n",
       "      <td>0.698381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54682.657934</td>\n",
       "      <td>6761152</td>\n",
       "      <td>2.498099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>54682.658637</td>\n",
       "      <td>6739138</td>\n",
       "      <td>0.290310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>54682.658760</td>\n",
       "      <td>6714890</td>\n",
       "      <td>3.276757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>54682.658997</td>\n",
       "      <td>6734033</td>\n",
       "      <td>5.024745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   band          time    pixel    radius\n",
       "1     6  54682.657022  6738278  0.698381\n",
       "2     3  54682.657934  6761152  2.498099\n",
       "4     4  54682.658637  6739138  0.290310\n",
       "5     1  54682.658760  6714890  3.276757\n",
       "6     0  54682.658997  6734033  5.024745"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "week = pickle.load(open(ff[0],'rb')); \n",
    "tstart = week['tstart']\n",
    "print(tstart, UTC(MJD(tstart)))\n",
    "p_df = _get_photons_near_source(config, source, week )\n",
    "p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def get_default_bins(config, exposure):\n",
    "    \"\"\"set up default bins from exposure; and config.mjd_range if set.\n",
    "    \n",
    "    adjust stop to come out even,    round to whole day\n",
    "    \"\"\"\n",
    "\n",
    "    start = np.round(exposure.start.values[0])\n",
    "    stop =  np.round(exposure.stop.values[-1])\n",
    "    if config.mjd_range is None:\n",
    "        config.mjd_range = (start,stop)\n",
    "\n",
    "    step = config.time_interval\n",
    "    nbins = int(round((stop-start)/step))\n",
    "    tb =time_bins = np.linspace(start, stop, nbins+1)\n",
    "    if config.verbose>0:\n",
    "        a,b = time_bins[0], time_bins[-1]\n",
    "        print(f'Time bins: {nbins} intervals of {step} days, '\\\n",
    "              f'from MJD {a:.1f}({UTC(a)[:10]}) to {b:.1f}({UTC(b)[:10]}))')\n",
    "    return time_bins        \n",
    "\n",
    "def _load_from_weekly_data(config, source):\n",
    "    \"\"\"\n",
    "    Generate combinded DataFrames from a list of pickled files\n",
    "    Either weekly or monthly\n",
    "    \"\"\"\n",
    "    \n",
    "    # check weights\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    assert weight_file is not None\n",
    "    \n",
    "    data_folder = Path(config.data_folder)\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    iname = data_folder.name\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        print(f\"Assembling photon data and exposure for source {source.name} from\"\\\n",
    "              f' folder \"{data_folder}\", with {len(data_files)} files,'\\\n",
    "              f' last={data_files[-1].name}')\n",
    "\n",
    "    verbose, config.verbose=config.verbose, 0\n",
    "    # list of data framees\n",
    "    pp = []\n",
    "    ee = []\n",
    "    for f in data_files:\n",
    "        print('.', end='')\n",
    "        with open(f, 'rb') as inp:\n",
    "            week = pickle.load(inp)\n",
    "        pp.append(_get_photons_near_source(config, source, week ))\n",
    "        ee.append(_calculate_exposure_for_source(config, source, week ))\n",
    "    print('');    \n",
    "    config.verbose=verbose\n",
    "    # concatenate the two lists of DataFrames\n",
    "    p_df = pd.concat(pp, ignore_index=True)\n",
    "    e_df = pd.concat(ee, ignore_index=True)\n",
    "\n",
    "    if config.verbose>1:\n",
    "        times = p_df.time.values\n",
    "        print(f'Loaded {len(p_df):,} photons from {UTC(times[0])} to  {UTC(times[-1])} ')\n",
    "        print(f'Calculated {len(e_df):,} exposure entries')\n",
    "        \n",
    "    # add weights to photon data\n",
    "    add_weights(config, p_df, source)\n",
    "        \n",
    "    return p_df, e_df\n",
    "\n",
    "class SourceData(object):\n",
    "    \"\"\" Load the photon data near the source and associated exposure. \n",
    "    \n",
    "    Either from:\n",
    "      1. `config.data_folder`, the Path to folder with list of pickle files with weekly or monthly data\n",
    "      2. the cache, with key `{source.name}_data`\n",
    "    \n",
    "    * `config` : basic configuration: expect `config.data_folder` to be set\n",
    "    * `source` : PointSource object\n",
    "    * `clear` : if set, overwrite the cached results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, source, clear=False):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        verbose = config.verbose\n",
    "        self.config = config\n",
    "        self.source = source\n",
    "        dname =  config.data_folder.name if config.data_folder is not None else ''\n",
    "        key = f'{source.name}_{dname}_data'\n",
    "        source.data_key = key\n",
    "        \n",
    "        if config.data_folder is None and key not in config.cache:\n",
    "            raise Exception(f'Data for {source.name} is not cached, and config.data_folder is not set')\n",
    "        \n",
    "        self.p_df, self.e_df = config.cache(key, \n",
    "                                    _load_from_weekly_data, config, source, \n",
    "                                    overwrite=clear,\n",
    "                                    description=f'photons and exposure for {source.name}')\n",
    "    def __repr__(self):\n",
    "        time = self.p_df.time.values\n",
    "        r = f'{self.__class__.__name__}: Source {self.source.name} with:'\\\n",
    "            f'\\n\\t data:     {len(self.p_df):9,} photons from   {UTC(time[0])[:10]} to {UTC(time[-1])[:10]}'\\\n",
    "            f'\\n\\t exposure: {len(self.e_df):9,} intervals from {UTC(self.e_df.iloc[0].start)[:10]}'\\\n",
    "            f' to {UTC(self.e_df.iloc[-1].stop)[:10]}'\n",
    "        return r\n",
    "    \n",
    "    def binned_exposure(self, time_bins=None):\n",
    "        \"\"\"Bin the exposure\n",
    "        \n",
    "        - time_bins: list of edges. if None, construct from exposure time limits and config.time_iterval \n",
    "        \n",
    "        returns a tuple with:\n",
    "        \n",
    "        - bexp: array of exposure integrated over each time bin, normalized to total\n",
    "        - time_bins\n",
    "\n",
    "        \"\"\"\n",
    "        config = self.config\n",
    "\n",
    "        # get exposure calculation\n",
    "        exp   = self.e_df.exp.values\n",
    "        estart= self.e_df.start.values\n",
    "        estop = self.e_df.stop.values\n",
    "\n",
    "        # determine bins, using config.time_interval, range of times\n",
    "            # default bins depends on exposure\n",
    "        if time_bins is None:\n",
    "            time_bins = get_default_bins(config, self.e_df)\n",
    "\n",
    "        #use cumulative exposure to integrate over larger periods\n",
    "        cumexp = np.concatenate(([0],np.cumsum(exp)) )\n",
    "\n",
    "        # get index into tstop array of the bin edges\n",
    "        edge_index = np.searchsorted(estop, time_bins)\n",
    "        # return the exposure integrated over the intervals\n",
    "        cum = cumexp[edge_index]\n",
    "\n",
    "        bexp = np.diff(cum)/(cum[-1]-cum[0]) * (len(time_bins)-1) \n",
    "        if config.verbose>1:\n",
    "            print(f'Relative exposure per bin:\\n{pd.Series(bexp).describe(percentiles=[])}')\n",
    "        return bexp, time_bins\n",
    "    \n",
    "    def plot(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig, (ax1,ax2, ax3,ax4) = plt.subplots(1,4, figsize=(15,4))\n",
    "        ax1.hist(self.p_df.time.values, 500, histtype='step');\n",
    "        ax1.set(xlabel='Time (MJD)')\n",
    "        ax2.hist(self.p_df.radius.values, 500, histtype='step');\n",
    "        ax2.set(xlabel='Radius (deg)');\n",
    "        \n",
    "        ax3.hist(self.p_df.band, 32, histtype='step', log=True);\n",
    "        ax3.set(xlabel='Band index')\n",
    "        ax4.hist(self.p_df.weight, 100, histtype='step')\n",
    "        ax4.set(xlabel='weight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-leisure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SourceData\" class=\"doc_header\"><code>class</code> <code>SourceData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SourceData</code>(**`config`**, **`source`**, **`clear`**=*`False`*)\n",
       "\n",
       "Load the photon data near the source and associated exposure. \n",
       "\n",
       "Either from:\n",
       "  1. `config.data_folder`, the Path to folder with list of pickle files with weekly or monthly data\n",
       "  2. the cache, with key `{source.name}_data`\n",
       "\n",
       "* [`config`](wtlike/config) : basic configuration: expect `config.data_folder` to be set\n",
       "* `source` : PointSource object\n",
       "* `clear` : if set, overwrite the cached results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SourceData.binned_exposure\" class=\"doc_header\"><code>SourceData.binned_exposure</code><a href=\"__main__.py#L105\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SourceData.binned_exposure</code>(**`time_bins`**=*`None`*)\n",
       "\n",
       "Bin the exposure\n",
       "\n",
       "- time_bins: list of edges. if None, construct from exposure time limits and config.time_iterval \n",
       "\n",
       "returns a tuple with:\n",
       "\n",
       "- bexp: array of exposure integrated over each time bin, normalized to total\n",
       "- time_bins"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "show_doc(SourceData)\n",
    "show_doc(SourceData.binned_exposure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-grounds",
   "metadata": {},
   "source": [
    "## Test extracting full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.verbose=2\n",
    "# sd = SourceData(config, source, clear=False); \n",
    "# print(sd)\n",
    "# sd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time exp, bins = sd.binned_exposure(); \n",
    "# exp.mean(), exp.std()\n",
    "\n",
    "# plt.hist(exp, 100);\n",
    "\n",
    "# sd.p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-disney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 01_effective_area.ipynb.\n",
      "Converted 02_gti.ipynb.\n",
      "Converted 02_source_data.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 04_photon_data.ipynb.\n",
      "Converted 05_weights.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_cell_data.ipynb.\n",
      "Converted 07_cells.ipynb.\n",
      "Converted 08_loglike.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 10_simulation.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted index.ipynb.\n",
      "Mon Apr 19 05:51:46 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config(verbose=2, data_folder=Path('/home/burnett/monthly'))\n",
    "# sd = SourceData(config, source, clear=True); \n",
    "# print(sd)\n",
    "# sd.plot()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_df, e_df = _load_from_weekly_data(config, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     weight_file =  check_weights(config,  source)\n",
    "#     assert weight_file is not None\n",
    "    \n",
    "#     data_folder = Path(config.data_folder)\n",
    "#     data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "#     iname = data_folder.name\n",
    "    \n",
    "#     if config.verbose>1:\n",
    "#         print(f\"Assembling photon data and exposure for source {source.name} from\"\\\n",
    "#               f' folder \"{data_folder}\", with {len(data_files)} file, last={data_files[-1].name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-transparency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
