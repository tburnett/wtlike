{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exposure\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-inside",
   "metadata": {},
   "source": [
    "# Exposure processing\n",
    "> Add exposure to each event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-preservation",
   "metadata": {},
   "source": [
    "For a given source, his module takes the exposure derived from the source direction and the spacecraft data, and a list of photons, both as DataFrames. \n",
    "It adds a column \"tau\" to the data, which is the exposure per photon in units of $\\mathrm{m^2 s}$\n",
    "\n",
    "It also identifies the runs from gaps in the exposure, and returns a set of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wtlike.config import *\n",
    "from wtlike.effective_area import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def contiguous(start, stop, min_gap=2, ):\n",
    "    assert len(start)==len(stop)\n",
    "    ssint = np.empty(2*len(start), float)\n",
    "    ssint[0::2] = start\n",
    "    ssint[1::2] = stop\n",
    "    \n",
    "    # Tag the (stpp,start) pairs < 10 sec as  not adjacent\n",
    "    not_adjacent = np.diff(ssint)[1::2] > min_gap ;\n",
    "    \n",
    "    # make a mask, keep ends\n",
    "    mask = np.empty(len(ssint), bool)\n",
    "    mask[0] = mask[-1] = True\n",
    "    # \n",
    "\n",
    "    # insert into mask -- keep only the (stop,start) pairs  which are not adjacent\n",
    "    mask[1:-2:2] = not_adjacent\n",
    "    mask[2:-1:2] = not_adjacent\n",
    "    \n",
    "    # apply mask, split into start and stop\n",
    "    keep = ssint[mask]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "teststart = np.array([0,1,2,4,6,7, 11], float)\n",
    "teststop =  np.array([1,2,4,5,7,8, 12], float)\n",
    "expect = np.array([0,8,11,12])\n",
    "assert (contiguous(teststart, teststop)==expect).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "sec_per_day = 24*3600\n",
    "\n",
    "def add_exposure_to_events(config, exposure, photons):\n",
    "    \"\"\"\n",
    "    Modifies the photon DF to add column \"tau\"\n",
    "    Returns DataFrame runs\n",
    "    \n",
    "    \"\"\"\n",
    "    # get interleaved start/stop pairs for contiguous intervals\n",
    "    estop = exposure.stop.values\n",
    "    etime = estart =exposure.start.values\n",
    "\n",
    "    run_times = contiguous(estart, estop, min_gap=10/sec_per_day)\n",
    "    fermi_start = run_times[0]\n",
    "    fermi_stop = run_times[-1]\n",
    "    if config.verbose>1:\n",
    "        print(f'Found {len(run_times)//2:,} contiguous intervals, interpret as runs'\n",
    "              f' from {UTC(fermi_start)[:10]} to {UTC(fermi_stop)[:10]}')\n",
    "\n",
    "    ### Determine run exposures\n",
    "    vexp = exposure.exp.values /1e4 # exposure per FT2 interval -- in m^2 \n",
    "    cumexp = np.insert(np.cumsum(vexp), 0,0)\n",
    "    if config.verbose>1: print(f'Total exposure: {cumexp[-1]*1e-6:.3f}  m^2 Ms')\n",
    "\n",
    "    run_index = np.searchsorted(estart, run_times)\n",
    "    run_start_exp = cumexp[run_index][0::2]\n",
    "\n",
    "#     run_exp_diff = np.diff(run_start_exp)\n",
    "\n",
    "    ### Assign Event exposure from event times\n",
    "\n",
    "    if config.verbose>1: \n",
    "        print(f'Examine {len(photons):,} photons to add exposure.')\n",
    "        print(f'Input:\\n{photons.head()}')\n",
    "    event = photons#['time weight'.split()].copy()\n",
    "    event_exposure = cumexp[np.searchsorted(estart, event.time)]\n",
    "    \n",
    "    \n",
    "    event.loc[:,'tau'] = np.diff(np.insert(event_exposure,0,0))\n",
    "    if config.verbose>2:\n",
    "        print(f'Added tau:\\n{photons.info()}')\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        dict(time=run_times[0::2], \n",
    "             exp=run_start_exp),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def generate_exposure(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-vector",
   "metadata": {},
   "source": [
    "### Test? cannot do the following since this is imported by SourceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# from wtlike.source_data import *\n",
    "# from wtlike.config import *\n",
    "# wtl = SourceData('3C 279')\n",
    "\n",
    "# exposure = wtl.exposure\n",
    "# photons = wtl.photons\n",
    "# source_name = wtl.source_name\n",
    "# verbose = wtl.config.verbose = 2\n",
    "\n",
    "# runs = add_exposure_to_events(wtl.config, wtl.exposure, wtl.photons)\n",
    "# run_start_exp = runs.exp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,3))\n",
    "\n",
    "# ax1.hist(np.diff(run_start_exp), bins = 100)\n",
    "# ax1.set(xlabel = r'Per run $\\mathrm{(m^2 s)}$')\n",
    "# ax1.grid(alpha=0.5);\n",
    "# ax = ax2\n",
    "# hkw = dict(bins = 100, histtype='step', log=True) #np.linspace(0,2000,100)\n",
    "# ax.hist(photons.tau[photons.weight>0.5], **hkw, label='weight>0.5');\n",
    "# ax.hist(photons.tau[photons.weight<0.5], **hkw, label='weight<0.5');\n",
    "# ax.legend(fontsize=12)\n",
    "# ax.set(xlabel=r'Per event, $\\mathrm{(m^2 s)}$', ylim=(5,None))\n",
    "# ax.grid(alpha=0.5);\n",
    "# fig.suptitle(f'{source_name} exposure analysis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Thu Jun  3 06:49:54 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-retail",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
