{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exposure\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-bruce",
   "metadata": {},
   "source": [
    "# Exposure processing\n",
    "> Add exposure to each event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-disco",
   "metadata": {},
   "source": [
    "For a given source, his module takes the exposure derived from the source direction and the spacecraft data, and a list of photons, both as DataFrames. \n",
    "It adds a column \"tau\" to the data, which is the exposure per photon in units of $\\mathrm{m^2 s}$\n",
    "\n",
    "It also identifies the runs from gaps in the exposure, and returns a set of runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def contiguous(start, stop, min_gap=2, ):\n",
    "    assert len(start)==len(stop)\n",
    "    ssint = np.empty(2*len(start), float)\n",
    "    ssint[0::2] = start\n",
    "    ssint[1::2] = stop\n",
    "    \n",
    "    # Tag the (stpp,start) pairs < 10 sec as  not adjacent\n",
    "    not_adjacent = np.diff(ssint)[1::2] > min_gap ;\n",
    "    \n",
    "    # make a mask, keep ends\n",
    "    mask = np.empty(len(ssint), bool)\n",
    "    mask[0] = mask[-1] = True\n",
    "    # \n",
    "\n",
    "    # insert into mask -- keep only the (stop,start) pairs  which are not adjacent\n",
    "    mask[1:-2:2] = not_adjacent\n",
    "    mask[2:-1:2] = not_adjacent\n",
    "    \n",
    "    # apply mask, split into start and stop\n",
    "    keep = ssint[mask]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "teststart = np.array([0,1,2,4,6,7, 11], float)\n",
    "teststop =  np.array([1,2,4,5,7,8, 12], float)\n",
    "expect = np.array([0,8,11,12])\n",
    "assert (contiguous(teststart, teststop)==expect).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "sec_per_day = 24*3600\n",
    "def add_exposure_to_events(config, exposure, photons):\n",
    "    \"\"\"\n",
    "    Modifies photons to add column \"tau\"\n",
    "    Returns DataFrame runs\n",
    "    \n",
    "    \"\"\"\n",
    "    # get interleaved start/stop pairs for contiguous intervals\n",
    "    estop = exposure.stop.values\n",
    "    etime = estart =exposure.start.values\n",
    "\n",
    "    run_times = contiguous(estart, estop, min_gap=10/sec_per_day)\n",
    "    fermi_start = run_times[0]\n",
    "    fermi_stop = run_times[-1]\n",
    "    if config.verbose>1:\n",
    "        print(f'Found {len(run_times)//2:,} contiguous intervals, interpret as runs'\n",
    "              f' from {UTC(fermi_start)[:10]} to {UTC(fermi_stop)[:10]}')\n",
    "\n",
    "    ### Determine run exposures\n",
    "    vexp = exposure.exp.values /1e4 # exposure per FT2 interval -- in m^2 \n",
    "    cumexp = np.insert(np.cumsum(vexp), 0,0)\n",
    "    if config.verbose>1: print(f'Total exposure: {cumexp[-1]*1e-6:.1f}  m^2 Ms')\n",
    "\n",
    "    run_index = np.searchsorted(estart, run_times)\n",
    "    run_start_exp = cumexp[run_index][0::2]\n",
    "\n",
    "#     run_exp_diff = np.diff(run_start_exp)\n",
    "\n",
    "    ### Assign Event exposure from event times\n",
    "\n",
    "    if config.verbose>1: print(f'Examine {len(photons):,} photons.')\n",
    "\n",
    "    event = photons#['time weight'.split()].copy()\n",
    "    event_exposure = cumexp[np.searchsorted(estart, event.time)]\n",
    "    event.loc[:,'tau'] = np.diff(np.insert(event_exposure,0,0))\n",
    "    return pd.DataFrame(dict(time=run_times[0::2], exp=run_start_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-engagement",
   "metadata": {},
   "source": [
    "### Test? cannot do the following since this is imported by SourceData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# from wtlike.source_data import *\n",
    "# from wtlike.config import *\n",
    "# wtl = SourceData('3C 279')\n",
    "\n",
    "# exposure = wtl.exposure\n",
    "# photons = wtl.photons\n",
    "# source_name = wtl.source_name\n",
    "# verbose = wtl.config.verbose = 2\n",
    "\n",
    "# runs = add_exposure_to_events(wtl.config, wtl.exposure, wtl.photons)\n",
    "# run_start_exp = runs.exp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,3))\n",
    "\n",
    "# ax1.hist(np.diff(run_start_exp), bins = 100)\n",
    "# ax1.set(xlabel = r'Per run $\\mathrm{(m^2 s)}$')\n",
    "# ax1.grid(alpha=0.5);\n",
    "# ax = ax2\n",
    "# hkw = dict(bins = 100, histtype='step', log=True) #np.linspace(0,2000,100)\n",
    "# ax.hist(photons.tau[photons.weight>0.5], **hkw, label='weight>0.5');\n",
    "# ax.hist(photons.tau[photons.weight<0.5], **hkw, label='weight<0.5');\n",
    "# ax.legend(fontsize=12)\n",
    "# ax.set(xlabel=r'Per event, $\\mathrm{(m^2 s)}$', ylim=(5,None))\n",
    "# ax.grid(alpha=0.5);\n",
    "# fig.suptitle(f'{source_name} exposure analysis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90-main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sun May 16 04:55:58 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-dayton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-glance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
