{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp cell_data\n",
    "from nbdev import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-programming",
   "metadata": {},
   "source": [
    "# Manage cell data\n",
    "> Create cells from source data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-oxygen",
   "metadata": {},
   "source": [
    "## Creating cells\n",
    "\n",
    "As set out in Kerr, we choose binning in time to group photons into \"cells\". For each cell, we determine its likelihood function, which is then used to estimate the signal rate for that duration.\n",
    "\n",
    "Eqn. 2 of Kerr shows the log likelihood as a sum of cells of the log likelihoods of the cells, where the $k^{th}$ cell's likelihood,  (with $\\beta=0$), is\n",
    "\n",
    "$$ \\displaystyle\\log\\mathcal{L}_k(\\alpha)\\ = \\sum_{w}  \\log \\big( 1 + \\alpha\\ w \\big) - \\alpha\\ f_k\\ S   $$\n",
    "\n",
    "The sum is over the weights $w$ in that cell, and $S$ is an estimate of the signal rate, and $f_k$ is the total exposure for that $k^{th}$ cell. Note that $f_k$ can be expressed as the sum of the exposure per photon, accumulated over the live time, that from the start of the run or the previous photon. \n",
    "\n",
    "*Fermi* data is broken into distinct runs, corresponding at most to a single orbit. The exposure rate, $\\frac{df}{dt}$ varies by a factor of 2 or 3 for each 90-min orbit, is typically  $ 3 000\\ \\mathrm{cm}^2$, or $0.3\\ \\mathrm{m^2}$ for $fermi$. This is measured in 30-s intervals.\n",
    "\n",
    "We'll call the exposure per photon $\\tau$ to emphasize that it plays the role of time. \n",
    "\n",
    " So for the $i^{th}$ photon,   \n",
    "$$\\displaystyle\\log\\mathcal{L}_i(\\alpha) = \\log( 1 + \\alpha\\ w_i) - \\alpha\\  \\tau_i\\ S  $$\n",
    "\n",
    "Note that this corresponds to an exponential distribution in $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from wtlike.config import *\n",
    "from wtlike.source_data import *\n",
    "from wtlike.loglike import LogLike, PoissonRep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CellData(SourceData):\n",
    "    \"\"\"Manage a set of cells generated from a data set\n",
    "    \n",
    "        Invoke superclass to load photon data and exposure for the source.\n",
    " \n",
    "        * time_bins, default config.time_bins\n",
    "        \n",
    "        \n",
    "        Note that the `e`  cell entry is the actual exposure for the cell in units $cm^2\\ s$, times $10^{-6}$.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, *pars, **kwargs): \n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        bins = kwargs.pop('bins', kwargs.pop('time_bins', Config().time_bins))\n",
    "        #  load source data\n",
    "        super().__init__(*pars, **kwargs )\n",
    "\n",
    "\n",
    "        self.rebin(bins)\n",
    "        self.parent = None\n",
    "\n",
    "    def rebin(self, newbins):\n",
    "        \"\"\"bin, or rebin \n",
    "        \"\"\"\n",
    "        photon_data = self.photons\n",
    "        self.cell_edges = edges = time_bin_edges(self.config, self.exposure, newbins)\n",
    "        if self.config.verbose>0:\n",
    "            step = newbins[2]\n",
    "            self.step_name = 'orbit-based' if step<=0 else bin_size_name(step)\n",
    "            print(f'CellData: Bin photon data into {int(len(edges)/2)} {self.step_name}'\\\n",
    "                  f' bins from {edges[0]:.1f} to {edges[-1]:.1f}')\n",
    "        \n",
    "        # note need to take care of interleave\n",
    "        self.binexp = self.binned_exposure( edges ) [0::2] \n",
    "\n",
    "        \n",
    "#         #self.fexposure=(expose/self.exptot).astype(np.float32)\n",
    "#         self.fexposure = expose \n",
    "        \n",
    "        self.get_cells()\n",
    "        \n",
    "    def get_cells(self, exposure_factor=1e-6):\n",
    "        \"\"\"\n",
    "        Generate the cell DataFrame\n",
    "        \n",
    "        - exposure_factor --  recast exposure as cm^2 * Ms if $10^{-6}$ \n",
    "        \n",
    "        Thus the `e`  cell entry is the actual exposure for the cell in units $cm^2\\ Ms$.\n",
    "        \"\"\"\n",
    "        # restrict photons to range of bin times\n",
    "        photons = self.photons.query(f'{self.cell_edges[0]}<time<{self.cell_edges[-1]}')\n",
    "        \n",
    "        # use photon times to get indices into photon list\n",
    "        edges = np.searchsorted(photons.time, self.cell_edges)\n",
    "        \n",
    "        wts = photons.weight.values\n",
    "        start,stop = self.cell_edges[0::2], self.cell_edges[1::2]\n",
    "        center = (start+stop)/2\n",
    "        width = (stop-start)\n",
    "        cells = []\n",
    "        ek = np.append(edges[0::2], edges[-1])\n",
    "        etot = self.exptot*exposure_factor\n",
    "\n",
    "        Sk, Bk = self.S/etot, self.B/etot\n",
    "\n",
    "        for k, (t, tw, e) in enumerate( zip(\n",
    "                    center, width, self.binexp*exposure_factor) ):\n",
    "            w = wts[ek[k]:ek[k+1]] \n",
    "            n = len(w)\n",
    "            cells.append(dict(t=t, tw=tw, \n",
    "                              e=e,\n",
    "                              n=n,\n",
    "                              w=w,\n",
    "                              S=e*Sk,\n",
    "                              B=e*Bk,\n",
    "                             )\n",
    "                        )\n",
    "        self.cells =  pd.DataFrame(cells)\n",
    "        return self.cells\n",
    "\n",
    "    def update(self): pass # virtual\n",
    "    \n",
    "    def view(self, newbins=None):\n",
    "        \"\"\"Return a \"view\": a new instance of this class with a perhaps a different set of cells\n",
    "        \n",
    "        - newbins -- a tuple (start, stop, step) to define new binning.\n",
    "          - start and stop are either MJD values, or offsets from the start or stop.\n",
    "          - step -- the cell size in days, or if zero, orbit-based binning\n",
    "        \"\"\"\n",
    "        import copy\n",
    "        if self.config.verbose>1:\n",
    "            print(f'Making a view of the class {self.__class__}')\n",
    "        r = copy.copy(self)\n",
    "\n",
    "        if newbins is not None:\n",
    "            r.rebin(newbins)\n",
    "        r.parent = self\n",
    "        r.update()\n",
    "        return r\n",
    "        \n",
    "#### needs fixxing    \n",
    "#     def __repr__(self):\n",
    "#         return f'''{self.__class__}:\n",
    "#         {len(self.fexposure)} intervals from {self.cell_edges[0]:.1f} to {self.cell_edges[-1]:.1f} for source {self.source_name}\n",
    "#         S {self.S:.2f}  B {self.B:.2f} '''\n",
    "\n",
    "    \n",
    "    def concatenate( self ):\n",
    "        \"\"\"\n",
    "        Combine this set of cells to one\n",
    "        Return a dict with summed n, S, B, and concatenated w\n",
    "        \"\"\"\n",
    "\n",
    "        cells = self.cells\n",
    "        \n",
    "        newcell = dict()\n",
    "\n",
    "        if 't' in cells:\n",
    "            ca, cb =cells.iloc[0], cells.iloc[-1]\n",
    "            newcell.update(dict(t= 0.5*(ca.t-ca.tw/2 + cb.t+cb.tw/2), tw=cb.t-ca.t ))\n",
    "\n",
    "        for col in ' n S B'.split():\n",
    "            newcell[col] = cells[col].sum()\n",
    "        newcell['w'] = np.concatenate(list(cells.w.values))\n",
    "        return newcell\n",
    "    \n",
    "        \n",
    "    def full_likelihood(self ):\n",
    "        \"\"\"Concatentate all the cells, return a LogLike object\n",
    "        \"\"\"\n",
    "        return LogLike(self.concatenate()) \n",
    "    \n",
    "    def plot_concatenated(self, fignum=1, **kwargs):\n",
    "        \"\"\"Likelihood function, with fit for concatenated data\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        lka = self.full_likelihood()\n",
    "        fig,ax = plt.subplots(figsize=(4,2), num=fignum)\n",
    "        lka.plot(ax=ax, **kwargs) \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# cd = CellData('Geminga')\n",
    "\n",
    "# cd.plot_concatenated(xlim=(0.99, 1.01), title=f'{cd.source.name}');\n",
    "# print('Parmeters from Poisson fit')\n",
    "# L = cd.full_likelihood()\n",
    "# pr = PoissonRep(L)\n",
    "# print(pd.Series(pr.info()))\n",
    "\n",
    "# (cd.cells.n/cd.cells.e).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-coffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"CellData\" class=\"doc_header\"><code>class</code> <code>CellData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>CellData</code>(**\\*`pars`**, **\\*\\*`kwargs`**) :: [`SourceData`](/wtlikesource_data#SourceData)\n",
       "\n",
       "Manage a set of cells generated from a data set\n",
       "\n",
       "Invoke superclass to load photon data and exposure for the source.\n",
       "\n",
       "* time_bins, default config.time_bins\n",
       "\n",
       "\n",
       "Note that the `e`  cell entry is the actual exposure for the cell in units $cm^2\\ s$, times $10^{-6}$."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CellData.view\" class=\"doc_header\"><code>CellData.view</code><a href=\"__main__.py#L88\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CellData.view</code>(**`newbins`**=*`None`*)\n",
       "\n",
       "Return a \"view\": a new instance of this class with a perhaps a different set of cells\n",
       "\n",
       "- newbins -- a tuple (start, stop, step) to define new binning.\n",
       "  - start and stop are either MJD values, or offsets from the start or stop.\n",
       "  - step -- the cell size in days, or if zero, orbit-based binning"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CellData)\n",
    "show_doc(CellData.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def concatenate_cells( cells):\n",
    "    \"\"\"\n",
    "    Combine a group of cells to one\n",
    "    - cells: dataframe with cells containing  n, w, S, B<br>\n",
    "            Optionally, if $t$ is present, generate t and tw\n",
    "    Return a dict with summed n, S, B, and concatenated w\n",
    "    \"\"\"\n",
    "    newcell = dict()\n",
    "    if 't' in cells:\n",
    "        ca, cb =cells.iloc[0], cells.iloc[-1]\n",
    "        newcell.update(dict(t= 0.5*(ca.t-ca.tw/2 + cb.t+cb.tw/2), tw=cb.t-ca.t ))\n",
    "\n",
    "    for col in ' n S B'.split():\n",
    "        newcell[col] = cells[col].sum()\n",
    "    newcell['w'] = np.concatenate(list(cells.w.values))\n",
    "    return newcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def partition_cells(config, cells, edges):\n",
    "    \"\"\" Partition a set of cells\n",
    "     - cells -- A DataFrame of cells\n",
    "     - edges  -- a list of edge times delimiting boundaries between cells\n",
    "     \n",
    "    Returns a DataFrame of combined cells, with times and widths adjusted to account for missing cells\n",
    "    \n",
    "    \"\"\"\n",
    "    # get indices of  cell idexes just beyond each edge time\n",
    "    ii = np.searchsorted(cells.t, edges)\n",
    "    \n",
    "    # Get the appropriate boundary times to apply to combined cells\n",
    "    # this is complicated by missing cells, need to put boundary in gaps if ncessary\n",
    "    ileft = ii[:-1]\n",
    "    cleft = cells.iloc[ileft ]\n",
    "    tleft =  (cleft.t - cleft.tw/2).values\n",
    "    iright = ii[1:]-1\n",
    "    cright = cells.iloc[iright ]  \n",
    "    tright = (cright.t+cright.tw/2).values\n",
    "    betweens = 0.5*(tleft[1:] + tright[:-1])\n",
    "    tboundary = np.append(np.insert(betweens, 0, tleft[0]), tright[-1])\n",
    "    \n",
    "    # now combine the cells, \n",
    "    newcells = []\n",
    "    for k in range(len(ii)-1):\n",
    "        a,b = ii[k:k+2]\n",
    "        subset = cells.iloc[a:b]; \n",
    "\n",
    "#         ca, cb = subset.iloc[0], subset.iloc[-1]\n",
    "#         newcell = dict(t= 0.5*(ca.t-ca.tw/2 + cb.t+cb.tw/2)  )\n",
    "        tl, tr = tboundary[k:k+2]\n",
    "        newcell = dict(t=0.5*(tl+tr), tw=tr-tl)\n",
    "        \n",
    "        for col in 'e n S B'.split():\n",
    "            newcell[col] = subset[col].sum()\n",
    "        newcell['e'] /= len(subset)\n",
    "        newcell['w'] = np.concatenate(list(subset.w.values)) #np.array(w, np.uint8)\n",
    "        newcells.append(newcell)\n",
    "    return pd.DataFrame(newcells)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-sociology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"partition_cells\" class=\"doc_header\"><code>partition_cells</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>partition_cells</code>(**`config`**, **`cells`**, **`edges`**)\n",
       "\n",
       "Partition a set of cells\n",
       " - cells -- A DataFrame of cells\n",
       " - edges  -- a list of edge times delimiting boundaries between cells\n",
       " \n",
       "Returns a DataFrame of combined cells, with times and widths adjusted to account for missing cells"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(partition_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-rehabilitation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90-main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Mon May 10 17:22:19 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-component",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
