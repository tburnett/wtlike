{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp config\n",
    "\n",
    "from nbdev.showdoc import show_doc\n",
    "from utilities.ipynb_docgen import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration data and basic functions\n",
    "> Basic functions and configuration stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements:\n",
    "\n",
    "- Cache\n",
    "- Config\n",
    "- MJD\n",
    "- UTC, UTCnow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os, sys, warnings, pickle\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class Cache(dict):\n",
    "    \"\"\"\n",
    "    Manage a file cache\n",
    "\n",
    "    - `path` -- string or `filepath` object <br> This is the folder where the index and data files are saved.\n",
    "    - `clear` -- set True to clear the cache on initialization\n",
    "\n",
    "    This uses pickle to save objects, associated with a hashable key, which is used to index the\n",
    "    filename in a file `index.pkl` in the same folder.\n",
    "\n",
    "    The `__call__` function is a convenient way to use it, so one call may either store a new entry or retrieve an existing one.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, clear:bool=False):\n",
    "\n",
    "        self.path = Path(path) if path else None\n",
    "        if self.path is None: return\n",
    "        if not self.path.exists() :\n",
    "            print(f'Warning: cache Path {self.path} does not exist, cache disabled ',file=sys.stderr)\n",
    "            self.path=None\n",
    "            return\n",
    "\n",
    "        self.index_file = self.path/'index.pkl'\n",
    "\n",
    "        if self.path.exists():\n",
    "            if clear:\n",
    "                print('Clearing cache!')\n",
    "                self.clear()\n",
    "            else:\n",
    "                self._load_index()\n",
    "\n",
    "    def _dump_index(self):\n",
    "        with open(self.index_file, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "\n",
    "    def _load_index(self):\n",
    "        if not self.index_file.exists():\n",
    "            self._dump_index()\n",
    "            return\n",
    "        with open(self.index_file, 'rb') as file:\n",
    "            self.update(pickle.load(file))\n",
    "\n",
    "    def add(self, key, object,  exist_ok=False):\n",
    "        if not self.path: return\n",
    "        assert type(key)==str, f'Expect key to be a string, got {key}'\n",
    "        if key  in self:\n",
    "            if not exist_ok:\n",
    "                print(f'Warning: cached object for key \"{key}\" exists', file=sys.stderr)\n",
    "            filename = self[key]\n",
    "        else:\n",
    "            filename = f'cache_file_{hex(key.__hash__())[3:]}.pkl'\n",
    "            self[key] = filename\n",
    "            self._dump_index()\n",
    "\n",
    "        with open(self.path/filename, 'wb') as file:\n",
    "            pickle.dump(object, file )\n",
    "\n",
    "\n",
    "    def get(self, key):\n",
    "        if key not in self:\n",
    "            return None\n",
    "        filename = self[key]\n",
    "        # temp fix\n",
    "        if str(filename)[0]=='/': filename=self.path/filename.name\n",
    "        \n",
    "        if not (self.path/filename).exists():\n",
    "            # perhaps deleted by another instance?\n",
    "            print(f'File for Cache key {key} not found, removing entry', file='sys.stderr')\n",
    "            selt.pop(key)\n",
    "            return None\n",
    "        with open(self.path/filename, 'rb') as file:\n",
    "            ret = pickle.load(file)\n",
    "        return ret\n",
    "\n",
    "    def clear(self):\n",
    "        if not self.path: return\n",
    "        for f in self.path.iterdir():\n",
    "            if f.is_file:\n",
    "                f.unlink()\n",
    "        super().clear()\n",
    "\n",
    "        self._dump_index()\n",
    "\n",
    "    def remove(self, key):\n",
    "        \"\"\"remove entry and associated file\"\"\"\n",
    "        if not self.path: return\n",
    "        if key not in self:\n",
    "            print(f'Cache: key {key} not found', file=sys.stderr)\n",
    "            return\n",
    "        filename = self.path/self[key]\n",
    "        try:\n",
    "            filename.unlink()\n",
    "        except:\n",
    "            print(f'Failed to unlink file {filename}', file=sys.stderr)\n",
    "            raise\n",
    "        super().pop(key)\n",
    "        self._dump_index()\n",
    "\n",
    "\n",
    "    def __call__(self, key, func, *pars, description='', overwrite=False, **kwargs,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        One-line usage interface for cache use\n",
    "\n",
    "        - `key` -- key to use, usually a string. Must be hashable <br>\n",
    "            If None, ignore cache and return the function evaluation\n",
    "        - `func` -- user function that will return an object that can be pickled\n",
    "        - `pars`, `kwargs` -- pass to `func`\n",
    "        - `description` -- optional string that will be printed\n",
    "        - `overwrite` -- if set, overwrite previous entry if exists\n",
    "\n",
    "        Example:\n",
    "        <pre>\n",
    "        mycache = Cache('/tmp/thecache', clear=True)\n",
    "\n",
    "        def myfun(x):\n",
    "            return x\n",
    "\n",
    "        result = mycache('mykey', myfun, x=99,  description='My data')\n",
    "\n",
    "        </pre>\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if key is None or self.path is None:\n",
    "            return func(*pars, **kwargs)\n",
    "\n",
    "\n",
    "        if description:\n",
    "            print(f'{description}: {\"Saving to\" if key not in self or overwrite else \"Restoring from\"} cache', end='')\n",
    "            print('' if key == description else f' with key \"{key}\"')\n",
    "        ret = self.get(key)\n",
    "        if ret is None or overwrite:\n",
    "            ret = func(*pars, **kwargs)\n",
    "            self.add(key, ret, exist_ok=overwrite)\n",
    "        return ret\n",
    "\n",
    "    def show(self, starts_with=''):\n",
    "        import datetime\n",
    "        if not self.path: return 'Cache not enabled'\n",
    "        if len(self.items())==0: return f'Cache at {self.path} is empty\\n'\n",
    "        title = 'Cache contents' if not starts_with else f'Cache entries starting with {starts_with}'\n",
    "        s = f'{title}\\n {\"key\":30}   {\"size\":>10}  {\"time\":20} {\"name\"}, folder {self.path}\\n'\n",
    "        for name, value in self.items():\n",
    "            # temporary: override file's path if set \n",
    "            if str(value)[0]=='/':\n",
    "                value = self.path/value.name\n",
    "            if name is None or not name.startswith(starts_with) : continue\n",
    "            try:\n",
    "                stat = (self.path/value).stat()\n",
    "                size = stat.st_size\n",
    "                mtime= str(datetime.datetime.fromtimestamp(stat.st_mtime))[:16]\n",
    "                s += f'  {name:30s}  {size:10}  {mtime:20} {value}\\n'\n",
    "            except Exception as msg:\n",
    "                s += f'{name} -- file not found\\n'\n",
    "                raise\n",
    "        return s\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Cache.__call__\" class=\"doc_header\"><code>Cache.__call__</code><a href=\"__main__.py#L103\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Cache.__call__</code>(**`key`**, **`func`**, **\\*`pars`**, **`description`**=*`''`*, **`overwrite`**=*`False`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "One-line usage interface for cache use\n",
       "\n",
       "- `key` -- key to use, usually a string. Must be hashable <br>\n",
       "    If None, ignore cache and return the function evaluation\n",
       "- `func` -- user function that will return an object that can be pickled\n",
       "- `pars`, `kwargs` -- pass to `func`\n",
       "- `description` -- optional string that will be printed\n",
       "- `overwrite` -- if set, overwrite previous entry if exists\n",
       "\n",
       "Example:\n",
       "<pre>\n",
       "mycache = Cache('/tmp/thecache', clear=True)\n",
       "\n",
       "def myfun(x):\n",
       "    return x\n",
       "\n",
       "result = mycache('mykey', myfun, x=99,  description='My data')\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Cache.__call__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing cache!\n",
      "Test: Saving to cache with key \"four\"\n",
      "Test: Restoring from cache with key \"four\"\n",
      "Before remove:\n",
      "Cache contents\n",
      " key                                    size  time                 name, folder /tmp/cache_test\n",
      "  one                                     18  2022-07-16 04:47     cache_file_a914b484ec77559.pkl\n",
      "  two                                     18  2022-07-16 04:47     cache_file_252f4e04c4c9e39.pkl\n",
      "  four                                    23  2022-07-16 04:47     cache_file_6306a9b495235f4.pkl\n",
      "\n",
      "After remove:\n",
      "Cache contents\n",
      " key                                    size  time                 name, folder /tmp/cache_test\n",
      "  one                                     18  2022-07-16 04:47     cache_file_a914b484ec77559.pkl\n",
      "  two                                     18  2022-07-16 04:47     cache_file_252f4e04c4c9e39.pkl\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: cached object for key \"two\" exists\n"
     ]
    }
   ],
   "source": [
    "#collapse_hide\n",
    "def cache_test(path):\n",
    "    c = Cache(path, clear=True)\n",
    "\n",
    "    # simmple interface\n",
    "    c.add('one', 'one');\n",
    "    c.add('two', 'two')\n",
    "    c.add('two', 'two') # getnerates warning\n",
    "    if path is not None:\n",
    "        assert c.get('two') == 'two'\n",
    "\n",
    "    # test function interface\n",
    "    func = lambda x:f'value: {x}'\n",
    "    \n",
    "    r1 = c('four',  func,  4, description='Test')\n",
    "    r2 = c('four',  func,  5,  description='Test') #should not get called\n",
    "    assert c.path is None or r1==r2, f'{r1}, {r2}'\n",
    "    \n",
    "    # remaving an entry\n",
    "    print(f'Before remove:\\n{c}')\n",
    "    assert 'four' in c\n",
    "    c.remove('four')\n",
    "    assert 'four' not in c\n",
    "    print(f'After remove:\\n{c}')\n",
    "    c.clear()\n",
    "\n",
    "test_path = Path('/tmp/cache_test')\n",
    "test_path.mkdir(exist_ok=True)\n",
    "cache_test(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Config():\n",
    "    defaults=\\\n",
    "    \"\"\"\n",
    "        verbose         : 1 # set to zero for no output\n",
    "        warnings        : ignore # \n",
    "\n",
    "        datapath        : ~/wtlike-data # where to find data--must be set\n",
    "        cachepath       : ~/.cache/wtlike #\n",
    "\n",
    "        # Expect 4FGL FITS file, e.g.,  gll_psc_v28.fit\n",
    "        catalog_file    : \n",
    "        \n",
    "        # multiprocessing\n",
    "        pool_size       : 1 # number of pool processes to use\n",
    "\n",
    "        # data cuts, processing\n",
    "        radius          : 4\n",
    "        cos_theta_max   : 0.4\n",
    "        z_max           : 100\n",
    "        offset_size     : 2.e-06  # scale factor used for event time\n",
    "\n",
    "        # binning -- actually determined by weight run\n",
    "        energy_edge_pars : [2,6,17] # pars for np.logspace\n",
    "        etypes          : [0, 1] # front, back\n",
    "        nside           : 1024\n",
    "        nest            : True\n",
    "\n",
    "        # data selection for cell creation\n",
    "        week_range      : []  # default all weeks found\n",
    "        time_bins       : [0, 0, 7] # full MJD range, 7-day cells\n",
    "        exp_min         : 5    # threshold for exposure per day, in cm^2 Ms units.\n",
    "\n",
    "        # cell fitting\n",
    "        use_kerr        : True  # Use the Kerr power-law exposure weighting\n",
    "        likelihood_rep  : poisson\n",
    "        poisson_tolerance : 0.2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        import yaml, warnings\n",
    "        from yaml import SafeLoader\n",
    "\n",
    "        # parameters: first defaults, then from ~/.config/wtlike/config.yaml, then kwars\n",
    "        pars = yaml.load(self.defaults, Loader=SafeLoader)\n",
    "        dp = Path('~/.config/wtlike/config.yaml').expanduser()\n",
    "        if dp.is_file():\n",
    "            with open(dp, 'r') as inp:\n",
    "                userpars = yaml.load(inp, Loader=SafeLoader)\n",
    "            pars.update(userpars)\n",
    "            #print(f'update from user file {dp}: {userpars}')\n",
    "        pars.update(kwargs)\n",
    "\n",
    "        self.__dict__.update(pars)\n",
    "\n",
    "        # set warnings filter if requested\n",
    "        if self.warnings is not None and self.warnings != 'None':\n",
    "            #print(f'*** set warnings filter to \"{self.warnings}\" ***')\n",
    "            warnings.simplefilter(self.warnings)\n",
    "\n",
    "        self.energy_edges = ee=np.logspace(*self.energy_edge_pars)\n",
    "        self.energy_bins = np.sqrt(ee[1:] * ee[:-1])\n",
    "        if not self.week_range:\n",
    "            self.week_range = (None, None)\n",
    "\n",
    "       # set up, check files paths\n",
    "        self.error_msg=''\n",
    "        if self.datapath is None:\n",
    "            self.error_msg+='\\ndatapath must be set to a folder with wtlike data'\n",
    "            \n",
    "        else:\n",
    "            self.datapath = df = Path(self.datapath).expanduser()\n",
    "            if not (self.datapath.is_dir() or  self.datapath.is_symlink()):\n",
    "                self.error_msg+=f'\\ndata_folder \"{df}\" is not a directory or symlink'\n",
    "            subs = 'aeff_files weight_files data_files'.split()\n",
    "            if self.error_msg=='':\n",
    "                for sub in subs:\n",
    "                    if not ( (df/sub).is_dir() or  (df/sub).is_symlink()) :\n",
    "                        self.error_msg+=f'\\n{df/sub} is not a directory or symlink'\n",
    "\n",
    "        self.cachepath =  Path(self.cachepath).expanduser()\n",
    "        os.makedirs(self.cachepath, exist_ok=True)\n",
    "        if not self.cachepath.is_dir():\n",
    "            self.error_msg +=f'cachepath {self.cachepath} is not a folder.'\n",
    "            \n",
    "        # look for 4FGL catalog file, gll_psc_v28.fit currently\n",
    "        fail = False\n",
    "        if self.catalog_file is None or self.datapath is None:\n",
    "            t = Path(self.datapath).expanduser()\n",
    "            u = sorted(list(t.glob('gll_psc_v*.fit')))\n",
    "            if len(u)>0:\n",
    "                self.catalog_file = u[-1]\n",
    "            else: \n",
    "                fail = True\n",
    "        elif Path(self.catalog_file).expanduser().is_file():\n",
    "            self.catalog_file = Path(config.catalog_file).expanduser()\n",
    "        else: fail=True\n",
    "        \n",
    "        if fail:\n",
    "            warnings.warn('There is no link to 4FGL catalog file: set \"catalog_file\" in your config.yaml'\n",
    "                  ' or specify if in the Config() call')\n",
    "\n",
    "\n",
    "    @property\n",
    "    def cache(self):\n",
    "        if not hasattr(self, '_cache'):\n",
    "            self._cache = Cache(self.cachepath, clear=False)\n",
    "        return self._cache\n",
    "\n",
    "    @property\n",
    "    def valid(self):\n",
    "        if len(self.error_msg)==0: return True\n",
    "        print(f'wtlike configuration is invalid:\\n{self.error_msg}',file=sys.stderr)\n",
    "        return False\n",
    "\n",
    "    def __str__(self):\n",
    "        s = 'Configuration parameters \\n'\n",
    "        for name, value in self.__dict__.items():\n",
    "            if name=='files' or name.startswith('_'): continue\n",
    "            s += f'  {name:15s} : {value}\\n'\n",
    "        return s\n",
    "\n",
    "    def __repr__(self): return str(self)\n",
    "    def get(self, *pars): return self.__dict__.get(*pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"Config\" class=\"doc_header\"><code>class</code> <code>Config</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>Config</code>(**\\*\\*`kwargs`**)\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### config.Config -- parameters are from three sources:\n",
       "- defaults\n",
       "- the file `~/.config/wtlike/config.yaml` if it exists\n",
       "- keyword args in Config constructor. For example, to suppress all printout:\n",
       "    ```\n",
       "config = Config(verbose=0)\n",
       "```\n",
       "\n",
       "##### Config defaults\n",
       "This is yaml-format, corresponding to `config.yaml`.\n",
       "\n",
       "        verbose         : 1 # set to zero for no output\n",
       "        warnings        : ignore # \n",
       "\n",
       "        datapath        : ~/wtlike-data # where to find data--must be set\n",
       "        cachepath       : ~/.cache/wtlike #\n",
       "\n",
       "        # Expect 4FGL FITS file, e.g.,  gll_psc_v28.fit\n",
       "        catalog_file    : \n",
       "        \n",
       "        # multiprocessing\n",
       "        pool_size       : 1 # number of pool processes to use\n",
       "\n",
       "        # data cuts, processing\n",
       "        radius          : 4\n",
       "        cos_theta_max   : 0.4\n",
       "        z_max           : 100\n",
       "        offset_size     : 2.e-06  # scale factor used for event time\n",
       "\n",
       "        # binning -- actually determined by weight run\n",
       "        energy_edge_pars : [2,6,17] # pars for np.logspace\n",
       "        etypes          : [0, 1] # front, back\n",
       "        nside           : 1024\n",
       "        nest            : True\n",
       "\n",
       "        # data selection for cell creation\n",
       "        week_range      : []  # default all weeks found\n",
       "        time_bins       : [0, 0, 7] # full MJD range, 7-day cells\n",
       "        exp_min         : 5    # threshold for exposure per day, in cm^2 Ms units.\n",
       "\n",
       "        # cell fitting\n",
       "        use_kerr        : True  # Use the Kerr power-law exposure weighting\n",
       "        likelihood_rep  : poisson\n",
       "        poisson_tolerance : 0.2\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "#### Config contents as set up here\n",
       "<details  class=\"nbdoc-description\" >  <summary> config parameter list </summary>  <div style=\"margin-left: 5%;\"><pre>Configuration parameters <br>  verbose         : 1<br>  warnings        : None<br>  datapath        : /mnt/c/Users/thbur/onedrive/fermi/wtlike-data<br>  cachepath       : /home/burnett/.cache/wtlike<br>  catalog_file    : /mnt/c/Users/thbur/onedrive/fermi/wtlike-data/gll_psc_v28.fit<br>  pool_size       : 4<br>  radius          : 4<br>  cos_theta_max   : 0.4<br>  z_max           : 100<br>  offset_size     : 2e-06<br>  energy_edge_pars : [2, 6, 17]<br>  etypes          : [0, 1]<br>  nside           : 1024<br>  nest            : True<br>  week_range      : (None, None)<br>  time_bins       : [0, 0, 7]<br>  exp_min         : 5<br>  use_kerr        : True<br>  likelihood_rep  : poisson<br>  poisson_tolerance : 0.2<br>  keep_pixels     : True<br>  energy_edges    : [1.00000000e+02 1.77827941e+02 3.16227766e+02 5.62341325e+02<br> 1.00000000e+03 1.77827941e+03 3.16227766e+03 5.62341325e+03<br> 1.00000000e+04 1.77827941e+04 3.16227766e+04 5.62341325e+04<br> 1.00000000e+05 1.77827941e+05 3.16227766e+05 5.62341325e+05<br> 1.00000000e+06]<br>  energy_bins     : [1.33352143e+02 2.37137371e+02 4.21696503e+02 7.49894209e+02<br> 1.33352143e+03 2.37137371e+03 4.21696503e+03 7.49894209e+03<br> 1.33352143e+04 2.37137371e+04 4.21696503e+04 7.49894209e+04<br> 1.33352143e+05 2.37137371e+05 4.21696503e+05 7.49894209e+05]<br>  error_msg       : <br></pre></div> </details>   \n",
       "\n",
       "\n",
       "\n",
       "##### config.cache -- a file cache\n",
       "The class `Cache`, available from `config.cache` implements a file cache in the folder\n",
       "`/home/burnett/.cache/wtlike`\n",
       "\n",
       "<details  class=\"nbdoc-description\" >  <summary> current cache contents </summary>  <div style=\"margin-left: 5%;\"><pre>Cache contents<br> key                                    size  time                 name, folder /home/burnett/.cache/wtlike<br>  PSR J0835-4510_data              156120367  2022-06-06 12:55     cache_file_ae6854be273acdc.pkl<br>  PSR J0633+1746_data              115577595  2022-06-06 13:00     cache_file_5707380cef0a5b.pkl<br>  PSR J1709-4429_data              120303131  2022-06-06 13:05     cache_file_7500f4770cd889.pkl<br>  PSR J2021+4026_data              130940729  2022-06-06 13:11     cache_file_a8dc38f04aab018.pkl<br>  PSR J1836+5925_data              125537569  2022-06-06 13:17     cache_file_a2b8a18b80eb164.pkl<br>  PSR J2021+3651_data              126539471  2022-06-06 13:23     cache_file_6661649ffbd07a9e.pkl<br>  PSR J0007+7303_data              134394303  2022-07-04 09:49     cache_file_cfa096edb809c60.pkl<br>  PSR J1809-2332_data              116347883  2022-06-06 13:40     cache_file_fb3fdcb2262565.pkl<br>  PSR J1826-1256_data              122240951  2022-06-06 13:45     cache_file_335ec043a4024484.pkl<br>  PSR J1907+0602_data              118468339  2022-06-06 13:51     cache_file_55b744805b5c9ff9.pkl<br>  PSR J1418-6058_data              132775053  2022-06-06 13:58     cache_file_29d5c1a8500b93e8.pkl<br>  PSR J1057-5226_data              101444077  2022-06-06 14:03     cache_file_384e41fd4ebdac81.pkl<br>  PSR J1813-1246_data              117386407  2022-06-06 14:08     cache_file_08c2148395b6f1f.pkl<br>  PSR J1028-5819_data              122583649  2022-06-06 14:15     cache_file_812ad9ec583a961.pkl<br>  PSR J2229+6114_data              133820383  2022-06-06 14:21     cache_file_e476bf55a06fa3e.pkl<br>  PSR J1048-5832_data              120018647  2022-06-06 14:27     cache_file_70b0714d172f3f2.pkl<br>  PSR J1732-3131_data              123636019  2022-06-06 14:33     cache_file_19edcba9cfff048.pkl<br>  PSR J1413-6205_data              132165683  2022-06-06 14:40     cache_file_6b3b60386724b1db.pkl<br>  PSR J1747-2958_data              126607289  2022-06-06 14:46     cache_file_2f67d22cb93688bb.pkl<br>  PSR J1952+3252_data              114920081  2022-06-06 14:52     cache_file_512aa5183afaad0.pkl<br>  P88Y3243_data                     88630555  2022-06-07 05:16     cache_file_66a9d9663efb97.pkl<br>  P88Y5020_data                    119817913  2022-06-07 10:33     cache_file_29e7a2e2944fb978.pkl<br>  504H-0317_data                   130475249  2022-06-07 10:39     cache_file_cb1897be30807d5.pkl<br>  P88Y6266_data                     99892151  2022-06-13 17:57     cache_file_35907c8f78cbed7c.pkl<br>  P88Y2172_data                    117721945  2022-06-17 11:49     cache_file_175f18c1a956e4d.pkl<br>  P88Y0096_data                    133548253  2022-07-04 09:54     cache_file_3c73e1e34f4c55.pkl<br>  P88Y0521_data                     95747295  2022-07-02 09:46     cache_file_7f6ab463d3ea18fd.pkl<br>  P88Y4060_data                     99826151  2022-07-02 10:05     cache_file_43042fb37426e122.pkl<br>  P88Y1134_data                     94193191  2022-07-02 09:51     cache_file_24a8a2282c30fdea.pkl<br>  P88Y6057_data                     89569443  2022-07-02 10:10     cache_file_4c75eeb98fde1be8.pkl<br>  P88Y1710_data                    125661037  2022-07-02 09:56     cache_file_342f3a94f9ee1f36.pkl<br>  P88Y1932_data                     87756585  2022-07-02 10:00     cache_file_5d597895910d1291.pkl<br>  P88Y0643_data                     97581865  2022-07-03 08:42     cache_file_4f9f229b3e50c0d6.pkl<br>  P88Y0514_data                     96424943  2022-07-03 13:01     cache_file_49fb71b2c22b562d.pkl<br>  PSR J0633+1746_weeks_9-11           419065  2022-07-04 10:03     cache_file_dd5ac11dbd01293.pkl<br>  P88Y3157_weeks_9-165              20704939  2022-07-04 10:06     cache_file_cd697fdc73ce09.pkl<br>  PSR J0534+2200_data              111891877  2022-07-13 01:08     cache_file_737398512a45c22.pkl<br>  P88Y3157_data                     88180753  2022-07-13 01:15     cache_file_5f7b91e889cf5dcf.pkl<br>  P88Y0615_data                     94004743  2022-07-14 13:03     cache_file_19435ee6bf1a46a2.pkl<br>  PSR J2021+4026-nk                138951485  2022-07-15 09:24     cache_file_6e0c2e81f58fab52.pkl<br></pre></div> </details>\n"
      ],
      "text/plain": [
       "<utilities.ipynb_docgen.doc_formatter.<locals>.MimeBundleObject at 0x7f3ab697dd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "@ipynb_doc\n",
    "def config_summary():\n",
    "    \"\"\"\n",
    "    \n",
    "    #### config.Config -- parameters are from three sources:\n",
    "    - defaults\n",
    "    - the file `~/.config/wtlike/config.yaml` if it exists\n",
    "    - keyword args in Config constructor. For example, to suppress all printout:\n",
    "        ```\n",
    "    config = Config(verbose=0)\n",
    "    ```\n",
    "    \n",
    "    ##### Config defaults\n",
    "    This is yaml-format, corresponding to `config.yaml`.\n",
    "    {config_defaults}\n",
    "    \n",
    "    \n",
    "    #### Config contents as set up here\n",
    "    {config_text}   \n",
    "\n",
    "\n",
    "    \n",
    "    ##### config.cache -- a file cache\n",
    "    The class `Cache`, available from `config.cache` implements a file cache in the folder\n",
    "    `{config.cachepath}`\n",
    "    \n",
    "    {cache_text}\n",
    "    \"\"\"\n",
    "    config = Config()\n",
    "    config_defaults = Config.defaults\n",
    "    config_text = monospace(config, summary='config parameter list')\n",
    "\n",
    "    try:\n",
    "        cache_text = monospace(config.cache, 'current cache contents' )\n",
    "    except Exception as msg:\n",
    "        cache_text = f'(Failed: {msg})'\n",
    "\n",
    "\n",
    "    return locals()\n",
    "config_summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time conversion\n",
    "\n",
    "- MET: mission elapsed time\n",
    "- MJD: modified Julian date (days)\n",
    "- UTC: ISO time\n",
    "- UTCnow: current ISO time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "day = 24*3600.\n",
    "first_data=54683\n",
    "#mission_start = Time('2001-01-01T00:00:00', scale='utc').mjd\n",
    "# From a FT2 file header\n",
    "# MJDREFI =               51910. / Integer part of MJD corresponding to SC clock S\n",
    "# MJDREFF =  0.00074287037037037 / Fractional part of MJD corresponding to SC cloc\n",
    "mission_start = 51910.00074287037\n",
    "from datetime import datetime\n",
    "from astropy.time import Time\n",
    "\n",
    "def MJD(arg):\n",
    "    \"\"\" convert MET or UTC to MJD\n",
    "    \"\"\"\n",
    "\n",
    "    if type(arg)==str:\n",
    "        if arg=='now':\n",
    "            return Time(datetime.utcnow()).mjd\n",
    "        while len(arg.split('-'))<3:\n",
    "            arg+='-1'\n",
    "        return Time(arg, format='iso').mjd\n",
    "    return (mission_start + arg/day  )\n",
    "\n",
    "def UTC(mjd):\n",
    "    \" convert MJD value to ISO date string\"\n",
    "    t=Time(mjd, format='mjd')\n",
    "    t.format='iso'; t.out_subfmt='date_hm'\n",
    "    return t.value\n",
    "\n",
    "def UTCnow():\n",
    "    \"\"\" current UTC \"\"\"\n",
    "    t=datetime.utcnow()\n",
    "    return f'UTC {t.year}-{t.month:02d}-{t.day} {t.hour:02d}:{t.minute:02d}'\n",
    "\n",
    "def mission_week(mjd):\n",
    "    \"\"\" return the mission week number for a MJD value\n",
    "    (Note that week #0 starts on UTC Thursday 2008-05-29 00:00, the first data is in week 9, and that week 525 is missing)\n",
    "    \"\"\"\n",
    "    return (mjd-54615)//(7)\n",
    "\n",
    "def mjd_range(start,stop, make_round=True):\n",
    "    \"\"\"Return a tuple with a valid MJD range\n",
    "    If either is greater than first_data, interpret as MJD\n",
    "    otherwise offset from first_data, or now\n",
    "    \n",
    "    So 0,0 is full range, -7, 0 is last 7 days, 0,7 is first 7 days\n",
    "\n",
    "    \"\"\"\n",
    "    a,b = start, stop\n",
    "    # extract MJD range\n",
    "    now = MJD('now')\n",
    "    if a<0: a = now-a\n",
    "    elif a < first_data : a+=first_data \n",
    "    if b <= 0 :  b = now-b\n",
    "    elif b < first_data: b += first_data\n",
    "    \n",
    "    return (round(a),round(b)) if make_round else (a,b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2001-01-01 00:01',\n",
       " '2008-08-05 00:00',\n",
       " 'UTC 2022-07-16 11:47',\n",
       " 59776.49153800692,\n",
       " '2022-07-16 11:47',\n",
       " (54683, 59776))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTC(MJD(0)), UTC(first_data), UTCnow(), MJD('now'), UTC(MJD('now')), mjd_range(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(*[mission_week(x) for x in mjd_range(0,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert UTC(MJD(0))=='2001-01-01 00:01'\n",
    "assert MJD('2008')==54466"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FermiInterval():\n",
    "    \"\"\"For iteration thru (start,stop) tuples in the Fermi data set\n",
    "    \"\"\"\n",
    "    def __init__(self, interval=30, offset=0):\n",
    "        from wtlike.config import  MJD, first_data\n",
    "        self.interval=interval\n",
    "        a,b = first_data+offset, MJD('now')\n",
    "        self.mm = np.arange(a,round(b),interval)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mm)-1\n",
    "    \n",
    "    def __getitem__(self, k):\n",
    "        return (self.mm[k], self.mm[k+1],) if k>=0 else (self.mm[k-1],self.mm[k])\n",
    "    \n",
    "    def __call__(self, k):\n",
    "        \"\"\" Return a timeinterval tuple\n",
    "        \"\"\"\n",
    "        a,b = self[k]\n",
    "        return (a,b,self.interval)\n",
    "    \n",
    "class FermiMonth(FermiInterval):\n",
    "    def __init__(self):\n",
    "        super().__init__(interval=30)\n",
    "        \n",
    "class FermiWeek(FermiInterval):\n",
    "    def __init__(self):\n",
    "        super().__init__(interval=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous utilities\n",
    "- bins_size_name\n",
    "- decorate_with\n",
    "- Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bin_size_name(bins):\n",
    "    \"\"\"Provide a nice name, e.g., 'day' for a time interval\n",
    "    \"\"\"\n",
    "    if np.isscalar(bins) :\n",
    "        binsize = bins\n",
    "    else:\n",
    "        binsize = np.mean(bins)\n",
    "\n",
    "    def check_unit(x):\n",
    "        unit_table = dict(year=1/365.25, week=1/7, day=1, hour=24, min=24*60) #, min=24*60, s=24*3600)\n",
    "        for name, unit in unit_table.items():\n",
    "            t = x*unit\n",
    "            r = np.mod(t+1e-9,1)\n",
    "\n",
    "            if r<1e-6 : #or t>1:\n",
    "                return t, name\n",
    "        return x, 'day'\n",
    "    n, unit =  check_unit(binsize)\n",
    "    nt = f'{n:.0f}' if np.mod(n,1)<1e-3 else f'{n:.1f}'\n",
    "    return f'{nt}-{unit}'# if n>1 else f'{unit}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15-min', '1-day', '2-day', '1-week', '2-week', '1-year']\n"
     ]
    }
   ],
   "source": [
    "print([bin_size_name(x) for x in (1/96, 1,2,7, 14, 365.25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def decorate_with(other_func):\n",
    "    def decorator(func):\n",
    "        func.__doc__ += other_func.__doc__\n",
    "        return func\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import time\n",
    "\n",
    "class Timer():\n",
    "    \"\"\"Usage:\n",
    "    ```\n",
    "    with Timer() as t:\n",
    "        time.sleep(5)\n",
    "    print(t)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t=time.time()\n",
    "        self.exit_time=1e6\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    def __exit__(self, *pars):\n",
    "        self.exit_time = time.time()-self.t\n",
    "    def __repr__(self):\n",
    "         return  f'elapsed time: {self.elapsed:.1f}s ({self.elapsed/60:.1f} min)'\n",
    "    @property\n",
    "    def elapsed(self):\n",
    "        return min(time.time()-self.t, self.exit_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intemediate elapsed time: 2.0s (0.0 min)\n",
      "Final elapsed time: 5.0s (0.1 min)\n"
     ]
    }
   ],
   "source": [
    "# collapse-show\n",
    "with Timer() as t:\n",
    "\n",
    "    time.sleep(2)\n",
    "    print('intemediate',t)\n",
    "    time.sleep(3)\n",
    "print('Final',t)\n",
    "assert(abs(t.elapsed -5)<0.1), f'wrong elapsed time: {t.elapsed}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 03_sources.ipynb.\n",
      "Converted 04_load_data.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 04_skymaps.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 10-time_series.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_presentation.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sat Jul 16 04:47:59 PDT 2022\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
