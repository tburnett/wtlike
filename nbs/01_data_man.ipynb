{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_man\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data management\n",
    "> Create, from FT1 and FT2, a compact data set with photon and livetime info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Fermi-LAT weekly data files are extracted from the [GSFC FTP server ](https://heasarc.gsfc.nasa.gov/FTP/fermi/data/lat/weekly), \n",
    "with subfolders for the photon data, `photon` and spacecraft data, `spacecraft`. It is [described here](http://fermi.gsfc.nasa.gov/ssc/data/access/http://fermi.gsfc.nasa.gov/ssc/data/access/)\n",
    "\n",
    "The class `FermiData` downloads these to temporary files and constructs a dict for each week with\n",
    "contents\n",
    "\n",
    "* photons: a table, one entry per selected photon with columns, converted with `get_ft1_data`\n",
    "  * `run_ref` (uint8), index into the `runs` list\n",
    "  * time since the run start, in 2 $\\mu$s intervals  (uint32)\n",
    "  * energy and event type (uint8)\n",
    "  * position as HEALPix index with nside, nest from Config, currently 1024, True (uint32) \n",
    "  \n",
    "* sc_data: a table, one entry per 30-s interval, with columns, all float32, converted with `get_ft2_info`\n",
    "  * start/stop time \n",
    "  * S/C direction \n",
    "  * zenith direction\n",
    "* runs list of run numbers\n",
    "* gti_times: an array of interleaved start/stop intervals\n",
    "* file_date: modification date for the FT1 file at GSFC.\n",
    "\n",
    "These dict objects are saved in a folder specfied by `Config().datapath`. Its contents can be \n",
    "checked with `check_data`, and updated using `update_data`.  The contents of a week can be visually \n",
    "inspected with `plot_week`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run numbers and timing\n",
    "\n",
    "The run number is an integer correspnding to the MET at the start of the run. For a week, with 15 orbits/day, we \n",
    "expect ~105 runs. We save a table of the run numbers per week, and use a uint8 index in the photon table.\n",
    "\n",
    "A run is typically an orbit or less, at most 6 ks. Integerizing the offset from the run start, or the run number, using 32 bits, one has 5e5 intervals/s, so we choose 2$\\mu$s. Thus we generate the time for each event from three sources: the 2$\\mu$s time offset for the run, the index of the run number into the  runs table, and the runs table itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import dateutil, datetime\n",
    "from astropy.io import fits\n",
    "from ftplib import FTP_TLS as FTP\n",
    "\n",
    "import healpy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from wtlike.config import Config, Timer, UTC, MJD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft1_data( config, ft1_file):\n",
    "\n",
    "        \"\"\"\n",
    "        Read in a photon data (FT1) file, bin in energy and position to convert to a compact table\n",
    "\n",
    "        - `ft1_file` -- A weekly file from GSFC\n",
    "\n",
    "        Depends on config items\n",
    "        - `theta_cut, z_cut` -- selection criteria\n",
    "        - `ebins, etypes` -- define band index\n",
    "        - `nside, nest` -- define HEALPix binning\n",
    "\n",
    "        Returns a dict with keys\n",
    "\n",
    "        - `tstart`, the start MET time from the FT1 header\n",
    "\n",
    "        - `photons`: a dict with keys and entries for each selected photon\n",
    "           - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
    "           - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
    "           - `run_ref` (uint8) reference to the run number, in the array `runs`\n",
    "           - `trun` (unit32): time since the run id in 2 $\\\\mu s$ units\n",
    "\n",
    "        - `gti_times` -- GTI times as an interleaved start, stop array.\n",
    "        - `runs` -- a list of the run numbers, each a MET time. Expect 109 per week\n",
    "\n",
    "        For the selected events above 100 MeV, this represents 10 bytes per photon, vs. 27 in the FT1 data\n",
    "        \"\"\"\n",
    "\n",
    "        delta_t = config.offset_size\n",
    "        ebins = config.energy_edges\n",
    "        etypes = config.etypes\n",
    "        nside = config.nside\n",
    "        nest = config.nest\n",
    "        z_cut =config.z_max\n",
    "        theta_cut = np.degrees(np.arccos(config.cos_theta_max))\n",
    "        verbose = config.verbose\n",
    "\n",
    "        with  fits.open(ft1_file) as ft1:\n",
    "            tstart = ft1[1].header['TSTART']\n",
    "\n",
    "            ## GTI - setup raveled array function to make cut\n",
    "            gti_data= ft1['GTI'].data\n",
    "            # extract arrays for values of interest\n",
    "            data =ft1['EVENTS'].data\n",
    "\n",
    "        a,b = sorted(gti_data.START), sorted(gti_data.STOP)\n",
    "\n",
    "        gti_times = np.ravel(np.column_stack((a,b)))\n",
    "        if np.any(np.diff(gti_times)<0):\n",
    "            raise Exception(f'Non-monatonic GTI found in file {ft1_file}')\n",
    "\n",
    "        # def apply_gti(time):\n",
    "        #     x = np.digitize(time, gti_times)\n",
    "        #     return np.bitwise_and(x,1).astype(bool)\n",
    "\n",
    "        # apply  selections\n",
    "\n",
    "        sel =  ((data['ENERGY'] > ebins[0]) &\n",
    "                (data['ZENITH_ANGLE'] < z_cut) &\n",
    "                (data['THETA'] < theta_cut))\n",
    "\n",
    "        dsel = data[sel]\n",
    "\n",
    "        # get the columns for output\n",
    "        glon, glat, energy, et, z, theta, time, ec =\\\n",
    "             [dsel[x] for x in 'L B ENERGY EVENT_TYPE ZENITH_ANGLE THETA TIME EVENT_CLASS'.split()]\n",
    "\n",
    "        # generate event_type masks\n",
    "        et_mask={}\n",
    "        for ie in etypes:\n",
    "            et_mask[ie]= et[:,-1-ie]\n",
    "\n",
    "\n",
    "        if verbose>2:\n",
    "            total = sum(b)-sum(a)\n",
    "            fraction = total/(b[-1]-a[0])\n",
    "\n",
    "            print(  f'FT1: {ft1_file.name}, GTI range {a[0]:.1f}-{b[-1]:.1f}:  {len(data):,} photons'\\\n",
    "                    f'\\n\\tSelection E > {ebins[0]:.0f} MeV. theta<{theta_cut:.1f} and z<{z_cut} remove:'\\\n",
    "                    f' {100.- 100*len(dsel)/float(len(data)):.2f}%'\n",
    "                 )\n",
    "\n",
    "\n",
    "                # event class -- turn into single int for later mask\n",
    "        #         bits = np.array([1<<n for n in range(20)])\n",
    "        #         def to_bin(x):\n",
    "        #             return np.sum(bits[x[:20]])\n",
    "        #         ec = [to_bin(row[20]) for row in ec\n",
    "\n",
    "        # pixelate direction\n",
    "        hpindex = healpy.ang2pix(nside, glon, glat, nest=nest, lonlat=True).astype(np.uint32)\n",
    "        hpname = 'nest_index' if nest else 'ring_index'\n",
    "\n",
    "        # digitize energy and create band index incluing (front/back)\n",
    "        band_index = (2*(np.digitize(energy, ebins, )-1) + et_mask[1]).astype(np.uint8)\n",
    "\n",
    "        #\n",
    "        run_id = dsel['RUN_ID'].astype(np.uint32)\n",
    "        \n",
    "        # save this for for run reference\n",
    "        runlist = np.unique(run_id)\n",
    "\n",
    "        # a dict with all photon info, which requires runlist\n",
    "        runs = np.unique(run_id)\n",
    "        photons = {'band' : band_index,\n",
    "                  hpname : hpindex,\n",
    "                  'run_ref': np.searchsorted(runs, run_id).astype(np.uint8),\n",
    "                  'trun'  : ((time-run_id)/delta_t).astype(np.uint32),\n",
    "                 }\n",
    "                  \n",
    "        if verbose>1:\n",
    "            print(f'FT1 data from file {ft1_file.name}: tstart={tstart:.0f} (UTC {UTC(MJD(tstart))[:-6]})' \n",
    "                  f' selected {len(dsel):,}/{len(data):,} photons, in {len(runlist)} runs.')\n",
    "\n",
    "        return  dict(tstart=tstart, #df,\n",
    "                     photons=photons, \n",
    "                     gti_times=gti_times, \n",
    "                     runlist=runlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft1_data\" class=\"doc_header\"><code>get_ft1_data</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft1_data</code>(**`config`**, **`ft1_file`**)\n",
       "\n",
       "Read in a photon data (FT1) file, bin in energy and position to convert to a compact table\n",
       "\n",
       "- `ft1_file` -- A weekly file from GSFC\n",
       "\n",
       "Depends on config items\n",
       "- `theta_cut, z_cut` -- selection criteria\n",
       "- `ebins, etypes` -- define band index\n",
       "- `nside, nest` -- define HEALPix binning\n",
       "\n",
       "Returns a dict with keys\n",
       "\n",
       "- `tstart`, the start MET time from the FT1 header\n",
       "\n",
       "- `photons`: a dict with keys and entries for each selected photon\n",
       "   - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
       "   - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
       "   - `run_ref` (uint8) reference to the run number, in the array `runs`\n",
       "   - `trun` (unit32): time since the run id in 2 $\\mu s$ units\n",
       "\n",
       "- `gti_times` -- GTI times as an interleaved start, stop array.\n",
       "- `runs` -- a list of the run numbers, each a MET time. Expect 109 per week\n",
       "\n",
       "For the selected events above 100 MeV, this represents 10 bytes per photon, vs. 27 in the FT1 data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft1_data, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft2_info(config, filename,\n",
    "                 gti=lambda t: True):\n",
    "    \"\"\"Process a FT2 file, with S/C history data, and return a summary dict\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * config -- verbose, cos_theta_max, z_max\n",
    "    * filename -- spacecraft (FT2) file\n",
    "    * gti -- GTI object that checkes for allowed intervals, in MJD units\n",
    "\n",
    "    Returns: A dict with fields consistent with GTI if specified\n",
    "\n",
    "    * start, stop -- interval in MJD units\n",
    "    * livetime -- sec\n",
    "    * ra_scz, dec_scz --spaceraft direction\n",
    "    * ra_zenith, dec_zenith -- local zenith\n",
    "    \"\"\"\n",
    "    # combine the files into a dict  with following fields besides START and STOP (lower case for column)\n",
    "    fields    = ['LIVETIME','RA_SCZ','DEC_SCZ', 'RA_ZENITH','DEC_ZENITH']\n",
    "    with fits.open(filename) as hdu:\n",
    "        scdata = hdu['SC_DATA'].data\n",
    "        tstart, tstop = [float(hdu[0].header[field]) for field in  ('TSTART','TSTOP') ]\n",
    "\n",
    "    if config.verbose>1:\n",
    "        print(f'FT2: {filename.name}, MET range {tstart:.1f}-{tstop:.1f},', end='')# {\"not\" if gti is None else \"\"} applying GTI')\n",
    "\n",
    "    # get times to check against MJD limits and GTI\n",
    "    start, stop = [MJD(np.array(scdata.START, float)),\n",
    "                   MJD(np.array(scdata.STOP, float))]\n",
    "\n",
    "    # apply GTI to bin center (avoid edge effects?)\n",
    "    in_gti = gti(0.5*(start+stop))\n",
    "    if config.verbose>1:\n",
    "        s = sum(in_gti)\n",
    "        print(f' {len(start)} entries, {s} ({100*s/len(start):.1f}%) in GTI')\n",
    "\n",
    "    t = [('start', start[in_gti]), ('stop',stop[in_gti])]+\\\n",
    "        [(field.lower(), np.array(scdata[field][in_gti],np.float32)) for field in fields ]\n",
    "\n",
    "\n",
    "    sc_data = dict(t)\n",
    "\n",
    "    return sc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft2_info\" class=\"doc_header\"><code>get_ft2_info</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft2_info</code>(**`config`**, **`filename`**, **`gti`**=*`<lambda>`*)\n",
       "\n",
       "Process a FT2 file, with S/C history data, and return a summary dict\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* config -- verbose, cos_theta_max, z_max\n",
       "* filename -- spacecraft (FT2) file\n",
       "* gti -- GTI object that checkes for allowed intervals, in MJD units\n",
       "\n",
       "Returns: A dict with fields consistent with GTI if specified\n",
       "\n",
       "* start, stop -- interval in MJD units\n",
       "* livetime -- sec\n",
       "* ra_scz, dec_scz --spaceraft direction\n",
       "* ra_zenith, dec_zenith -- local zenith"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft2_info, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def filepaths(week):\n",
    "    \"\"\"Returns: A tuple with two elements for the week number, each with two triplets with:\n",
    "        ftp folder, ftp filename, local simple filename\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for ftype, alias in  [('spacecraft','ft2'), ('photon','ft1')]:\n",
    "         urls.append((\n",
    "             f'{ftype}',\n",
    "             f'lat_{ftype}_weekly_w{week:03d}_p{\"305\" if ftype==\"photon\" else \"310\" }_v001.fits',\n",
    "             f'week{week:03d}_{alias}.fits',\n",
    "            ))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class GSFCweekly(dict):\n",
    "\n",
    "    ftp_site = 'heasarc.gsfc.nasa.gov'\n",
    "    ftp_path = 'fermi/data/lat/weekly'\n",
    "    local_path  = '/tmp/from_gsfc'\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\" Obtain lists of the weekly FT1 and FT2 files at GSFC, Set up as a dict, with\n",
    "        keys= week numbers, values=mofification date strings\n",
    "        \"\"\"\n",
    "        self.config = config or Config()\n",
    "        # self.wtlike_data_file_path = Path(self.config.datapath/'data_files')\n",
    "        # assert self.wtlike_data_file_path.is_dir(), 'Data path invalid'\n",
    "        # os.makedirs(self.local_path, exist_ok=True)\n",
    "        try:\n",
    "            with FTP(self.ftp_site) as ftp:\n",
    "                ftp.login()\n",
    "                ftp.prot_p()\n",
    "                ftp.cwd(self.ftp_path+'/photon') # or spacecraft\n",
    "                # aet modification time and type for all files in folder\n",
    "\n",
    "                parse_week = lambda fn:  int(fn.split('_')[3][1:])\n",
    "                flist = ftp.mlsd(facts=['modify', 'type'])\n",
    "                self.fileinfo = sorted([(parse_week(name), fact['modify']) for name,fact in flist\n",
    "                                 if fact['type']=='file' and name.startswith('lat') ])\n",
    "        except Exception as msg:\n",
    "            print(f'FTP login to or download from {self.ftp_site} failed:\\n\\t--> {msg}',file=sys.stderr)\n",
    "            self.valid=False\n",
    "            return\n",
    "        self.update(self.fileinfo)\n",
    "        self.valid=True\n",
    "    \n",
    "    def download(self, week):\n",
    "        \"\"\" Download the given week's files to the local folder\n",
    "        \n",
    "        week -- the mission week number, starting at 9. If negative, get recent one\n",
    "        \n",
    "        return the ft1, ft2 local filenames\n",
    "        \"\"\"\n",
    "        \n",
    "        if week<0:\n",
    "            week = list(self.keys())[week]\n",
    "        assert week in self, f'week {week} not found at FTP site'\n",
    "        files = []\n",
    "        with FTP(self.ftp_site) as ftp:\n",
    "            ftp.login()\n",
    "            ftp.prot_p()\n",
    "            for ftp_folder, ftp_filename, local_filename in filepaths(week):\n",
    "                ftp.cwd('/'+self.ftp_path+'/'+ftp_folder)\n",
    "                if self.config.verbose>0:\n",
    "                    print(f'GSFCweekly: {ftp_folder}/{ftp_filename} --> {local_filename}', flush=True)\n",
    "                with open(f'{self.local_path}/{local_filename}', 'wb') as  localfile:\n",
    "                    ftp.retrbinary('RETR ' + ftp_filename, localfile.write) \n",
    "                files.append(local_filename)\n",
    "        return files\n",
    "    \n",
    "    def week_number(self, met):\n",
    "        return (met-233711940)//(7*24*3600)\n",
    "    \n",
    "    def load_week(self, week):\n",
    "        \"\"\"Load a pickled week summary \"\"\"\n",
    "        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'\n",
    "        assert filename.exists(), f'File {filename} does not exist'\n",
    "        with open(filename, 'rb') as imp:\n",
    "            ret = pickle.load(imp)\n",
    "        return ret\n",
    "\n",
    "    def check_week(self, week):\n",
    "        \"\"\"Returns True if the local week needs updating\"\"\"\n",
    "        data = self.load_week(week)\n",
    "        if 'file_date' not in data:\n",
    "            return True\n",
    "        # check that file date agrees\n",
    "        return data['file_date'] != self[week]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise access from GSFC and FT1 data handling\n",
      "GSFCweekly: spacecraft/lat_spacecraft_weekly_w728_p310_v001.fits --> week728_ft2.fits\n",
      "GSFCweekly: photon/lat_photon_weekly_w728_p305_v001.fits --> week728_ft1.fits\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "print( 'Exercise access from GSFC and FT1 data handling')\n",
    "gd = GSFCweekly(Config(verbose=3))\n",
    "if gd.config.valid:\n",
    "    ft2, ft1 = gd.download(-2)\n",
    "    v = get_ft1_data(gd.config, Path(gd.local_path)/ft1);\n",
    "\n",
    "    print('Contents of the output dict\\n'\n",
    "          ' key      value type')\n",
    "    for (k,x) in v.items():\n",
    "        print(f'  {k:10} {x.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FermiData(GSFCweekly):\n",
    "    \"\"\" Manage the full data set in weekly chunks\n",
    "    * Checking the current set of files at GSFC\n",
    "    * downloading a week at a time to a local tmp\n",
    "    * Converting to condensed format and saving to pickled dicts in wtlike_data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config)\n",
    "        self.wtlike_data_file_path = Path(self.config.datapath/'data_files')\n",
    "        assert self.wtlike_data_file_path.is_dir(), 'Data path is invalid'\n",
    "        os.makedirs(self.local_path, exist_ok=True)\n",
    "        \n",
    "    @property\n",
    "    def local_filedate(self):\n",
    "        \"\"\" the datetime object representing the last file date in local storage\"\"\"\n",
    "        from dateutil.parser import parse\n",
    "        weekly_folder = self.config.datapath/'data_files'\n",
    "        ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "        if len(ff)==0:\n",
    "            print(f'FermiData: No weekly summary files found in {weekly_folder}', file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "        lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "        return dateutil.parser.parse(lastweek['file_date'])\n",
    "\n",
    "    @property\n",
    "    def gsfc_filedate(self):\n",
    "        return dateutil.parser.parse(list(self.values())[-1])\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'FermiData: {len(self.fileinfo)} week files at GSFC, from {self.fileinfo[0]} to {self.fileinfo[-1]}'\n",
    "\n",
    "    def in_temp(self):\n",
    "        \"\"\"return list of GSFC copied files in the local_path folder\"\"\"\n",
    "        names = [f.name for f in Path(self.local_path).glob('*')]\n",
    "        return names\n",
    "\n",
    "    def __call__(self, week, test=False, tries_left=2):\n",
    "        \"\"\" Download and convert the given week:\n",
    "        * download FT1 and FT2 from GSFC to scratch space\n",
    "        * convert each\n",
    "        * save pickled dict summary\n",
    "        * remove files\n",
    "\n",
    "        \"\"\"\n",
    "        assert week in self, f'week {week} not found at FTP site'\n",
    "        ff = filepaths(week)\n",
    "        ft1_file = Path(self.local_path)/ff[1][2]\n",
    "        ft2_file = Path(self.local_path)/ff[0][2]\n",
    "\n",
    "        if self.config.verbose>1:\n",
    "            print(f'FermiData: converting week {week}')\n",
    "\n",
    "        while tries_left>0:\n",
    "            try:\n",
    "                if not (ft1_file.exists() and ft2_file.exists()):\n",
    "                    self.download(week)\n",
    "                # convert photon data to compact form, as a dice\n",
    "                week_summary = get_ft1_data(self.config, ft1_file)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'*** ERROR *** Failed to convert {ft1_file}: {e} download it again)', \n",
    "                      file=sys.stderr)\n",
    "                tries_left -=1\n",
    "                if tries_left==0:\n",
    "                    print(f'Failed to convert week file {ft1_file}: quitting', file=sys.stderr)\n",
    "                    return None\n",
    "                else:\n",
    "                    os.unlink(ft1_file)\n",
    "\n",
    "        def apply_gti(time): # note MJD\n",
    "            x = np.digitize(time, MJD(week_summary['gti_times']))\n",
    "            return np.bitwise_and(x,1).astype(bool)\n",
    "        sc_data = get_ft2_info(self.config, ft2_file, apply_gti)\n",
    "\n",
    "        # finished with copies of FT1 and FT2 files: delete them\n",
    "        for file in (ft1_file,ft2_file):\n",
    "            os.unlink(file)\n",
    "\n",
    "        # add file date and space craft summary\n",
    "        week_summary['file_date'] = self[week]\n",
    "        week_summary['sc_data'] = sc_data\n",
    "        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'\n",
    "        if filename.exists() and self.config.verbose>0:\n",
    "            print(f'FermiData: replacing existing {filename}',flush=True)\n",
    "        if not test:\n",
    "            with open(filename, 'wb') as out:\n",
    "                pickle.dump(week_summary, out)\n",
    "\n",
    "        if self.config.verbose>0:\n",
    "            print(f'FermiData: Saved to {filename}', flush=True)\n",
    "\n",
    "    def download_and_convert(self, week_range,  processes=None):\n",
    "        \"\"\"Download FT1 and FT2 files from GSFC and create summary files for the weeks\n",
    "        \n",
    "        * week_range: a (first, last+1) tumple, or a iterable \n",
    "        \n",
    "        \"\"\"\n",
    "        from multiprocessing import Pool\n",
    "\n",
    "        processes = processes or self.config.pool_size\n",
    "        txt = f', using {processes} processes ' if processes>1 else ''\n",
    "\n",
    "        if self.config.verbose>0:\n",
    "            print(f'\\tDownloading {len(week_range)} week files{txt}\\n', end='', flush=True)\n",
    "\n",
    "        if processes>1:\n",
    "            with Pool(processes=processes) as pool:\n",
    "                pool.map(self, week_range)\n",
    "        else:\n",
    "            list(map(self,  week_range))\n",
    " \n",
    "    def needs_update(self):\n",
    "        \"\"\" Compare files on disk with the GSFC list and compile list that need to be downloaded\n",
    "\n",
    "        Check the file date of the last one on disk and update it as well if it has a different filedate\n",
    "        \"\"\"\n",
    "        gg =self.wtlike_data_file_path.glob('*.pkl')\n",
    "        file_weeks= map(lambda n: int(n.name[5:8]), gg)\n",
    "        ondisk = np.array(list(file_weeks))\n",
    "\n",
    "        missing =  set(self.keys()).difference(set(ondisk))\n",
    "        if len(ondisk)==0:\n",
    "            return list(missing)\n",
    "\n",
    "        last = sorted(ondisk)[-1]\n",
    "        if self.check_week(last):\n",
    "             missing.add(last)\n",
    "        return sorted(list(missing))\n",
    "\n",
    "    def check_data(self):\n",
    "        \"\"\"\n",
    "        Return: sorted list of summary files, last week number, number of days in last week\"\"\"\n",
    "        config = self.config\n",
    "        if config.valid:\n",
    "            weekly_folder = config.datapath/'data_files'\n",
    "            ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "            if len(ff)==0:\n",
    "                print(f'No .pkl files found in {weekly_folder}', file=sys.stderr)\n",
    "                return\n",
    "            getwk = lambda f: int(os.path.splitext(f)[0][-3:])\n",
    "            wk = [getwk(f) for f in ff] #list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "            lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "\n",
    "            file_date = lastweek['file_date']\n",
    "            gti = lastweek['gti_times'];\n",
    "            days = (gti[-1]-gti[0])/(24*3600)\n",
    "            if config.verbose>0:\n",
    "                print(f'Weekly folder \"{weekly_folder}\" contains {len(wk)} weeks.'\\\n",
    "                      f'\\n\\t Last week in local dataset, #{wk[-1]}, has {days:.3f} days, ends at UTC {UTC(MJD(gti[-1]))}, filedate {file_date}' )\n",
    "            #return ff, wk[-1], days\n",
    "        else:\n",
    "            print(f'Config not valid, {config.errors}', file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "    def update_data(self):\n",
    "        \"\"\"Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.\n",
    "        If the last one is the current week, check to see if needs updating, by comparing file date, in days,\n",
    "        from the last update with the current one at GSFC.\n",
    "        \"\"\"\n",
    "        self.check_data()\n",
    "        needs = self.needs_update()\n",
    "        if len(needs)==0:\n",
    "            print('--> Up to date!')\n",
    "            return\n",
    "        return self.download_and_convert(needs)\n",
    "\n",
    "    def get_run_times(self, week):\n",
    "        r = self.load_week(week)\n",
    "        pdict = r['photons']\n",
    "        if 'run_id' in pdict:\n",
    "            runs = np.unique(pdict['run_id'])\n",
    "        elif 'run_ref' in pdict:\n",
    "            runs = r['runlist'][pdict['run_ref']]\n",
    "        else:\n",
    "            assert False\n",
    "        return MJD(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "config = Config()\n",
    "if config.valid:\n",
    "    self = FermiData(config)\n",
    "    self.check_data()\n",
    "    print(f'Weeks needing download: {self.needs_update()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needs = self.needs_update()\n",
    "print(needs)\n",
    "self.download_and_convert(needs, processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.update_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(FermiData)\n",
    "show_doc(FermiData.download)\n",
    "show_doc(FermiData.needs_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_data(config=None, update=False):\n",
    "    \"\"\"\n",
    "    Print current status, and update if requested\n",
    "    \"\"\"\n",
    "    ff = FermiData(config)\n",
    "    ff.check_data()\n",
    "    print(f'Weeks needing download: {ff.needs_update()}')\n",
    "    if update:\n",
    "        ff.update_data()\n",
    "    \n",
    "def update_data( config=None):\n",
    "    \"\"\"Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.\n",
    "    If the last one is the current week, check to see if needs updating, by comparing file date, in days,\n",
    "    from the last update with the current one at GSFC.\n",
    "    \"\"\"\n",
    "    ff = FermiData(config)\n",
    "    return ff.update_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(check_data,  title_level=2)\n",
    "show_doc(update_data, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def get_week_files(config, week_range=None):\n",
    "    \"\"\"Return list of week files\n",
    "\n",
    "    - week_range [None] -- tuple with inclusive range. If None, get all\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    data_folder = config.datapath/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    weeks = week_range or  config.week_range\n",
    "    if week_range is not None:\n",
    "\n",
    "        slc = slice(*week_range) if type(week_range)==tuple else slice(week_range,week_range)\n",
    "        wk_table = pd.Series(data=[df for df in data_files],\n",
    "                     index= [ int(df.name[-7:-4]) for df in  data_files],\n",
    "                    )\n",
    "        data_files = wk_table.loc[slc].values\n",
    "\n",
    "        if config.verbose>1:\n",
    "            q = lambda x: x if x is not None else \"\"\n",
    "            print(f'LoadData: Loading weeks[{q(slc.start)}:{q(slc.stop)}:{q(slc.step)}]', end='' if config.verbose<2 else '\\n')\n",
    "    else:\n",
    "        if config.verbose>1: print(f'LoadData: loading all {len(data_files)} weekly files')\n",
    "\n",
    "    if len(data_files)==0:\n",
    "        msg =  f'Specified week_range {week_range} produced no output. Note that week numbers are 9-'\n",
    "        raise Exception(msg)\n",
    "\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def plot_week(week=None, mjd=None, nside=32, **kwargs):\n",
    "    \"\"\"\n",
    "    Make an AIT plot of the given week's photon data\n",
    "    \n",
    "    Combine all energies for now\n",
    "    \n",
    "    - week -- the week number from 9\n",
    "    - mjd -- [None] If set, derive the week from it\n",
    "    - nside [32] -- HEALPix nside to project data before plotting.\n",
    "    - kwargs -- args for healpix.ait_plot\n",
    "    \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from utilities import healpix as hpx\n",
    "    from wtlike.config import mission_week\n",
    "        \n",
    "    assert nside & (nside-1) == 0, 'nside must be power of 2'\n",
    "    config = Config()\n",
    "    if not config.valid:\n",
    "        print('No data to plot since config not valid', file=sys.stderr)\n",
    "        return\n",
    "    kw = dict(log=True, tick_labels=False, vmin=5, vmax=1e3,\n",
    "             cblabel=f'counts per nside={nside} pixel')\n",
    "    kw.update(kwargs)\n",
    "    if mjd is not None: week=mission_week(mjd)\n",
    "\n",
    "    file = get_week_files(config,(week,week))[0]\n",
    "    with open(file, 'rb') as inp:\n",
    "        u = pickle.load(inp)\n",
    "    ni = u['photons']['nest_index']\n",
    "    utc = UTC(MJD((u['gti_times'][0])))[:-5]\n",
    "    n = 12*nside**2\n",
    "    # to convert data nside (1024 normally)\n",
    "    shift = int(np.log2(config.nside/nside))\n",
    "    pmap,_ = np.histogram( np.right_shift(ni,2*shift), np.linspace(0,n,n+1)) \n",
    "    t = hpx.HPmap(pmap, f'week_{week:d}\\n{utc}', nest=True)\n",
    "    t.ait_plot( **kw);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(plot_week, title_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of a week's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config().valid:\n",
    "    plot_week(mjd=55171)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
