{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_man\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data management\n",
    "> Create, from FT1 and FT2, a compact data set with photon and livetime info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Fermi-LAT weekly data files are extracted from the [GSFC FTP server ](https://heasarc.gsfc.nasa.gov/FTP/fermi/data/lat/weekly), \n",
    "with subfolders for the photon data, `photon` and spacecraft data, `spacecraft`. It is [described here](http://fermi.gsfc.nasa.gov/ssc/data/access/http://fermi.gsfc.nasa.gov/ssc/data/access/)\n",
    "\n",
    "The class `FermiData` downloads these to temporary files and constructs a dict for each week with\n",
    "contents\n",
    "\n",
    "* photons: a table, one entry per selected photon with columns, converted with `get_ft1_data`\n",
    "  * `run_ref` (uint8), index into the `runs` list\n",
    "  * time since the run start, in 2 $\\mu$s intervals  (uint32)\n",
    "  * energy and event type (uint8)\n",
    "  * position as HEALPix index with nside, nest from Config, currently 1024, True (uint32) \n",
    "  \n",
    "* sc_data: a table, one entry per 30-s interval, with columns, all float32, converted with `get_ft2_info`\n",
    "  * start/stop time \n",
    "  * S/C direction \n",
    "  * zenith direction\n",
    "* runs list of run numbers\n",
    "* gti_times: an array of interleaved start/stop intervals\n",
    "* file_date: modification date for the FT1 file at GSFC.\n",
    "\n",
    "These dict objects are saved in a folder specfied by `Config().datapath`. Its contents can be \n",
    "checked with `check_data`, and updated using `update_data`.  The contents of a week can be visually \n",
    "inspected with `plot_week`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run numbers and timing\n",
    "\n",
    "The run number is an integer correspnding to the MET at the start of the run. For a week, with 15 orbits/day, we \n",
    "expect ~105 runs. We save a table of the run numbers per week, and use a uint8 index in the photon table.\n",
    "\n",
    "A run is typically an orbit or less, at most 6 ks. Integerizing the offset from the run start, or the run number, using 32 bits, one has 5e5 intervals/s, so we choose 2$\\mu$s. Thus we generate the time for each event from three sources: the 2$\\mu$s time offset for the run, the index of the run number into the  runs table, and the runs table itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import dateutil, datetime\n",
    "from astropy.io import fits\n",
    "from ftplib import FTP_TLS as FTP\n",
    "\n",
    "import healpy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from wtlike.config import Config, Timer, UTC, MJD, mjd_range, mission_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft1_data( config, ft1_file):\n",
    "\n",
    "        \"\"\"\n",
    "        Read in a photon data (FT1) file, bin in energy and position to convert to a compact table\n",
    "\n",
    "        - `ft1_file` -- A weekly file from GSFC\n",
    "\n",
    "        Depends on config items\n",
    "        - `theta_cut, z_cut` -- selection criteria\n",
    "        - `ebins, etypes` -- define band index\n",
    "        - `nside, nest` -- define HEALPix binning\n",
    "\n",
    "        Returns a dict with keys\n",
    "\n",
    "        - `tstart`, the start MET time from the FT1 header\n",
    "\n",
    "        - `photons`: a dict with keys and entries for each selected photon\n",
    "           - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
    "           - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
    "           - `run_ref` (uint8) reference to the run number, in the array `runs`\n",
    "           - `trun` (unit32): time since the run id in 2 $\\\\mu s$ units\n",
    "\n",
    "        - `gti_times` -- GTI times as an interleaved start, stop array.\n",
    "        - `runs` -- a list of the run numbers, each a MET time. Expect 109 per week\n",
    "\n",
    "        For the selected events above 100 MeV, this represents 10 bytes per photon, vs. 27 in the FT1 data\n",
    "        \"\"\"\n",
    "\n",
    "        delta_t = config.offset_size\n",
    "        ebins = config.energy_edges\n",
    "        etypes = config.etypes\n",
    "        nside = config.nside\n",
    "        nest = config.nest\n",
    "        z_cut =config.z_max\n",
    "        theta_cut = np.degrees(np.arccos(config.cos_theta_max))\n",
    "        verbose = config.verbose\n",
    "\n",
    "        with  fits.open(ft1_file) as ft1:\n",
    "            tstart = ft1[1].header['TSTART']\n",
    "\n",
    "            ## GTI - setup raveled array function to make cut\n",
    "            gti_data= ft1['GTI'].data\n",
    "            # extract arrays for values of interest\n",
    "            data =ft1['EVENTS'].data\n",
    "\n",
    "        a,b = sorted(gti_data.START), sorted(gti_data.STOP)\n",
    "\n",
    "        gti_times = np.ravel(np.column_stack((a,b)))\n",
    "        if np.any(np.diff(gti_times)<0):\n",
    "            raise Exception(f'Non-monatonic GTI found in file {ft1_file}')\n",
    "\n",
    "        # def apply_gti(time):\n",
    "        #     x = np.digitize(time, gti_times)\n",
    "        #     return np.bitwise_and(x,1).astype(bool)\n",
    "\n",
    "        # apply  selections\n",
    "\n",
    "        sel =  ((data['ENERGY'] > ebins[0]) &\n",
    "                (data['ZENITH_ANGLE'] < z_cut) &\n",
    "                (data['THETA'] < theta_cut))\n",
    "\n",
    "        dsel = data[sel]\n",
    "\n",
    "        # get the columns for output\n",
    "        glon, glat, energy, et, z, theta, time, ec =\\\n",
    "             [dsel[x] for x in 'L B ENERGY EVENT_TYPE ZENITH_ANGLE THETA TIME EVENT_CLASS'.split()]\n",
    "\n",
    "        # generate event_type masks\n",
    "        et_mask={}\n",
    "        for ie in etypes:\n",
    "            et_mask[ie]= et[:,-1-ie]\n",
    "\n",
    "\n",
    "        if verbose>2:\n",
    "            total = sum(b)-sum(a)\n",
    "            fraction = total/(b[-1]-a[0])\n",
    "\n",
    "            print(  f'FT1: {ft1_file.name}, GTI range {a[0]:.1f}-{b[-1]:.1f}:  {len(data):,} photons'\\\n",
    "                    f'\\n\\tSelection E > {ebins[0]:.0f} MeV. theta<{theta_cut:.1f} and z<{z_cut} remove:'\\\n",
    "                    f' {100.- 100*len(dsel)/float(len(data)):.2f}%'\n",
    "                 )\n",
    "\n",
    "\n",
    "                # event class -- turn into single int for later mask\n",
    "        #         bits = np.array([1<<n for n in range(20)])\n",
    "        #         def to_bin(x):\n",
    "        #             return np.sum(bits[x[:20]])\n",
    "        #         ec = [to_bin(row[20]) for row in ec\n",
    "\n",
    "        # pixelate direction\n",
    "        hpindex = healpy.ang2pix(nside, glon, glat, nest=nest, lonlat=True).astype(np.uint32)\n",
    "        hpname = 'nest_index' if nest else 'ring_index'\n",
    "\n",
    "        # digitize energy and create band index incluing (front/back)\n",
    "        band_index = (2*(np.digitize(energy, ebins, )-1) + et_mask[1]).astype(np.uint8)\n",
    "\n",
    "        #\n",
    "        run_id = dsel['RUN_ID'].astype(np.uint32)\n",
    "\n",
    "        # save this for for run reference\n",
    "        runlist = np.unique(run_id)\n",
    "\n",
    "        # a dict with all photon info, which requires runlist\n",
    "        runs = np.unique(run_id)\n",
    "        photons = {'band' : band_index,\n",
    "                  hpname : hpindex,\n",
    "                  'run_ref': np.searchsorted(runs, run_id).astype(np.uint8),\n",
    "                  'trun'  : ((time-run_id)/delta_t).astype(np.uint32),\n",
    "                 }\n",
    "\n",
    "        if verbose>1:\n",
    "            print(f'FT1 data from file {ft1_file.name}: tstart={tstart:.0f} (UTC {UTC(MJD(tstart))[:-6]})'\n",
    "                  f' selected {len(dsel):,}/{len(data):,} photons, in {len(runlist)} runs.')\n",
    "\n",
    "        return  dict(tstart=tstart, #df,\n",
    "                     photons=photons,\n",
    "                     gti_times=gti_times,\n",
    "                     runlist=runlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft1_data\" class=\"doc_header\"><code>get_ft1_data</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft1_data</code>(**`config`**, **`ft1_file`**)\n",
       "\n",
       "Read in a photon data (FT1) file, bin in energy and position to convert to a compact table\n",
       "\n",
       "- `ft1_file` -- A weekly file from GSFC\n",
       "\n",
       "Depends on config items\n",
       "- `theta_cut, z_cut` -- selection criteria\n",
       "- `ebins, etypes` -- define band index\n",
       "- `nside, nest` -- define HEALPix binning\n",
       "\n",
       "Returns a dict with keys\n",
       "\n",
       "- `tstart`, the start MET time from the FT1 header\n",
       "\n",
       "- `photons`: a dict with keys and entries for each selected photon\n",
       "   - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
       "   - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
       "   - `run_ref` (uint8) reference to the run number, in the array `runs`\n",
       "   - `trun` (unit32): time since the run id in 2 $\\mu s$ units\n",
       "\n",
       "- `gti_times` -- GTI times as an interleaved start, stop array.\n",
       "- `runs` -- a list of the run numbers, each a MET time. Expect 109 per week\n",
       "\n",
       "For the selected events above 100 MeV, this represents 10 bytes per photon, vs. 27 in the FT1 data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft1_data, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def photon_week(config, week_dict):\n",
    "    \"\"\"\n",
    "    Return a dict with photon info for the week, with a 'time' field, in MJD reconstructed from\n",
    "    the run_ref and trun fields. The latter removed.\n",
    "\n",
    "    - week_dict -- standard photon info for the week\n",
    "    - config -- used for config.offset_size\n",
    "\n",
    "    \"\"\"\n",
    "    delta_t = config.offset_size\n",
    "    phd = week_dict['photons'].copy()\n",
    "    rl = week_dict['runlist']\n",
    "    phd['time'] = MJD(phd['trun']* delta_t + rl[phd['run_ref']]) #- week_dict['tstart']\n",
    "    phd.pop('run_ref')\n",
    "    phd.pop('trun')\n",
    "    return phd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"photon_week\" class=\"doc_header\"><code>photon_week</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>photon_week</code>(**`config`**, **`week_dict`**)\n",
       "\n",
       "Return a dict with photon info for the week, with a 'time' field, in MJD reconstructed from\n",
       "the run_ref and trun fields. The latter removed.\n",
       "\n",
       "- week_dict -- standard photon info for the week\n",
       "- config -- used for config.offset_size"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(photon_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft2_info(config, filename,\n",
    "                 gti=lambda t: True):\n",
    "    \"\"\"Process a FT2 file, with S/C history data, and return a summary dict\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * config -- verbose, cos_theta_max, z_max\n",
    "    * filename -- spacecraft (FT2) file\n",
    "    * gti -- GTI object that checkes for allowed intervals, in MJD units\n",
    "\n",
    "    Returns: A dict with fields consistent with GTI if specified\n",
    "\n",
    "    * start, stop -- interval in MJD units\n",
    "    * livetime -- sec\n",
    "    * ra_scz, dec_scz --spaceraft direction\n",
    "    * ra_zenith, dec_zenith -- local zenith\n",
    "    \"\"\"\n",
    "    # combine the files into a dict  with following fields besides START and STOP (lower case for column)\n",
    "    fields    = ['LIVETIME','RA_SCZ','DEC_SCZ', 'RA_ZENITH','DEC_ZENITH']\n",
    "    with fits.open(filename) as hdu:\n",
    "        scdata = hdu['SC_DATA'].data\n",
    "        tstart, tstop = [float(hdu[0].header[field]) for field in  ('TSTART','TSTOP') ]\n",
    "\n",
    "    if config.verbose>1:\n",
    "        print(f'FT2: {filename.name}, MET range {tstart:.1f}-{tstop:.1f},', end='')# {\"not\" if gti is None else \"\"} applying GTI')\n",
    "\n",
    "    # get times to check against MJD limits and GTI\n",
    "    start, stop = [MJD(np.array(scdata.START, float)),\n",
    "                   MJD(np.array(scdata.STOP, float))]\n",
    "\n",
    "    # apply GTI to bin center (avoid edge effects?)\n",
    "    in_gti = gti(0.5*(start+stop))\n",
    "    if config.verbose>1:\n",
    "        s = sum(in_gti)\n",
    "        print(f' {len(start)} entries, {s} ({100*s/len(start):.1f}%) in GTI')\n",
    "\n",
    "    t = [('start', start[in_gti]), ('stop',stop[in_gti])]+\\\n",
    "        [(field.lower(), np.array(scdata[field][in_gti],np.float32)) for field in fields ]\n",
    "\n",
    "\n",
    "    sc_data = dict(t)\n",
    "\n",
    "    return sc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft2_info\" class=\"doc_header\"><code>get_ft2_info</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft2_info</code>(**`config`**, **`filename`**, **`gti`**=*`<lambda>`*)\n",
       "\n",
       "Process a FT2 file, with S/C history data, and return a summary dict\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* config -- verbose, cos_theta_max, z_max\n",
       "* filename -- spacecraft (FT2) file\n",
       "* gti -- GTI object that checkes for allowed intervals, in MJD units\n",
       "\n",
       "Returns: A dict with fields consistent with GTI if specified\n",
       "\n",
       "* start, stop -- interval in MJD units\n",
       "* livetime -- sec\n",
       "* ra_scz, dec_scz --spaceraft direction\n",
       "* ra_zenith, dec_zenith -- local zenith"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft2_info, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "def filepaths(week):\n",
    "    \"\"\"Returns: A tuple with two elements for the week number, each with two triplets with:\n",
    "        ftp folder, ftp filename, local simple filename\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for ftype, alias in  [('spacecraft','ft2'), ('photon','ft1')]:\n",
    "         urls.append((\n",
    "             f'{ftype}',\n",
    "             f'lat_{ftype}_weekly_w{week:03d}_p{\"305\" if ftype==\"photon\" else \"310\" }_v001.fits',\n",
    "             f'week{week:03d}_{alias}.fits',\n",
    "            ))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "class GSFCweekly(dict):\n",
    "\n",
    "    ftp_site = 'heasarc.gsfc.nasa.gov'\n",
    "    ftp_path = 'fermi/data/lat/weekly'\n",
    "    local_path  = '/tmp/from_gsfc'\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\" Obtain lists of the weekly FT1 and FT2 files at GSFC, Set up as a dict, with\n",
    "        keys= week numbers, values=mofification date strings\n",
    "        \"\"\"\n",
    "        self.config = config or Config()\n",
    "        # self.wtlike_data_file_path = Path(self.config.datapath/'data_files')\n",
    "        # assert self.wtlike_data_file_path.is_dir(), 'Data path invalid'\n",
    "        # os.makedirs(self.local_path, exist_ok=True)\n",
    "        try:\n",
    "            with FTP(self.ftp_site) as ftp:\n",
    "                ftp.login()\n",
    "                ftp.prot_p()\n",
    "                ftp.cwd(self.ftp_path+'/photon') # or spacecraft\n",
    "                # aet modification time and type for all files in folder\n",
    "\n",
    "                parse_week = lambda fn:  int(fn.split('_')[3][1:])\n",
    "                flist = ftp.mlsd(facts=['modify', 'type'])\n",
    "                self.fileinfo = sorted([(parse_week(name), fact['modify']) for name,fact in flist\n",
    "                                 if fact['type']=='file' and name.startswith('lat') ])\n",
    "        except Exception as msg:\n",
    "            print(f'FTP login to or download from {self.ftp_site} failed:\\n\\t--> {msg}',file=sys.stderr)\n",
    "            self.valid=False\n",
    "            return\n",
    "        self.update(self.fileinfo)\n",
    "        self.valid=True\n",
    "\n",
    "    def download(self, week):\n",
    "        \"\"\" Download the given week's files to the local folder\n",
    "\n",
    "        week -- the mission week number, starting at 9. If negative, get recent one\n",
    "\n",
    "        return the ft1, ft2 local filenames\n",
    "        \"\"\"\n",
    "\n",
    "        if week<0:\n",
    "            week = list(self.keys())[week]\n",
    "        assert week in self, f'week {week} not found at FTP site'\n",
    "        files = []\n",
    "        with FTP(self.ftp_site) as ftp:\n",
    "            ftp.login()\n",
    "            ftp.prot_p()\n",
    "            for ftp_folder, ftp_filename, local_filename in filepaths(week):\n",
    "                ftp.cwd('/'+self.ftp_path+'/'+ftp_folder)\n",
    "                if self.config.verbose>0:\n",
    "                    print(f'GSFCweekly: {ftp_folder}/{ftp_filename} --> {local_filename}', flush=True)\n",
    "                with open(f'{self.local_path}/{local_filename}', 'wb') as  localfile:\n",
    "                    ftp.retrbinary('RETR ' + ftp_filename, localfile.write)\n",
    "                files.append(local_filename)\n",
    "        return files\n",
    "\n",
    "    def week_number(self, met):\n",
    "        return (met-233711940)//(7*24*3600)\n",
    "\n",
    "    def load_week(self, week):\n",
    "        \"\"\"Load a pickled week summary \"\"\"\n",
    "        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'\n",
    "        assert filename.exists(), f'File {filename} does not exist'\n",
    "        with open(filename, 'rb') as imp:\n",
    "            ret = pickle.load(imp)\n",
    "        return ret\n",
    "\n",
    "    def check_week(self, week):\n",
    "        \"\"\"Returns True if the local week needs updating\"\"\"\n",
    "        data = self.load_week(week)\n",
    "        if 'file_date' not in data:\n",
    "            return True\n",
    "        # check that file date agrees\n",
    "        return data['file_date'] != self[week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise access from GSFC and FT1 data handling\n",
      "GSFCweekly: spacecraft/lat_spacecraft_weekly_w741_p310_v001.fits --> week741_ft2.fits\n",
      "GSFCweekly: photon/lat_photon_weekly_w741_p305_v001.fits --> week741_ft1.fits\n",
      "FT1: week741_ft1.fits, GTI range 681869208.7-682476252.1:  2,437,830 photons\n",
      "\tSelection E > 100 MeV. theta<66.4 and z<100 remove: 83.62%\n",
      "FT1 data from file week741_ft1.fits: tstart=681869208 (UTC 2022-08-11) selected 399,240/2,437,830 photons, in 107 runs.\n",
      "Contents of the output dict\n",
      " key      value type\n",
      "  tstart     float\n",
      "  photons    dict\n",
      "  gti_times  ndarray\n",
      "  runlist    ndarray\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "print( 'Exercise access from GSFC and FT1 data handling')\n",
    "gd = GSFCweekly(Config(verbose=3))\n",
    "if gd.config.valid:\n",
    "    ft2, ft1 = gd.download(-2)\n",
    "    v = get_ft1_data(gd.config, Path(gd.local_path)/ft1);\n",
    "\n",
    "    print('Contents of the output dict\\n'\n",
    "          ' key      value type')\n",
    "    for (k,x) in v.items():\n",
    "        print(f'  {k:10} {x.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FermiData(GSFCweekly):\n",
    "    \"\"\" Manage the full data set in weekly chunks\n",
    "    * Checking the current set of files at GSFC\n",
    "    * downloading a week at a time to a local tmp\n",
    "    * Converting to condensed format and saving to pickled dicts in wtlike_data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config)\n",
    "        self.wtlike_data_file_path = Path(self.config.datapath/'data_files')\n",
    "        assert self.wtlike_data_file_path.is_dir(), 'Data path is invalid'\n",
    "        os.makedirs(self.local_path, exist_ok=True)\n",
    "\n",
    "    @property\n",
    "    def local_filedate(self):\n",
    "        \"\"\" the datetime object representing the last file date in local storage\"\"\"\n",
    "        from dateutil.parser import parse\n",
    "        weekly_folder = self.config.datapath/'data_files'\n",
    "        ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "        if len(ff)==0:\n",
    "            print(f'FermiData: No weekly summary files found in {weekly_folder}', file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "        lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "        return dateutil.parser.parse(lastweek['file_date'])\n",
    "\n",
    "    @property\n",
    "    def gsfc_filedate(self):\n",
    "        return dateutil.parser.parse(list(self.values())[-1])\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'FermiData: {len(self.fileinfo)} week files at GSFC, from {self.fileinfo[0]} to {self.fileinfo[-1]}'\n",
    "\n",
    "    def in_temp(self):\n",
    "        \"\"\"return list of GSFC copied files in the local_path folder\"\"\"\n",
    "        names = [f.name for f in Path(self.local_path).glob('*')]\n",
    "        return names\n",
    "\n",
    "    def __call__(self, week, test=False, tries_left=2):\n",
    "        \"\"\" Download and convert the given week:\n",
    "        * download FT1 and FT2 from GSFC to scratch space\n",
    "        * convert each\n",
    "        * save pickled dict summary\n",
    "        * remove files\n",
    "\n",
    "        \"\"\"\n",
    "        assert week in self, f'week {week} not found at FTP site'\n",
    "        ff = filepaths(week)\n",
    "        ft1_file = Path(self.local_path)/ff[1][2]\n",
    "        ft2_file = Path(self.local_path)/ff[0][2]\n",
    "\n",
    "        if self.config.verbose>1:\n",
    "            print(f'FermiData: converting week {week}')\n",
    "\n",
    "        while tries_left>0:\n",
    "            try:\n",
    "                if not (ft1_file.exists() and ft2_file.exists()):\n",
    "                    self.download(week)\n",
    "                # convert photon data to compact form, as a dice\n",
    "                week_summary = get_ft1_data(self.config, ft1_file)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'*** ERROR *** Failed to convert {ft1_file}: {e} download it again)',\n",
    "                      file=sys.stderr)\n",
    "                tries_left -=1\n",
    "                if tries_left==0:\n",
    "                    print(f'Failed to convert week file {ft1_file}: quitting', file=sys.stderr)\n",
    "                    return None\n",
    "                else:\n",
    "                    os.unlink(ft1_file)\n",
    "\n",
    "        def apply_gti(time): # note MJD\n",
    "            x = np.digitize(time, MJD(week_summary['gti_times']))\n",
    "            return np.bitwise_and(x,1).astype(bool)\n",
    "        sc_data = get_ft2_info(self.config, ft2_file, apply_gti)\n",
    "\n",
    "        # finished with copies of FT1 and FT2 files: delete them\n",
    "        for file in (ft1_file,ft2_file):\n",
    "            os.unlink(file)\n",
    "\n",
    "        # add file date and space craft summary\n",
    "        week_summary['file_date'] = self[week]\n",
    "        week_summary['sc_data'] = sc_data\n",
    "        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'\n",
    "\n",
    "        if not test:\n",
    "            with open(filename, 'wb') as out:\n",
    "                pickle.dump(week_summary, out)\n",
    "\n",
    "            if self.config.verbose>0:\n",
    "                print(f'FermiData: {\"replaced\" if filename.exists() else \"saved to\" } {filename.name}', flush=True)\n",
    "        else:\n",
    "            print(f'testing... no change to {filename.name}')\n",
    "\n",
    "    def download_and_convert(self, week_range,  processes=None):\n",
    "        \"\"\"Download FT1 and FT2 files from GSFC and create summary files for the weeks\n",
    "\n",
    "        * week_range: a (first, last+1) tumple, or a iterable\n",
    "\n",
    "        \"\"\"\n",
    "        from multiprocessing import Pool\n",
    "\n",
    "        processes = processes or self.config.pool_size\n",
    "        txt = f', using {processes} processes ' if processes>1 else ''\n",
    "\n",
    "        if self.config.verbose>0:\n",
    "            print(f'\\tDownloading {len(week_range)} week files{txt}\\n', end='', flush=True)\n",
    "\n",
    "        if processes>1:\n",
    "            with Pool(processes=processes) as pool:\n",
    "                pool.map(self, week_range)\n",
    "        else:\n",
    "            list(map(self,  week_range))\n",
    "\n",
    "    def needs_update(self):\n",
    "        \"\"\" Compare files on disk with the GSFC list and compile list that need to be downloaded\n",
    "\n",
    "        Check the file date of the last one on disk and update it as well if it has a different filedate\n",
    "        \"\"\"\n",
    "        gg =self.wtlike_data_file_path.glob('*.pkl')\n",
    "        file_weeks= map(lambda n: int(n.name[5:8]), gg)\n",
    "        ondisk = np.array(list(file_weeks))\n",
    "\n",
    "        missing =  set(self.keys()).difference(set(ondisk))\n",
    "        if len(ondisk)==0:\n",
    "            return list(missing)\n",
    "\n",
    "        last = sorted(ondisk)[-1]\n",
    "        if self.check_week(last):\n",
    "             missing.add(last)\n",
    "        return sorted(list(missing))\n",
    "\n",
    "    def check_data(self):\n",
    "        \"\"\"\n",
    "        Return: sorted list of summary files, last week number, number of days in last week\"\"\"\n",
    "        config = self.config\n",
    "        if config.valid:\n",
    "            weekly_folder = config.datapath/'data_files'\n",
    "            ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "            if len(ff)==0:\n",
    "                print(f'No .pkl files found in {weekly_folder}', file=sys.stderr)\n",
    "                return\n",
    "            getwk = lambda f: int(os.path.splitext(f)[0][-3:])\n",
    "            wk = [getwk(f) for f in ff] #list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "            lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "\n",
    "            file_date = lastweek['file_date']\n",
    "            gti = lastweek['gti_times'];\n",
    "            days = (gti[-1]-gti[0])/(24*3600)\n",
    "            if config.verbose>0:\n",
    "                print(f'Weekly folder \"{weekly_folder}\" contains {len(wk)} weeks.'\\\n",
    "                      f'\\n\\t Last week in local dataset, #{wk[-1]}, has {days:.3f} days, ends at UTC {UTC(MJD(gti[-1]))}, filedate {file_date}' )\n",
    "            #return ff, wk[-1], days\n",
    "        else:\n",
    "            print(f'Config not valid, {config.errors}', file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "    def update_data(self):\n",
    "        \"\"\"Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.\n",
    "        If the last one is the current week, check to see if needs updating, by comparing file date, in days,\n",
    "        from the last update with the current one at GSFC.\n",
    "        \"\"\"\n",
    "        self.check_data()\n",
    "        needs = self.needs_update()\n",
    "        if len(needs)==0:\n",
    "            print('--> Up to date!')\n",
    "            return\n",
    "        return self.download_and_convert(needs)\n",
    "\n",
    "    def get_run_times(self, week):\n",
    "        r = self.load_week(week)\n",
    "        pdict = r['photons']\n",
    "        if 'run_id' in pdict:\n",
    "            runs = np.unique(pdict['run_id'])\n",
    "        elif 'run_ref' in pdict:\n",
    "            runs = r['runlist'][pdict['run_ref']]\n",
    "        else:\n",
    "            assert False\n",
    "        return MJD(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly folder \"/mnt/c/Users/thbur/onedrive/fermi/wtlike-data/data_files\" contains 731 weeks.\n",
      "\t Last week in local dataset, #740, has 5.131 days, ends at UTC 2022-08-09 03:24, filedate 20220809103709\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "config = Config()\n",
    "if config.valid:\n",
    "    self = FermiData(config)\n",
    "    self.check_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"FermiData\" class=\"doc_header\"><code>class</code> <code>FermiData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>FermiData</code>(**`config`**=*`None`*) :: [`GSFCweekly`](/wtlikedata_man.html#GSFCweekly)\n",
       "\n",
       "Manage the full data set in weekly chunks\n",
       "* Checking the current set of files at GSFC\n",
       "* downloading a week at a time to a local tmp\n",
       "* Converting to condensed format and saving to pickled dicts in wtlike_data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"GSFCweekly.download\" class=\"doc_header\"><code>GSFCweekly.download</code><a href=\"__main__.py#L34\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>GSFCweekly.download</code>(**`week`**)\n",
       "\n",
       "Download the given week's files to the local folder\n",
       "\n",
       "week -- the mission week number, starting at 9. If negative, get recent one\n",
       "\n",
       "return the ft1, ft2 local filenames"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FermiData.needs_update\" class=\"doc_header\"><code>FermiData.needs_update</code><a href=\"__main__.py#L117\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FermiData.needs_update</code>()\n",
       "\n",
       "Compare files on disk with the GSFC list and compile list that need to be downloaded\n",
       "\n",
       "Check the file date of the last one on disk and update it as well if it has a different filedate"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FermiData)\n",
    "show_doc(FermiData.download)\n",
    "show_doc(FermiData.needs_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_data(config=None, update=False):\n",
    "    \"\"\"\n",
    "    Print current status, and update if requested\n",
    "    \"\"\"\n",
    "    config = config or Config()\n",
    "    if not config.valid:\n",
    "        print('Config datapath is not valid', file=sys.stderr)\n",
    "        return\n",
    "    ff = FermiData(config)\n",
    "    ff.check_data()\n",
    "    print(f'Weeks needing download: {ff.needs_update()}')\n",
    "    if update:\n",
    "        ff.update_data()\n",
    "\n",
    "def update_data( config=None):\n",
    "    \"\"\"Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.\n",
    "    If the last one is the current week, check to see if needs updating, by comparing file date, in days,\n",
    "    from the last update with the current one at GSFC.\n",
    "    \"\"\"\n",
    "    ff = FermiData(config)\n",
    "    return ff.update_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"check_data\" class=\"doc_header\"><code>check_data</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>check_data</code>(**`config`**=*`None`*, **`update`**=*`False`*)\n",
       "\n",
       "Print current status, and update if requested"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"update_data\" class=\"doc_header\"><code>update_data</code><a href=\"__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>update_data</code>(**`config`**=*`None`*)\n",
       "\n",
       "Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.\n",
       "If the last one is the current week, check to see if needs updating, by comparing file date, in days,\n",
       "from the last update with the current one at GSFC."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(check_data,  title_level=2)\n",
    "show_doc(update_data, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def get_week_files(config, week_range=None):\n",
    "    \"\"\"Return list of week files\n",
    "\n",
    "    - week_range [None] -- tuple with inclusive range. If None, get all\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    data_folder = config.datapath/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    weeks = week_range or  config.week_range\n",
    "    if week_range is not None:\n",
    "\n",
    "        slc = slice(*week_range) if type(week_range)==tuple else slice(week_range,week_range)\n",
    "        wk_table = pd.Series(data=[df for df in data_files],\n",
    "                     index= [ int(df.name[-7:-4]) for df in  data_files],\n",
    "                    )\n",
    "        data_files = wk_table.loc[slc].values\n",
    "\n",
    "        if config.verbose>1:\n",
    "            q = lambda x: x if x is not None else \"\"\n",
    "            print(f'LoadData: Loading weeks[{q(slc.start)}:{q(slc.stop)}:{q(slc.step)}]', end='' if config.verbose<2 else '\\n')\n",
    "    else:\n",
    "        if config.verbose>1: print(f'LoadData: loading all {len(data_files)} weekly files')\n",
    "\n",
    "    if len(data_files)==0:\n",
    "        msg =  f'Specified week_range {week_range} produced no output. Note that week numbers are 9-'\n",
    "        raise Exception(msg)\n",
    "\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataView(object):\n",
    "    \"\"\"\n",
    "    Manage various views of the data set\n",
    "    \"\"\"\n",
    "    def __init__(self,  times:tuple=(0,0),  config=None, nside=128, bmin=0):\n",
    "        \"\"\"\n",
    "        Constructor selects a time range\n",
    "\n",
    "        - times\n",
    "        - nside [128] HEALPix nside to use\n",
    "        - bmin [0] minimum energy band index\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # convert times to MJD, then week range\n",
    "        self.config = config or Config()\n",
    "        assert self.config.valid, f'Config not valid:\\n{self.config.error_msg}'\n",
    "        self.nside=nside\n",
    "        self.bmin=bmin\n",
    "\n",
    "        # extract MJD range\n",
    "        self.time_range = mjd_range(*times)\n",
    "\n",
    "        # get corresponding week range, and list of all week files\n",
    "        start, stop = [mission_week(x) for x in self.time_range]\n",
    "        self.week_files = get_week_files(self.config, (start, stop))\n",
    "\n",
    "        self.time_filter = lambda time: (time>self.time_range[0]) & (time<self.time_range[1])\n",
    "\n",
    "    def get_week_data(self, i):\n",
    "        file = self.week_files[i]\n",
    "        with open(file, 'rb') as inp:\n",
    "            return pickle.load(inp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.week_files)\n",
    "\n",
    "\n",
    "    def count_map(self, nside=None, bmin=None):\n",
    "        \"\"\" all-sky count map\n",
    "\n",
    "        - nside [64] Project map to this value, from data's 1024\n",
    "        - bmin [0] minimum band index (8 for 1 GeV cut)\n",
    "\n",
    "        Return a HEALPix counts map, in RING ordering\n",
    "        \"\"\"\n",
    "\n",
    "        def get_map(file):\n",
    "\n",
    "            with open(file, 'rb') as inp:\n",
    "                u = pickle.load(inp)\n",
    "            phd = photon_week(self.config, u)\n",
    "            time = phd['time']\n",
    "            filter = self.time_filter(time)\n",
    "            if bmin>0:\n",
    "                filter = filter & (phd['band']>=self.bmin)\n",
    "            ni = phd['nest_index'][filter]\n",
    "\n",
    "            # to convert from data nside (1024 normally)\n",
    "            shift = int(np.log2(self.config.nside//self.nside))\n",
    "            n = 12*nside**2\n",
    "            pmap,_ = np.histogram( np.right_shift(ni,2*shift), np.linspace(0,n,n+1))\n",
    "            return pmap\n",
    "\n",
    "        nside = nside or self.nside\n",
    "        bmin = bmin or self.bmin\n",
    "        pmap = get_map(self.week_files[0],)\n",
    "\n",
    "        if len(self.week_files)>1:\n",
    "            for week in self.week_files[1:]:\n",
    "                pmap += get_map(week)\n",
    "        # finally reorder\n",
    "        return healpy.reorder(pmap, n2r=True)\n",
    "\n",
    "    def livetime_map(self, nside=None,  sigma=0):\n",
    "        \"\"\" all-sky pointing livetime map\n",
    "\n",
    "        - nside [None]\n",
    "        - sigma: Gaussian smooting parameter (degrees)\n",
    "\n",
    "        Return a HEALPix map, RING ordering, of the integrated livetime per pixel\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        from astropy.coordinates import SkyCoord\n",
    "        nside = nside or self.nside\n",
    "        N = 12*nside**2\n",
    "\n",
    "        def get_ltmap(file):\n",
    "            with open(file, 'rb') as inp:\n",
    "                u =pickle.load(inp)\n",
    "            sc_data = u['sc_data']\n",
    "            filter = self.time_filter(sc_data['start'])\n",
    "            #print('filter:', sum(filter), '/', len(filter))\n",
    "            df = pd.DataFrame(sc_data)[filter]\n",
    "\n",
    "            coords =  SkyCoord(df.ra_scz, df.dec_scz, unit='deg', frame='fk5').galactic\n",
    "            indx= healpy.ang2pix(nside, coords.l.deg, coords.b.deg, lonlat=True)\n",
    "            ltmap, _ = np.histogram( indx, bins=np.arange(N+1),  weights=df.livetime)\n",
    "            return ltmap\n",
    "\n",
    "        ltmap = get_ltmap(self.week_files[0],)\n",
    "        if len(self.week_files)>1:\n",
    "            for week in self.week_files[1:]:\n",
    "                ltmap += get_ltmap(week)\n",
    "        if sigma>0:\n",
    "            ltmap = healpy.smoothing(ltmap, np.radians(sigma))\n",
    "        return ltmap\n",
    "\n",
    "    def exposure_map(self, beam_window, nside=None,):\n",
    "        \"\"\" ### all-sky exposure map\n",
    "\n",
    "        - beam_window --a list of coefficients of a Legendre polynomial expansion\n",
    "        - nside [None]\n",
    "\n",
    "        Return a HEALPix map of the weighted exposure\n",
    "        \"\"\"\n",
    "        nside = nside or self.nside\n",
    "\n",
    "        ltmap = self.livetime_map(nside=nside)\n",
    "\n",
    "        # do a spherical convolution of the live-time map with a aeff beam window\n",
    "        ### NEED TO ALLOW for BMIN!\n",
    "        return healpy.alm2map(\n",
    "                healpy.smoothalm(\n",
    "                    healpy.map2alm(ltmap),\n",
    "                    beam_window=beam_window,\n",
    "                    ),\n",
    "                nside=nside,\n",
    "                )\n",
    "    def flux_map(self, beam_window=None):\n",
    "        \"\"\"\n",
    "        Return a flux map, the ratio of counts/exposure / sr\n",
    "        \n",
    "        where the counts are now divided by the pixel solid angle.\n",
    "        So units are now: counts cm-2 s-1 sr-1\n",
    "        \"\"\"\n",
    "        return self.count_map()/self.exposure_map(beam_window) * 12*self.nside**2/(4*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def sc_process(config, skycoord, sc_data):\n",
    "\n",
    "    \"\"\"\n",
    "    - skycoord -- direction\n",
    "    - sc_data -- DF constructed from spacecraft data (FT2).\n",
    "\n",
    "    Return: a dict with the S/C data for the source direction, wtih cos theta and zenith cuts\n",
    "\n",
    "    columns:\n",
    "    - start, stop, livetime -- from the FT2 info\n",
    "    - cos_theta -- angle between bore and direction\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = skycoord.fk5\n",
    "    ra_r,dec_r = sc.ra.radian, sc.dec.radian\n",
    "    \n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    cos_thetas = cosines(sc_data.ra_scz,    sc_data.dec_scz)\n",
    "    zcosines = cosines(sc_data.ra_zenith, sc_data.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (cos_thetas >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = sc_data.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "    \n",
    "    return  pd.DataFrame(\n",
    "        dict(\n",
    "            start=sc_data.start[mask].astype(np.float64),\n",
    "            stop=sc_data.stop[mask].astype(np.float64),\n",
    "            livetime=livetime,\n",
    "            cos_theta=cos_thetas[mask],\n",
    "            )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from astropy.coordinates import SkyCoord\n",
    "# dv = DataView()\n",
    "# file = dv.week_files[1]\n",
    "# with open(file, 'rb') as inp:\n",
    "#     u = pickle.load(inp)\n",
    "# # sc = SkyCoord(0,0,unit='deg', frame='galactic')z\n",
    "# sc_data = pd.DataFrame(u['sc_data'])\n",
    "\n",
    "# %time sc_process(Config(verbose=1), sc, sc_data).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### DataView: summarize data for week 1\n",
       "\n",
       "dict keys: <p style=\"margin-left: 5%\"><samp>['tstart', 'photons', 'gti_times', 'runlist', 'file_date', 'sc_data']</samp></p>\n",
       "<details open class=\"nbdoc-description\" >  <summary> sc_data nfo </summary>  <div style=\"margin-left: 5%;\"><pre>&lt;class 'pandas.core.frame.DataFrame'&gt;<br>RangeIndex: 16826 entries, 0 to 16825<br>Data columns (total 7 columns):<br> #   Column      Non-Null Count  Dtype  <br>---  ------      --------------  -----  <br> 0   start       16826 non-null  float64<br> 1   stop        16826 non-null  float64<br> 2   livetime    16826 non-null  float32<br> 3   ra_scz      16826 non-null  float32<br> 4   dec_scz     16826 non-null  float32<br> 5   ra_zenith   16826 non-null  float32<br> 6   dec_zenith  16826 non-null  float32<br>dtypes: float32(5), float64(2)<br>memory usage: 591.7 KB<br></pre></div> </details>\n",
       "<details open class=\"nbdoc-description\" >  <summary> photons info </summary>  <div style=\"margin-left: 5%;\"><pre>&lt;class 'pandas.core.frame.DataFrame'&gt;<br>RangeIndex: 352693 entries, 0 to 352692<br>Data columns (total 4 columns):<br> #   Column      Non-Null Count   Dtype <br>---  ------      --------------   ----- <br> 0   band        352693 non-null  uint8 <br> 1   nest_index  352693 non-null  uint32<br> 2   run_ref     352693 non-null  uint8 <br> 3   trun        352693 non-null  uint32<br>dtypes: uint32(2), uint8(2)<br>memory usage: 3.4 MB<br></pre></div> </details>\n"
      ],
      "text/plain": [
       "<utilities.ipynb_docgen.doc_formatter.<locals>.MimeBundleObject at 0x7fb86e988b20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "from utilities.ipynb_docgen import *\n",
    "@ipynb_doc\n",
    "def dataview_dump(week):\n",
    "    \"\"\"\n",
    "    ### DataView: summarize data for week {week}\n",
    "    \n",
    "    dict keys: {keys}\n",
    "    {sc_info}\n",
    "    {photon_info}\n",
    "    \"\"\"\n",
    "    dv = DataView()\n",
    "    u = dv.get_week_data(week)\n",
    "    \n",
    "    keys = list(u.keys())\n",
    "\n",
    "    import pandas as pd\n",
    "    with capture_show('sc_data nfo') as sc_info:\n",
    "        pd.DataFrame(u['sc_data']).info()\n",
    "    with capture_show('photons info') as photon_info:\n",
    "        pd.DataFrame(u['photons']).info()\n",
    "\n",
    "    return locals()\n",
    "\n",
    "if Config().valid:\n",
    "    dataview_dump(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"DataView\" class=\"doc_header\"><code>class</code> <code>DataView</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>DataView</code>(**`times`**:`tuple`=*`(0, 0)`*, **`config`**=*`None`*, **`nside`**=*`128`*, **`bmin`**=*`0`*)\n",
       "\n",
       "Manage various views of the data set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"DataView.count_map\" class=\"doc_header\"><code>DataView.count_map</code><a href=\"__main__.py#L40\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>DataView.count_map</code>(**`nside`**=*`None`*, **`bmin`**=*`None`*)\n",
       "\n",
       "all-sky count map \n",
       "\n",
       "- nside [64] Project map to this value, from data's 1024 \n",
       "- bmin [0] minimum band index (8 for 1 GeV cut) \n",
       "\n",
       "Return a HEALPix counts map, in RING ordering"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(DataView)\n",
    "show_doc(DataView.count_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# # export\n",
    "# def get_week_map(week=None,  nside=32  ):\n",
    "#     \"\"\"Return a HEALPix count map for the given week, selected by week index\n",
    "    \n",
    "#     - week -- week number \n",
    "#     - nside -- [32] \n",
    "    \n",
    "#     Return:\n",
    "#         a HEALPix array, RING ordering\n",
    "#     \"\"\"\n",
    "       \n",
    "#     assert nside & (nside-1) == 0, 'nside must be power of 2'\n",
    "#     config = Config()\n",
    "#     if not config.valid:\n",
    "#         print('No data to plot since config not valid', file=sys.stderr)\n",
    "#         return\n",
    "    \n",
    "#     file = get_week_files(config,(week,week))[0]\n",
    "#     with open(file, 'rb') as inp:\n",
    "#         u = pickle.load(inp)\n",
    "#     ni = u['photons']['nest_index']\n",
    "#     # utc = UTC(MJD((u['gti_times'][0])))[:-5]\n",
    "#     n = 12*nside**2\n",
    "#     # to convert from data nside (1024 normally)\n",
    "#     shift = int(np.log2(config.nside//nside))\n",
    "#     pmap,_ = np.histogram( np.right_shift(ni,2*shift), np.linspace(0,n,n+1)) \n",
    "    \n",
    "#     # convert to RING ordering\n",
    "#     return healpy.reorder(pmap, n2r=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 03_sources.ipynb.\n",
      "Converted 04_load_data.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 04_skymaps.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 10-time_series.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_presentation.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Wed Aug 24 08:53:07 PDT 2022\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
