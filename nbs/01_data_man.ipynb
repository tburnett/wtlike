{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_man\n",
    "from nbdev import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data management\n",
    "> Create, from FT1 and FT2, a compact data set with photon and livetime info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Fermi-LAT weekly data files are extracted from the [GSFC FTP server ](https://heasarc.gsfc.nasa.gov/FTP/fermi/data/lat/weekly), \n",
    "with subfolders for the photon data, `photon` and spacecraft data, `spacecraft`. It is [described here](http://fermi.gsfc.nasa.gov/ssc/data/access/http://fermi.gsfc.nasa.gov/ssc/data/access/)\n",
    "\n",
    "The class `WeeklyData` downloads these to temporary files and constructs a dict for each week with\n",
    "contents\n",
    "\n",
    "* photons: a table, entry per selected photon with columns \n",
    "  * time since the weekly tstart (float32)\n",
    "  * energy and event type (uint8)\n",
    "  * position as HEALPix index (uint32)\n",
    "* sc_data: a table, an entry per 30-s interval, with columns, all float32\n",
    "  * start/stop time \n",
    "  * S/C direction \n",
    "  * zenith direction\n",
    "* gti_times: an array of interleaved start/stop intervals\n",
    "\n",
    "These dict objects, one per week, are saved in a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Use run number for offset instead of week time\n",
    "Save run number (int32) as \"sparse array\" \n",
    "A run is at most 6 ks. If I integerize the offset from this with 32 bits, I have 5e5 intervals/s, 2 us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "from astropy.io import fits\n",
    "\n",
    "import healpy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 2)\n",
    "import pickle\n",
    "from wtlike.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft1_data( config, ft1_file):\n",
    "\n",
    "        \"\"\"\n",
    "        Read in a photon data (FT1) file, bin in energy and position to convert to a compact DataFrame\n",
    "\n",
    "        - `ft1_file` -- A monthly file generated by J. Ballet, or a weekly file from GSFC\n",
    "\n",
    "        Depends on config items\n",
    "        - `theta_cut, z_cut` -- selection criteria\n",
    "        - `ebins, etypes` -- define band index\n",
    "        - `nside, nest` -- define HEALPix binning\n",
    "\n",
    "        Returns a tuple with\n",
    "\n",
    "        - `tstart`, the start MET time\n",
    "\n",
    "        - DataFrame  with columns\n",
    "           - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
    "           - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
    "           - `run_diff`   Run number difference from previous entry. (Saved as spase arary)\n",
    "           - `time` (float32):    the elapsed time in s from header value TSTART in the FT1 file\n",
    "           - `rtime` (float32): MET time relative to run id.\n",
    "\n",
    "        - gti times as an interleaved start, stop array.\n",
    "\n",
    "        For the selected events above 100 MeV, this represents 9 bytes per photon, vs. 27.\n",
    "        \"\"\"\n",
    "\n",
    "        delta_t = config.offset_size\n",
    "        ebins = config.energy_edges\n",
    "        etypes = config.etypes\n",
    "        nside = config.nside\n",
    "        nest = config.nest\n",
    "        z_cut =config.z_max\n",
    "        theta_cut = np.degrees(np.arccos(config.cos_theta_max))\n",
    "        verbose = config.verbose\n",
    "\n",
    "        with  fits.open(ft1_file) as ft1:\n",
    "            tstart = ft1[1].header['TSTART']\n",
    "\n",
    "            ## GTI - setup raveled array function to make cut\n",
    "            gti_data= ft1['GTI'].data\n",
    "            # extract arrays for values of interest\n",
    "            data =ft1['EVENTS'].data\n",
    "\n",
    "        a,b = sorted(gti_data.START), sorted(gti_data.STOP)\n",
    "\n",
    "        gti_times = np.ravel(np.column_stack((a,b)))\n",
    "        if np.any(np.diff(gti_times)<0):\n",
    "            raise Exception(f'Non-monatonic GTI found')\n",
    "\n",
    "        def apply_gti(time):\n",
    "            x = np.digitize(time, gti_times)\n",
    "            return np.bitwise_and(x,1).astype(bool)\n",
    "\n",
    "        # apply  selections\n",
    "\n",
    "        sel =  ((data['ENERGY'] > ebins[0]) &\n",
    "                (data['ZENITH_ANGLE'] < z_cut) &\n",
    "                (data['THETA'] < theta_cut))\n",
    "\n",
    "        dsel = data[sel]\n",
    "\n",
    "        # get the columns for output\n",
    "        glon, glat, energy, et, z, theta, time, ec =\\\n",
    "             [dsel[x] for x in 'L B ENERGY EVENT_TYPE ZENITH_ANGLE THETA TIME EVENT_CLASS'.split()]\n",
    "\n",
    "        # generate event_type masks\n",
    "        et_mask={}\n",
    "        for ie in etypes:\n",
    "            et_mask[ie]= et[:,-1-ie]\n",
    "\n",
    "\n",
    "        if verbose>1:\n",
    "            total = sum(b)-sum(a)\n",
    "            fraction = total/(b[-1]-a[0])\n",
    "\n",
    "            print(  f'FT1: {ft1_file.name}, GTI range {a[0]:.1f}-{b[-1]:.1f}:  {len(data):,} photons'\\\n",
    "                    f'\\n\\tSelection E > {ebins[0]:.0f} MeV. theta<{theta_cut:.1f} and z<{z_cut} remove:'\\\n",
    "                    f' {100.- 100*len(dsel)/float(len(data)):.2f}%'\n",
    "#                     f', GTI cut removes {sum(~gti_cut)}'\n",
    "                 )\n",
    "\n",
    "\n",
    "        # event class -- turn into single int for later mask\n",
    "#         bits = np.array([1<<n for n in range(20)])\n",
    "#         def to_bin(x):\n",
    "#             return np.sum(bits[x[:20]])\n",
    "#         ec = [to_bin(row[20]) for row in ec\n",
    "\n",
    "        # pixelate direction\n",
    "        hpindex = healpy.ang2pix(nside, glon, glat, nest=nest, lonlat=True).astype(np.uint32)\n",
    "        hpname = 'nest_index' if nest else 'ring_index'\n",
    "\n",
    "        # digitize energy and create band index incluing (front/back)\n",
    "        band_index = (2*(np.digitize(energy, ebins, )-1) + et_mask[1]).astype(np.uint8)\n",
    "\n",
    "        #\n",
    "        run_id = dsel['RUN_ID'].astype(np.uint32)\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "                {   'band'  : band_index,\n",
    "                    hpname  : hpindex,\n",
    "                    #'time'  : (time-tstart).astype(np.float32), # the old time\n",
    "                    'run_id': pd.Categorical(run_id),\n",
    "                    'trun'  : ((time-run_id)/delta_t).astype(np.uint32),\n",
    "                }  )\n",
    "        if verbose>1:\n",
    "            print(f'\\tReturning tstart={tstart:.0f}, {len(dsel):,} photons.')\n",
    "\n",
    "        return  tstart, df, gti_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft1_data\" class=\"doc_header\"><code>get_ft1_data</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft1_data</code>(**`config`**, **`ft1_file`**)\n",
       "\n",
       "Read in a photon data (FT1) file, bin in energy and position to convert to a compact DataFrame\n",
       "\n",
       "- `ft1_file` -- A monthly file generated by J. Ballet, or a weekly file from GSFC\n",
       "\n",
       "Depends on config items\n",
       "- `theta_cut, z_cut` -- selection criteria\n",
       "- `ebins, etypes` -- define band index\n",
       "- `nside, nest` -- define HEALPix binning\n",
       "\n",
       "Returns a tuple with\n",
       "\n",
       "- `tstart`, the start MET time\n",
       "\n",
       "- DataFrame  with columns\n",
       "   - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
       "   - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
       "   - `run_diff`   Run number difference from previous entry. (Saved as spase arary)\n",
       "   - `time` (float32):    the elapsed time in s from header value TSTART in the FT1 file\n",
       "   - `rtime` (float32): MET time relative to run id.\n",
       "\n",
       "- gti times as an interleaved start, stop array.\n",
       "\n",
       "For the selected events above 100 MeV, this represents 9 bytes per photon, vs. 27."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft1_data, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft2_info(config, filename,\n",
    "                 gti=lambda t: True):\n",
    "    \"\"\"Process a FT2 file, with S/C history data, and return a summary DataFrame\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * config -- verbose, cos_theta_max, z_max\n",
    "    * filename -- spacecraft (FT2) file\n",
    "    * gti -- GTI object that checkes for allowed intervals, in MJD units\n",
    "\n",
    "    Returns: A DataFrame with fields consistent with GTI if specified\n",
    "\n",
    "    * start, stop -- interval in MJD units\n",
    "    * livetime -- sec\n",
    "    * ra_scz, dec_scz --spaceraft direction\n",
    "    * ra_zenith, dec_zenith -- local zenith\n",
    "    \"\"\"\n",
    "    # combine the files into a DataFrame with following fields besides START and STOP (lower case for column)\n",
    "    fields    = ['LIVETIME','RA_SCZ','DEC_SCZ', 'RA_ZENITH','DEC_ZENITH']\n",
    "    with fits.open(filename) as hdu:\n",
    "        scdata = hdu['SC_DATA'].data\n",
    "        tstart, tstop = [float(hdu[0].header[field]) for field in  ('TSTART','TSTOP') ]\n",
    "\n",
    "    if config.verbose>1:\n",
    "        print(f'FT2: {filename.name}, MET range {tstart:.1f}-{tstop:.1f},', end='')# {\"not\" if gti is None else \"\"} applying GTI')\n",
    "\n",
    "    # get times to check against MJD limits and GTI\n",
    "    start, stop = [MJD(np.array(scdata.START, float)),\n",
    "                   MJD(np.array(scdata.STOP, float))]\n",
    "\n",
    "    # apply GTI to bin center (avoid edge effects?)\n",
    "    in_gti = gti(0.5*(start+stop))\n",
    "    if config.verbose>1:\n",
    "        s = sum(in_gti)\n",
    "        print(f' {len(start)} entries, {s} ({100*s/len(start):.1f}%) in GTI')\n",
    "\n",
    "    t = [('start', start[in_gti]), ('stop',stop[in_gti])]+\\\n",
    "        [(field.lower(), np.array(scdata[field][in_gti],np.float32)) for field in fields ]\n",
    "\n",
    "    sc_data = pd.DataFrame(dict(t) )\n",
    "\n",
    "    return sc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft2_info\" class=\"doc_header\"><code>get_ft2_info</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft2_info</code>(**`config`**, **`filename`**, **`gti`**=*`<lambda>`*)\n",
       "\n",
       "Process a FT2 file, with S/C history data, and return a summary DataFrame\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* config -- verbose, cos_theta_max, z_max\n",
       "* filename -- spacecraft (FT2) file\n",
       "* gti -- GTI object that checkes for allowed intervals, in MJD units\n",
       "\n",
       "Returns: A DataFrame with fields consistent with GTI if specified\n",
       "\n",
       "* start, stop -- interval in MJD units\n",
       "* livetime -- sec\n",
       "* ra_scz, dec_scz --spaceraft direction\n",
       "* ra_zenith, dec_zenith -- local zenith"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft2_info, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hide\n",
    "# filename =Path('/tmp/from_gsfc/week668_ft2.fits')\n",
    "# if os.path.exists(filename):\n",
    "#     config = Config()\n",
    "#     config.verbose=3\n",
    "#     t = get_ft2_info(config, filename); \n",
    "#     print(t.head())\n",
    "# else:\n",
    "#     print(f'File {filename} does not exist, skipping get_ft2_info test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WeeklyData(object):\n",
    "    \"\"\"Download and process weekly Fermi-LAT files from GSFC,\n",
    "\n",
    "    at FTP 'https://heasarc.gsfc.nasa.gov/FTP/fermi/data/lat/weekly'\n",
    "\n",
    "    * week: a mission week number\n",
    "    * saveto: path to save the files\n",
    "\n",
    "    Creates a pickled file as  dict with\n",
    "\n",
    "    * tstart--start time (MJD)\n",
    "    * photons -- DataFrame with photon data--see `get_ft1_data`\n",
    "    * sc_data -- DataFrame with Spacecraft data--see `get_ft2_info`\n",
    "    * gti_times -- array of interleaved start/stop times from the photon data\n",
    "    \"\"\"\n",
    "\n",
    "    ftp = 'https://heasarc.gsfc.nasa.gov/FTP/fermi/data/lat/weekly'\n",
    "    tmp = Path('/tmp/from_gsfc')\n",
    "\n",
    "    def __init__(self, config, week,  overwrite=False):\n",
    "        \"\"\"\n",
    "        * week: a mission week number, starting at 9\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        import wget\n",
    "        self.config= config\n",
    "        self.saveto=Path(config.wtlike_data/'data_files')\n",
    "        os.makedirs(self.saveto, exist_ok=True)\n",
    "        assert week>8\n",
    "        self.wk = week\n",
    "\n",
    "        self.ft2_file, self.ft1_file = fits_files = [self.tmp/f'week{week:03d}_{x}.fits'\n",
    "                                                     for x in 'ft2 ft1'.split()]\n",
    "        os.makedirs(self.tmp, exist_ok=True)\n",
    "\n",
    "        urls = []\n",
    "        for ftype in  ['spacecraft', 'photon']:\n",
    "             urls.append(f'{self.ftp}/{ftype}/lat_{ftype}_weekly_w{week:03d}_p{\"305\" if ftype==\"photon\" else \"310\" }_v001.fits')\n",
    "\n",
    "        for url, fname in zip(urls, fits_files):\n",
    "            if not fname.exists() or overwrite:\n",
    "                if config.verbose>1:\n",
    "                    print(f'{url.split(\"/\")[-1]} -> {fname}')\n",
    "                try:\n",
    "                    if fname.exists(): os.remove(fname)\n",
    "                    wget.download(str(url), str(fname))\n",
    "                except Exception as msg:\n",
    "                    print(f'Failed to download {url}: {msg}')\n",
    "                    raise\n",
    "            else:\n",
    "                if config.verbose>1: print(f'{fname} exists')\n",
    "\n",
    "        self.process_ft1()\n",
    "        self.process_ft2()\n",
    "\n",
    "\n",
    "    def process_ft1(self):\n",
    "        self.tstart, self.photon_data, self.gti_times = get_ft1_data(self.config, self.ft1_file)\n",
    "\n",
    "    def process_ft2(self):\n",
    "\n",
    "        def apply_gti(time): # note MJD\n",
    "            x = np.digitize(time, MJD(self.gti_times))\n",
    "            return np.bitwise_and(x,1).astype(bool)\n",
    "\n",
    "        self.sc_data = get_ft2_info(self.config, self.ft2_file, apply_gti)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"process, then save aa dict\"\"\"\n",
    "\n",
    "#         self.process_ft1()\n",
    "#         self.process_ft2()\n",
    "\n",
    "        d = dict(tstart = self.tstart,\n",
    "                photons = self.photon_data,\n",
    "                sc_data = self.sc_data,\n",
    "                gti_times = self.gti_times)\n",
    "        filename = self.saveto/f'week_{self.wk:03d}.pkl'\n",
    "        pickle.dump(d, open(filename, 'wb'))\n",
    "        if self.config.verbose>0:\n",
    "            print(f'Saved to {filename}')\n",
    "        if self.config.verbose>1:\n",
    "            print(self.photon_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"WeeklyData\" class=\"doc_header\"><code>class</code> <code>WeeklyData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>WeeklyData</code>(**`config`**, **`week`**, **`overwrite`**=*`False`*)\n",
       "\n",
       "Download and process weekly Fermi-LAT files from GSFC,\n",
       "\n",
       "at FTP 'https://heasarc.gsfc.nasa.gov/FTP/fermi/data/lat/weekly'\n",
       "\n",
       "* week: a mission week number\n",
       "* saveto: path to save the files\n",
       "\n",
       "Creates a pickled file as  dict with\n",
       "\n",
       "* tstart--start time (MJD)\n",
       "* photons -- DataFrame with photon data--see [`get_ft1_data`](/wtlikedata_man.html#get_ft1_data)\n",
       "* sc_data -- DataFrame with Spacecraft data--see [`get_ft2_info`](/wtlikedata_man.html#get_ft2_info)\n",
       "* gti_times -- array of interleaved start/stop times from the photon data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(WeeklyData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the weekly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def data_check(config=None):\n",
    "    \"\"\"\n",
    "    Return: list of files, last week number, number of days in last week\"\"\"\n",
    "    if config is None: config=Config()\n",
    "    if config.valid:\n",
    "        weekly_folder = config.wtlike_data/'data_files'\n",
    "        ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "        if len(ff)==0:\n",
    "            print(f'No .pkl files found in {weekly_folder}', file=sys.stderr)\n",
    "            return\n",
    "        wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "        lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "        gti = lastweek['gti_times'];\n",
    "        days = (gti[-1]-gti[0])/(24*3600)\n",
    "        if config.verbose>0:\n",
    "            print(f'Weekly folder \"{weekly_folder}\" contains {len(wk)} weeks.'\\\n",
    "            f'\\n\\t Last week, # {wk[-1]}, has {days:.3f} days, ends at UTC {UTC(MJD(gti[-1]))}' )\n",
    "        return ff, wk[-1], days\n",
    "    else:\n",
    "        print(f'Config not valid, {config.errors} no files found', file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def check_data(config=None):\n",
    "    data_check(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def update_recent(config=None, test=False):\n",
    "    if config is None: config=Config()\n",
    "    ff, last_wk, days = data_check(config);\n",
    "    if days<6.98:\n",
    "        wk = last_wk\n",
    "        print(f'Will reload week {last_wk} ...')\n",
    "    else:\n",
    "        wk = last_wk+1\n",
    "        print(f'Will load next week, # {last_wk+1} ...')\n",
    "    if not test:\n",
    "        WeeklyData(config, wk, overwrite=True).save()\n",
    "        _,last_wk, new_days = data_check(config)\n",
    "        #print(f'Now {days}')\n",
    "        return new_days!=days\n",
    "    else:\n",
    "        print('... but testing')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly folder \"/home/burnett/wtlike_data/data_files\" contains 673 weeks.\n",
      "\t Last week, # 682, has 2.354 days, ends at UTC 2021-06-26 08:51\n",
      "Will reload week 682 ...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4a41ea90a229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# #hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#check_data();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mupdate_recent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#WeeklyData(Config(), 680, overwrite=True).save()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3ee06edaccdf>\u001b[0m in \u001b[0;36mupdate_recent\u001b[0;34m(config, test)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Will load next week, # {last_wk+1} ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mWeeklyData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_wk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_days\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#print(f'Now {days}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5bd13a61d531>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, week, overwrite)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwtlike_data\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'data_files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "# #hide\n",
    "#check_data();\n",
    "#update_recent();\n",
    "#WeeklyData(Config(), 680, overwrite=True).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export    \n",
    "def get_data_files(config=None):\n",
    "    \"\"\"\n",
    "    Return a list of the pickled data files\n",
    "    \"\"\"\n",
    "    if config.valid:\n",
    "        weekly_folder = config.wtlike_data/'data_files'\n",
    "        ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "        if len(ff)==0:\n",
    "            print(f'No .pkl files found in {weekly_folder}', file=sys.stderr)\n",
    "            return\n",
    "        wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "        lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "        if config.verbose>0:\n",
    "            gti = lastweek['gti_times'];\n",
    "            days = (gti[-1]-gti[0])/(24*3600)\n",
    "            print(f'Weekly folder \"{weekly_folder}\" contains {len(wk)} weeks,  {wk[0]} to {wk[-1]},'\\\n",
    "            f' last week has {days:.1f} days, ends at {UTC(MJD(gti[-1]))}' )\n",
    "        return ff\n",
    "    else:\n",
    "        print(f'Config not valid, {config.errors} no files found', file=sys.stderr)\n",
    "        return []\n",
    "\n",
    "\n",
    "def load_weeks(week_range, config=None, overwrite=True):\n",
    "    if config is None: config=Config()\n",
    "    assert config.valid\n",
    "    tmp = Path('/tmp/from_gsfc')\n",
    "    os.makedirs(tmp, exist_ok=True)\n",
    "    files_on_disk  = list(tmp.glob('*.fits'))\n",
    "#     print(f'Currently {len(files_on_disk)} FTx files on disk at {tmp}')\n",
    "    print(f'Downloading range {week_range})')\n",
    "\n",
    "    for week in range(*week_range):\n",
    "        fits_files = [tmp/f'week{week:03d}_{x}.fits'  for x in 'ft2 ft1'.split()]\n",
    "\n",
    "        is_on_disk = fits_files[0] in files_on_disk or fits_files[1] in files_on_disk\n",
    "        if is_on_disk:\n",
    "            if not overwrite:\n",
    "                print(f'Files for week {week} on disk--not overwriting')\n",
    "                return\n",
    "            else:\n",
    "                print(f'Will overwrite existing week {week} files')\n",
    "                [os.remove(f) for f in files_on_disk]\n",
    "        print(f'week {week} files ... ', end='')\n",
    "        WeeklyData(config,  week, overwrite=overwrite).save()\n",
    "        for f in fits_files:\n",
    "            f.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config=Config(); config.verbose=2\n",
    "# weeks = range(678,682)\n",
    "# wd=[]\n",
    "# for wk in weeks:\n",
    "# #     t = WeeklyData(config, week=wk); t.save()\n",
    "#     wd.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_data(ax, week):\n",
    "#     pixels = wd[week-678].photon_data.nest_index.values\n",
    "#     import healpy\n",
    "#     l,b = healpy.pix2ang(nside=1024, ipix=pixels, nest=True,lonlat=True)\n",
    "#     l[l>180]-=360\n",
    "#     sinb = np.sin(np.radians(b))\n",
    "#     ax.set(xlim=(180,-180), ylim=(-1,1), xlabel='$l$', ylabel=r'$\\mathrm{\\sin(b)}$',\n",
    "#           title=f'week {week}')\n",
    "#     u = ax.hexbin(l,sinb, bins='log');\n",
    "#     #plt.colorbar(u);\n",
    "# fig, axx = plt.subplots(2,2, figsize=(12,8), sharex=True, sharey=True);\n",
    "# plt.subplots_adjust(hspace=0.2, wspace=0.15)\n",
    "# for ax, week in zip(axx.flatten(), range(678,682)):\n",
    "#     plot_data(ax, week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_scz(week, ax=None):\n",
    "#     sc = wd[week-678].sc_data\n",
    "#     ra=sc.ra_scz.values\n",
    "#     ra[ra>180]-=360\n",
    "#     dec=sc.dec_scz.values\n",
    "#     fig, ax = plt.subplots() if ax is None else (ax.figure, ax)\n",
    "#     ax.plot(ra, dec, '.');\n",
    "#     ax.plot(-94, -29, '+r', ms=20)\n",
    "#     ax.set(xlim=(180,-180), ylim=(-90,90), xlabel='RA', ylabel='Dec', title=f'week {week}')\n",
    "# fig, axx = plt.subplots(2,2, figsize=(12,8), sharex=True, sharey=True)\n",
    "# fig.suptitle('Fermi pointing')\n",
    "# plt.subplots_adjust(hspace=0.2, wspace=0.15)\n",
    "# for ax, week in zip(axx.flatten(), range(678,682)):\n",
    "#     plot_scz(week, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_sources.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_load_data.ipynb.\n",
      "Converted 04_select_data.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sat Jun 26 11:58:54 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
