{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data_man\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data management\n",
    "> Create, from FT1 and FT2, a compact data set with photon and livetime info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Fermi-LAT weekly data files are extracted from the [GSFC FTP server ](https://heasarc.gsfc.nasa.gov/FTP/fermi/data/lat/weekly), \n",
    "with subfolders for the photon data, `photon` and spacecraft data, `spacecraft`. It is [described here](http://fermi.gsfc.nasa.gov/ssc/data/access/http://fermi.gsfc.nasa.gov/ssc/data/access/)\n",
    "\n",
    "The class `FermiData` downloads these to temporary files and constructs a dict for each week with\n",
    "contents\n",
    "\n",
    "* photons: a table, entry per selected photon with columns, converted with `get_ft1_data`\n",
    "\n",
    "  * run number (uint32, stored as a category by pandas)\n",
    "  * time since the run start, in 2 $\\mu$s intervals  (uint32)\n",
    "  * energy and event type (uint8)\n",
    "  * position as HEALPix index (uint32)\n",
    "  \n",
    "* sc_data: a table, an entry per 30-s interval, with columns, all float32, converted with `get_ft2_info`\n",
    "  * start/stop time \n",
    "  * S/C direction \n",
    "  * zenith direction\n",
    "* gti_times: an array of interleaved start/stop intervals\n",
    "* file_date: modification date for the FT1 file at GSFC.\n",
    "\n",
    "These dict objects, one per week, are saved in a folder\n",
    "\n",
    "#### A note about timing\n",
    "A run is typically an orbit or less, at most 6 ks. Integerizing the offset from this using 32 bits, one has 5e5 intervals/s, so \n",
    "we choose 2$\\mu$s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import dateutil, datetime\n",
    "from astropy.io import fits\n",
    "from ftplib import FTP_TLS as FTP\n",
    "\n",
    "import healpy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from wtlike.config import Config, Timer, UTC, MJD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft1_data( config, ft1_file):\n",
    "\n",
    "        \"\"\"\n",
    "        Read in a photon data (FT1) file, bin in energy and position to convert to a compact DataFrame\n",
    "\n",
    "        - `ft1_file` -- A monthly file generated by J. Ballet, or a weekly file from GSFC\n",
    "\n",
    "        Depends on config items\n",
    "        - `theta_cut, z_cut` -- selection criteria\n",
    "        - `ebins, etypes` -- define band index\n",
    "        - `nside, nest` -- define HEALPix binning\n",
    "\n",
    "        Returns a tuple with\n",
    "\n",
    "        - `tstart`, the start MET time\n",
    "\n",
    "        - DataFrame  with columns\n",
    "           - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
    "           - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
    "           - `run_id` (uint32) The run number, stored as a categorical uint32 array\n",
    "           - `trun` (unit32): time since run the id in 2 $\\mu s$ units\n",
    "\n",
    "        - gti times as an interleaved start, stop array.\n",
    "\n",
    "        For the selected events above 100 MeV, this represents 9 bytes per photon, vs. 27.\n",
    "        \"\"\"\n",
    "\n",
    "        delta_t = config.offset_size\n",
    "        ebins = config.energy_edges\n",
    "        etypes = config.etypes\n",
    "        nside = config.nside\n",
    "        nest = config.nest\n",
    "        z_cut =config.z_max\n",
    "        theta_cut = np.degrees(np.arccos(config.cos_theta_max))\n",
    "        verbose = config.verbose\n",
    "\n",
    "        with  fits.open(ft1_file) as ft1:\n",
    "            tstart = ft1[1].header['TSTART']\n",
    "\n",
    "            ## GTI - setup raveled array function to make cut\n",
    "            gti_data= ft1['GTI'].data\n",
    "            # extract arrays for values of interest\n",
    "            data =ft1['EVENTS'].data\n",
    "\n",
    "        a,b = sorted(gti_data.START), sorted(gti_data.STOP)\n",
    "\n",
    "        gti_times = np.ravel(np.column_stack((a,b)))\n",
    "        if np.any(np.diff(gti_times)<0):\n",
    "            raise Exception(f'Non-monatonic GTI found in file {ft1_file}')\n",
    "\n",
    "        # def apply_gti(time):\n",
    "        #     x = np.digitize(time, gti_times)\n",
    "        #     return np.bitwise_and(x,1).astype(bool)\n",
    "\n",
    "        # apply  selections\n",
    "\n",
    "        sel =  ((data['ENERGY'] > ebins[0]) &\n",
    "                (data['ZENITH_ANGLE'] < z_cut) &\n",
    "                (data['THETA'] < theta_cut))\n",
    "\n",
    "        dsel = data[sel]\n",
    "\n",
    "        # get the columns for output\n",
    "        glon, glat, energy, et, z, theta, time, ec =\\\n",
    "             [dsel[x] for x in 'L B ENERGY EVENT_TYPE ZENITH_ANGLE THETA TIME EVENT_CLASS'.split()]\n",
    "\n",
    "        # generate event_type masks\n",
    "        et_mask={}\n",
    "        for ie in etypes:\n",
    "            et_mask[ie]= et[:,-1-ie]\n",
    "\n",
    "\n",
    "        if verbose>1:\n",
    "            total = sum(b)-sum(a)\n",
    "            fraction = total/(b[-1]-a[0])\n",
    "\n",
    "            print(  f'FT1: {ft1_file.name}, GTI range {a[0]:.1f}-{b[-1]:.1f}:  {len(data):,} photons'\\\n",
    "                    f'\\n\\tSelection E > {ebins[0]:.0f} MeV. theta<{theta_cut:.1f} and z<{z_cut} remove:'\\\n",
    "                    f' {100.- 100*len(dsel)/float(len(data)):.2f}%'\n",
    "#                     f', GTI cut removes {sum(~gti_cut)}'\n",
    "                 )\n",
    "\n",
    "\n",
    "        # event class -- turn into single int for later mask\n",
    "#         bits = np.array([1<<n for n in range(20)])\n",
    "#         def to_bin(x):\n",
    "#             return np.sum(bits[x[:20]])\n",
    "#         ec = [to_bin(row[20]) for row in ec\n",
    "\n",
    "        # pixelate direction\n",
    "        hpindex = healpy.ang2pix(nside, glon, glat, nest=nest, lonlat=True).astype(np.uint32)\n",
    "        hpname = 'nest_index' if nest else 'ring_index'\n",
    "\n",
    "        # digitize energy and create band index incluing (front/back)\n",
    "        band_index = (2*(np.digitize(energy, ebins, )-1) + et_mask[1]).astype(np.uint8)\n",
    "\n",
    "        #\n",
    "        run_id = dsel['RUN_ID'].astype(np.uint32)\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "                {   'band'  : band_index,\n",
    "                    hpname  : hpindex,\n",
    "                    #'time'  : (time-tstart).astype(np.float32), # the old time\n",
    "                    'run_id': pd.Categorical(run_id),\n",
    "                    'trun'  : ((time-run_id)/delta_t).astype(np.uint32),\n",
    "                }  )\n",
    "        if verbose>1:\n",
    "            print(f'\\tReturning tstart={tstart:.0f}, {len(dsel):,} photons.')\n",
    "\n",
    "        return  tstart, df, gti_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft1_data\" class=\"doc_header\"><code>get_ft1_data</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft1_data</code>(**`config`**, **`ft1_file`**)\n",
       "\n",
       "Read in a photon data (FT1) file, bin in energy and position to convert to a compact DataFrame\n",
       "\n",
       "- `ft1_file` -- A monthly file generated by J. Ballet, or a weekly file from GSFC\n",
       "\n",
       "Depends on config items\n",
       "- `theta_cut, z_cut` -- selection criteria\n",
       "- `ebins, etypes` -- define band index\n",
       "- `nside, nest` -- define HEALPix binning\n",
       "\n",
       "Returns a tuple with\n",
       "\n",
       "- `tstart`, the start MET time\n",
       "\n",
       "- DataFrame  with columns\n",
       "   - `band` (uint8):    energy band index*2 + 0,1 for Front/Back\n",
       "   - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside\n",
       "   - `run_id` (uint32) The run number, stored as a categorical uint32 array\n",
       "   - `trun` (unit32): time since run the id in 2 $\\mu s$ units\n",
       "\n",
       "- gti times as an interleaved start, stop array.\n",
       "\n",
       "For the selected events above 100 MeV, this represents 9 bytes per photon, vs. 27."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft1_data, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ft2_info(config, filename,\n",
    "                 gti=lambda t: True):\n",
    "    \"\"\"Process a FT2 file, with S/C history data, and return a summary DataFrame\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    * config -- verbose, cos_theta_max, z_max\n",
    "    * filename -- spacecraft (FT2) file\n",
    "    * gti -- GTI object that checkes for allowed intervals, in MJD units\n",
    "\n",
    "    Returns: A DataFrame with fields consistent with GTI if specified\n",
    "\n",
    "    * start, stop -- interval in MJD units\n",
    "    * livetime -- sec\n",
    "    * ra_scz, dec_scz --spaceraft direction\n",
    "    * ra_zenith, dec_zenith -- local zenith\n",
    "    \"\"\"\n",
    "    # combine the files into a DataFrame with following fields besides START and STOP (lower case for column)\n",
    "    fields    = ['LIVETIME','RA_SCZ','DEC_SCZ', 'RA_ZENITH','DEC_ZENITH']\n",
    "    with fits.open(filename) as hdu:\n",
    "        scdata = hdu['SC_DATA'].data\n",
    "        tstart, tstop = [float(hdu[0].header[field]) for field in  ('TSTART','TSTOP') ]\n",
    "\n",
    "    if config.verbose>1:\n",
    "        print(f'FT2: {filename.name}, MET range {tstart:.1f}-{tstop:.1f},', end='')# {\"not\" if gti is None else \"\"} applying GTI')\n",
    "\n",
    "    # get times to check against MJD limits and GTI\n",
    "    start, stop = [MJD(np.array(scdata.START, float)),\n",
    "                   MJD(np.array(scdata.STOP, float))]\n",
    "\n",
    "    # apply GTI to bin center (avoid edge effects?)\n",
    "    in_gti = gti(0.5*(start+stop))\n",
    "    if config.verbose>1:\n",
    "        s = sum(in_gti)\n",
    "        print(f' {len(start)} entries, {s} ({100*s/len(start):.1f}%) in GTI')\n",
    "\n",
    "    t = [('start', start[in_gti]), ('stop',stop[in_gti])]+\\\n",
    "        [(field.lower(), np.array(scdata[field][in_gti],np.float32)) for field in fields ]\n",
    "\n",
    "    sc_data = pd.DataFrame(dict(t) )\n",
    "\n",
    "    return sc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"get_ft2_info\" class=\"doc_header\"><code>get_ft2_info</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>get_ft2_info</code>(**`config`**, **`filename`**, **`gti`**=*`<lambda>`*)\n",
       "\n",
       "Process a FT2 file, with S/C history data, and return a summary DataFrame\n",
       "\n",
       "Parameters:\n",
       "\n",
       "* config -- verbose, cos_theta_max, z_max\n",
       "* filename -- spacecraft (FT2) file\n",
       "* gti -- GTI object that checkes for allowed intervals, in MJD units\n",
       "\n",
       "Returns: A DataFrame with fields consistent with GTI if specified\n",
       "\n",
       "* start, stop -- interval in MJD units\n",
       "* livetime -- sec\n",
       "* ra_scz, dec_scz --spaceraft direction\n",
       "* ra_zenith, dec_zenith -- local zenith"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_ft2_info, title_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def filepaths(week):\n",
    "    \"\"\"Returns: A tuple with two elements for the week number, each with two triplets with:\n",
    "        ftp folder, ftp filename, local simple filename\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for ftype, alias in  [('spacecraft','ft2'), ('photon','ft1')]:\n",
    "         urls.append((\n",
    "             f'{ftype}',\n",
    "             f'lat_{ftype}_weekly_w{week:03d}_p{\"305\" if ftype==\"photon\" else \"310\" }_v001.fits',\n",
    "             f'week{week:03d}_{alias}.fits',\n",
    "            ))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FermiData(dict):\n",
    "    \"\"\" Manage the full data set in weekly chunks\n",
    "    * Checking the current set of files at GSFC\n",
    "    * downloading a week at a time to a local tmp\n",
    "    * Converting to condensed format and saving to pickled dicts in wtlike_data\n",
    "    \"\"\"\n",
    "\n",
    "    ftp_site = 'heasarc.gsfc.nasa.gov'\n",
    "    ftp_path = 'fermi/data/lat/weekly'\n",
    "    local_path  = '/tmp/from_gsfc'\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\" Obtain list of the weekly FT1 and FT2 files at GSFC, Set up as a dict, with\n",
    "        keys= week numbers, values=mofification date strings\n",
    "        \"\"\"\n",
    "        self.config = config or Config()\n",
    "        self.wtlike_data_file_path = Path(self.config.datapath/'data_files')\n",
    "        assert self.wtlike_data_file_path.is_dir(), 'Data path invalid'\n",
    "        os.makedirs(self.local_path, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with FTP(self.ftp_site) as ftp:\n",
    "                ftp.login()\n",
    "                ftp.prot_p()\n",
    "                ftp.cwd(self.ftp_path+'/photon') # or spacecraft\n",
    "                # aet modification time and type for all files in folder\n",
    "\n",
    "                parse_week = lambda fn:  int(fn.split('_')[3][1:])\n",
    "                flist = ftp.mlsd(facts=['modify', 'type'])\n",
    "                self.fileinfo = sorted([(parse_week(name), fact['modify']) for name,fact in flist\n",
    "                                 if fact['type']=='file' and name.startswith('lat') ])\n",
    "        except Exception as msg:\n",
    "            raise Exception(f'FTP login to or download from {self.ftp_site} failed: {msg}')\n",
    "        self.update(self.fileinfo)\n",
    "\n",
    "    @property\n",
    "    def local_filedate(self):\n",
    "        \"\"\" the datetime object representing the last file date in local storage\"\"\"\n",
    "        from dateutil.parser import parse\n",
    "        weekly_folder = self.config.datapath/'data_files'\n",
    "        ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "        if len(ff)==0:\n",
    "            print(f'No .pkl files found in {weekly_folder}', file=sys.stderr)\n",
    "            return None\n",
    "\n",
    "        wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "        lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "        return dateutil.parser.parse(lastweek['file_date'])\n",
    "\n",
    "    @property\n",
    "    def gsfc_filedate(self):\n",
    "        return dateutil.parser.parse(list(self.values())[-1])\n",
    "\n",
    "    def download(self, week):\n",
    "        \"\"\" Download the given week to the tmp folder\n",
    "        \"\"\"\n",
    "        assert week in self, f'week {week} not found at FTP site'\n",
    "        with FTP(self.ftp_site) as ftp:\n",
    "            ftp.login()\n",
    "            ftp.prot_p()\n",
    "            for ftp_folder, ftp_filename, local_filename in filepaths(week):\n",
    "                ftp.cwd('/'+self.ftp_path+'/'+ftp_folder)\n",
    "                if self.config.verbose>0:\n",
    "                    print(f'FermiData: {ftp_folder}/{ftp_filename} --> {local_filename}')\n",
    "                with open(f'{self.local_path}/{local_filename}', 'wb') as  localfile:\n",
    "                    ftp.retrbinary('RETR ' + ftp_filename, localfile.write)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'FermiData: {len(self.fileinfo)} week files at GSFC, from {self.fileinfo[0]} to {self.fileinfo[-1]}'\n",
    "\n",
    "    def in_temp(self):\n",
    "        \"\"\"return list of GSFC copied files in the local_path folder\"\"\"\n",
    "        names = [f.name for f in Path(self.local_path).glob('*')]\n",
    "        return names\n",
    "\n",
    "    def __call__(self, week, test=False, tries_left=2):\n",
    "        \"\"\" Process the given week:\n",
    "        * download from GSFC\n",
    "        * convert each\n",
    "        * save pickled dict summary\n",
    "\n",
    "        \"\"\"\n",
    "        assert week in self, f'week {week} not found at FTP site'\n",
    "        ff = filepaths(week)\n",
    "        ft1_file = Path(self.local_path)/ff[1][2]\n",
    "        ft2_file = Path(self.local_path)/ff[0][2]\n",
    "\n",
    "        if self.config.verbose>1:\n",
    "            print(f'FermiData: converting week {week}')\n",
    "\n",
    "        while tries_left>0:\n",
    "            try:\n",
    "                if not (ft1_file.exists() and ft2_file.exists()):\n",
    "                    self.download(week)\n",
    "                tstart, photon_data, gti_times  = get_ft1_data(self.config, ft1_file)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'*** ERROR *** Failed to convert {ft1_file}: {e} download it again)')\n",
    "                tries_left -=1\n",
    "                if tries_left==0:\n",
    "                    print(f'Failed to convert week file {ft1_file}: quitting', file=sys.stderr)\n",
    "                    return None\n",
    "                else:\n",
    "                    os.unlink(ft1_file)\n",
    "\n",
    "        def apply_gti(time): # note MJD\n",
    "            x = np.digitize(time, MJD(gti_times))\n",
    "            return np.bitwise_and(x,1).astype(bool)\n",
    "        sc_data = get_ft2_info(self.config, ft2_file, apply_gti)\n",
    "\n",
    "        # finished with copies of FT1 and FT2 files: delete them\n",
    "        for file in (ft1_file,ft2_file):\n",
    "            os.unlink(file)\n",
    "\n",
    "        # package info into a dict for pickle\n",
    "        d = dict(tstart = tstart,\n",
    "                photons = photon_data,\n",
    "                sc_data = sc_data,\n",
    "                gti_times = gti_times,\n",
    "                file_date = self[week])\n",
    "        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'\n",
    "        if filename.exists():\n",
    "            print(f'FermiData: replacing existing {filename}')\n",
    "        if not test:\n",
    "            with open(filename, 'wb') as out:\n",
    "                pickle.dump(d, out)\n",
    "\n",
    "        if self.config.verbose>0:\n",
    "            print(f'FermiData: Saved to {filename}')\n",
    "            if self.config.verbose>1:\n",
    "                print(photon_data.info())\n",
    "\n",
    "    def load_week(self, week):\n",
    "        \"\"\"Load a pickled week summary \"\"\"\n",
    "        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'\n",
    "        assert filename.exists(), f'File {filename} does not exist'\n",
    "        with open(filename, 'rb') as imp:\n",
    "            ret = pickle.load(imp)\n",
    "        return ret\n",
    "\n",
    "    def check_week(self, week):\n",
    "        \"\"\"Returns True if the local week needs updating\"\"\"\n",
    "        data = self.load_week(week)\n",
    "        if 'file_date' not in data:\n",
    "            return True\n",
    "        return data['file_date'] == self[week]\n",
    "\n",
    "    def needs_update(self, threshold=0):\n",
    "        \"\"\" Compare files on disk with the GSFC list and compile list that need to be downloaded\n",
    "\n",
    "        Check the file date of the last one on disk and include it if:\n",
    "            * it short and there is one or more GSFC weeks following it,\n",
    "            * It is the most recent week and is short by more than *threshold* days\n",
    "        \"\"\"\n",
    "        gg =self.wtlike_data_file_path.glob('*.pkl')\n",
    "\n",
    "        file_weeks= map(lambda n: int(n.name[5:8]), gg)\n",
    "        ondisk = np.array(list(file_weeks))\n",
    "\n",
    "        missing =  list(set(self.keys()).difference(set(ondisk)))\n",
    "\n",
    "        last = ondisk[-1]\n",
    "        if last not in missing and not self.check_week( last):\n",
    "            delta = (self.gsfc_filedate -self.local_filedate).seconds/24/3600\n",
    "            if delta> threshold:\n",
    "                missing.append(last)\n",
    "        return missing\n",
    "\n",
    "    def process(self, days=1):\n",
    "        \"\"\" Download and process all weeks missing or needing an update, if within `days`\n",
    "        from the present\n",
    "\n",
    "        Return status: True if  anything changed\n",
    "        \"\"\"\n",
    "        # will use multprocessing if len(todo)>1 and pool_size>1\n",
    "        todo = self.needs_update(days)\n",
    "        if len(todo)==0: return False\n",
    "        if self.config.pool_size >1 and len(todo)>1:\n",
    "            print('multitasking not applied yet', file=sys.stderr)\n",
    "            pass\n",
    "        list(map(self, todo))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FermiData methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FermiData.download\" class=\"doc_header\"><code>FermiData.download</code><a href=\"__main__.py#L55\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FermiData.download</code>(**`week`**)\n",
       "\n",
       "Download the given week to the tmp folder\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FermiData.download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FermiData.needs_update\" class=\"doc_header\"><code>FermiData.needs_update</code><a href=\"__main__.py#L149\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FermiData.needs_update</code>(**`threshold`**=*`0`*)\n",
       "\n",
       "Compare files on disk with the GSFC list and compile list that need to be downloaded\n",
       "\n",
       "Check the file date of the last one on disk and include it if:\n",
       "    * it short and there is one or more GSFC weeks following it,\n",
       "    * It is the most recent week and is short by more than *threshold* days"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FermiData.needs_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FermiData: 711 week files at GSFC, from (9, '20220228224421') to (720, '20220317161457') [720, 717, 718, 719, 716]\n"
     ]
    }
   ],
   "source": [
    "self = FermiData()\n",
    "print(self, self.needs_update())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FermiData.process\" class=\"doc_header\"><code>FermiData.process</code><a href=\"__main__.py#L170\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FermiData.process</code>(**`days`**=*`1`*)\n",
       "\n",
       "Download and process all weeks missing or needing an update, if within `days`\n",
       "from the present\n",
       "\n",
       "Return status: True if  anything changed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FermiData.process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"FermiData.check_week\" class=\"doc_header\"><code>FermiData.check_week</code><a href=\"__main__.py#L142\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>FermiData.check_week</code>(**`week`**)\n",
       "\n",
       "Returns True if the local week needs updating"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(FermiData.check_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FermiData: 711 week files at GSFC, from (9, '20220228224421') to (720, '20220317161457') \n",
      "\tweek(s) needing update: [720, 717, 718, 719, 716]\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "if config.valid:\n",
    "    self = FermiData(Config(verbose=1))\n",
    "    check =self.needs_update(0.5)\n",
    "    print(self, f'\\n\\tweek(s) needing update: {check}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#     with Timer() as t:\n",
    "#         self.process(0.5)\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check the weekly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_data(config=None):\n",
    "    \"\"\"\n",
    "    Return: sorted list of files, last week number, number of days in last week\"\"\"\n",
    "    if config is None: config=Config()\n",
    "    if config.valid:\n",
    "        weekly_folder = config.datapath/'data_files'\n",
    "        ff = sorted(list(weekly_folder.glob('*.pkl')))\n",
    "        if len(ff)==0:\n",
    "            print(f'No .pkl files found in {weekly_folder}', file=sys.stderr)\n",
    "            return\n",
    "        wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))\n",
    "        lastweek = pickle.load(open(ff[-1],'rb'))\n",
    "        file_date = lastweek['file_date']\n",
    "        gti = lastweek['gti_times'];\n",
    "        days = (gti[-1]-gti[0])/(24*3600)\n",
    "        if config.verbose>0:\n",
    "            print(f'Weekly folder \"{weekly_folder}\" contains {len(wk)} weeks.'\\\n",
    "                  f'\\n\\t Last week, # {wk[-1]}, has {days:.3f} days, ends at UTC {UTC(MJD(gti[-1]))}, filedate {file_date}' )\n",
    "        return ff, wk[-1], days\n",
    "    else:\n",
    "        print(f'Config not valid, {config.errors}', file=sys.stderr)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ret = check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def update_data(update_threshold=1, config=None):\n",
    "    \"\"\"Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.\n",
    "    If the last one is the current week, check to see if needs updating, by comparing file date, in days,\n",
    "    from the last update with the current one at GSFC.\n",
    "    \"\"\"\n",
    "    ff = FermiData(config)\n",
    "    return ff.process(update_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_week_files(config, week_range=None):\n",
    "    \"\"\"Return list of week files\n",
    "\n",
    "    - week_range [None] -- tuple with inclusive range. If None, get all\n",
    "    \"\"\"\n",
    "    data_folder = config.datapath/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    weeks = week_range or  config.week_range\n",
    "    if week_range is not None:\n",
    "\n",
    "        slc = slice(*week_range) if type(week_range)==tuple else slice(week_range,week_range)\n",
    "        wk_table = pd.Series(data=[df for df in data_files],\n",
    "                     index= [ int(df.name[-7:-4]) for df in  data_files],\n",
    "                    )\n",
    "        data_files = wk_table.loc[slc].values\n",
    "\n",
    "        if config.verbose>0:\n",
    "            q = lambda x: x if x is not None else \"\"\n",
    "            print(f'LoadData: Loading weeks[{q(slc.start)}:{q(slc.stop)}:{q(slc.step)}]', end='' if config.verbose<2 else '\\n')\n",
    "    else:\n",
    "        if config.verbose>0: print(f'LoadData: loading all {len(data_files)} weekly files')\n",
    "\n",
    "    if len(data_files)==0:\n",
    "        msg =  f'Specified week_range {week_range} produced no output. Note that week numbers are 9-'\n",
    "        raise Exception(msg)\n",
    "\n",
    "    return data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 03_sources.ipynb.\n",
      "Converted 04_load_data.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 10-time_series.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_presentation.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Thu Mar 17 09:21:00 PDT 2022\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
