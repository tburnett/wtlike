{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp source_data\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-bridal",
   "metadata": {},
   "source": [
    "# Source Data management\n",
    "> Extract data for a source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-nelson",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Given a point source, the class `SourceData` manages all data-oriented operations, providing all that is necessary to create a set of cells. It depends on the modules\n",
    "\n",
    "* `config` \n",
    "    This must set up the paths to the data created by `data_man`, and define paths for the effective area and weight files\n",
    "\n",
    "* effective_area\n",
    "* weights\n",
    "* exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try: # make conda-build happy?\n",
    "    import healpy\n",
    "except:\n",
    "    pass\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from wtlike.config import *\n",
    "from wtlike.data_man import *\n",
    "from wtlike.effective_area import *\n",
    "from wtlike.weights import *\n",
    "from wtlike.exposure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-quantum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters \n",
      "  verbose         : 2\n",
      "  wtlike_data     : /home/burnett/wtlike_data\n",
      "  cachepath       : /home/burnett/wtlike_cache\n",
      "  radius          : 4\n",
      "  cos_theta_max   : 0.4\n",
      "  z_max           : 100\n",
      "  week_range      : (None, None)\n",
      "  time_bins       : (0, 0, 7)\n",
      "  use_uint8       : False\n",
      "  nside           : 1024\n",
      "  nest            : True\n",
      "  bins_per_decade : 4\n",
      "  base_spectrum   : lambda E: (E/1000)**-2.1\n",
      "  energy_range    : (100.0, 1000000.0)\n",
      "  likelihood_rep  : poisson\n",
      "  errors          : []\n",
      "\n",
      "Weekly folder \"/home/burnett/wtlike_data/data_files\" contains 666 weeks,  9 to 675, end at 2021-05-13\n",
      "Source \"BL Lac\" at: (l,b)=(92.590,-10.441)\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "# check the weekly files\n",
    "config = Config(wtlike_data='~/wtlike_data', verbose=2)\n",
    "if config.valid:\n",
    "    ff = get_data_files(config)\n",
    "    source = PointSource('BL Lac') #Geminga')\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _exposure(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)\n",
    "\n",
    "def _calculate_exposure_for_source(config, source, week):\n",
    "    \"\"\"\n",
    "    Calcualate the exposure for the source during the given week\n",
    "    \"\"\"\n",
    "    df = week['sc_data']\n",
    "    \n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "\n",
    "    return  pd.DataFrame( \n",
    "        dict(\n",
    "            start=df.start[mask], \n",
    "            stop=df.stop[mask], \n",
    "            exp=_exposure(config, livetime, pcosines[mask]),\n",
    "            cos_theta=pcosines[mask],\n",
    "        ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-reply",
   "metadata": {},
   "source": [
    "### Check exposure with last data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFound 17,013 S/C entries:  4,958 remain after zenith and theta cuts\n",
      "Calculate exposure using the energy domain 100.0-1000000.0 4 bins/decade\n",
      "      start      stop        exp  cos_theta\n",
      "0  59340.03  59340.03  117444.77       0.78\n",
      "1  59340.03  59340.03  116524.09       0.78\n",
      "2  59340.03  59340.03  115348.05       0.77\n",
      "3  59340.03  59340.03  114144.62       0.77\n",
      "4  59340.03  59340.03  112747.56       0.76\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "if config.valid:\n",
    "    filename=ff[-1]\n",
    "    week = pickle.load( open(filename,'rb') )\n",
    "    e_df = _calculate_exposure_for_source(config, source, week); \n",
    "    print(e_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _get_photons_near_source(config, source, week): #tzero, photon_df):\n",
    "    \"\"\"\n",
    "    Select the photons near a source\n",
    "    \n",
    "    - source : a PointSource object\n",
    "    - week : dict with \n",
    "        - tzero : start time for the photon\n",
    "        - photon_df : DataFrame with photon data\n",
    "    \n",
    "    Returns a DF with \n",
    "    - `band` index, \n",
    "    - `time` in MJD (added tstart and converted from MET)\n",
    "    - `pixel` index, nest indexing \n",
    "    - `radius` distance in deg from source direction\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cone(config, source, nest=True):\n",
    "        # cone geometry stuff: get corresponding pixels and center vector\n",
    "        l,b,radius = source.l, source.b, config.radius\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "        conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "        center = healpy.dir2vec(l,b, lonlat=True)\n",
    "        return center, conepix\n",
    "    \n",
    "    center, conepix = _cone(config,source)\n",
    "\n",
    "    df = week['photons']\n",
    "    tstart = week['tstart']\n",
    "    allpix = df.nest_index.values\n",
    "\n",
    "    # select by comparing high-order pixels (faster)\n",
    "    shift=11\n",
    "    a = np.right_shift(allpix, shift)\n",
    "    c = np.unique(np.right_shift(conepix, shift))\n",
    "    incone = np.isin(a,c)\n",
    "    \n",
    "    if sum(incone)<2:\n",
    "        if config.verbose>1:\n",
    "            print(f'\\nWeek at {UTC(MJD(tstart))} has 0 or 1 photons')\n",
    "        return\n",
    "    \n",
    "    if config.verbose>2:\n",
    "        a, b = sum(incone), len(allpix)\n",
    "        print(f'Select photons for source {source.name}:\\n\\tPixel cone cut: select {a} from {b} ({100*a/b:.1f}%)')\n",
    "\n",
    "    # cut df to entries in the cone\n",
    "    dfc = df[incone]\n",
    "\n",
    "    # distance from center for all accepted photons\n",
    "    ll,bb = healpy.pix2ang(config.nside, dfc.nest_index,  nest=True, lonlat=True)\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32))\n",
    "    in_cone = t2<config.radius\n",
    "\n",
    "    if config.verbose>2:\n",
    "        print(f'\\tGeometric cone cut: select {sum(in_cone)}')\n",
    "    # assume all in the GTI (should check)\n",
    "\n",
    "    # times: convert to float, add tstart, convert to MJD\n",
    "    time = MJD(np.array(dfc.time, float)+tstart)\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "        [np.array(dfc.band), time, dfc.nest_index, np.atleast_1d(t2)],\n",
    "        names='band time pixel radius'.split()))[in_cone]\n",
    "\n",
    "    # make sure times are monotonic by sorting (needed for most weeks after March 2018)\n",
    "    out_df = out_df.sort_values(by='time')\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-tobago",
   "metadata": {},
   "source": [
    "### test photon data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  hide\n",
    "# # if config.valid:\n",
    "# for i,f in enumerate(ff[300:]):\n",
    "#     week = pickle.load(open(f,'rb')); \n",
    "#     tstart = week['tstart']\n",
    "#     print(f'{i} ', end='')\n",
    "#     #print(f'TSTART: MET {tstart}, UTC {UTC(MJD(tstart))}')\n",
    "#     p_df = _get_photons_near_source(config, source, week )\n",
    "# #     print(len(p_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export  \n",
    "\n",
    "#     gstart, gstop = keep[0::2], keep[1::2]\n",
    "#     df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "    \n",
    "#     # add column with duration in sec\n",
    "#     df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "#     return df.query(f'duration>{min_duration}')\n",
    "\n",
    "def time_bin_edges(config, exposure, tbin=None):\n",
    "    \"\"\"Return an interleaved array of start/stop values\n",
    "    \n",
    "    tbin: an array (a,b,d), default config.time_bins\n",
    "    \n",
    "    interpretation of a, b:\n",
    "\n",
    "        if > 50000, interpret as MJD\n",
    "        if <0, back from stop\n",
    "        otherwise, offset from start\n",
    "        \n",
    "    d : if positive, the day bin size\n",
    "        if 0; return contiguous bins\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # nominal total range, MJD edges\n",
    "    start = np.round(exposure.start.values[0])\n",
    "    stop =  np.round(exposure.stop.values[-1])\n",
    "\n",
    "    a, b, step = tbin if tbin is not None else config.time_bins\n",
    "    \n",
    "\n",
    "    if a>50000: start=a\n",
    "    elif a<0: start = stop+a\n",
    "    else : start += a\n",
    "\n",
    "\n",
    "    if b>50000: stop=b\n",
    "    elif b>0: stop = start+b\n",
    "    else: stop += b\n",
    "    \n",
    "    if step<=0:\n",
    "        return contiguous_bins(exposure.query(f'{start}<start<{stop}'),)\n",
    "\n",
    "    # adjust stop\n",
    "    nbins = int((stop-start)/step)\n",
    "    assert nbins>1, 'Bad binning: no bins'\n",
    "    stop = start+(nbins)*step\n",
    "    u =  np.linspace(start,stop, nbins+1 )\n",
    "    \n",
    "    # make an interleaved start/stop array\n",
    "    v = np.empty(2*nbins, float)\n",
    "    v[0::2] = u[:-1]\n",
    "    v[1::2] = u[1:]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def contiguous_bins(exposure, min_gap=20, min_duration=600):\n",
    "    \n",
    "    \"\"\" return a dataframe with start and stop columns that \n",
    "    denote contiguous intervals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    stop = exposure.stop.values\n",
    "    start = exposure.start.values\n",
    "\n",
    "    # interleave  the starts and stops\n",
    "    ssint = np.empty(2*len(start))\n",
    "    ssint[0::2] = start\n",
    "    ssint[1::2] = stop\n",
    "\n",
    "    # Tag the (stpp,start) pairs < 10 sec as  not adjacent\n",
    "    not_adjacent = np.diff(ssint)[1::2] > min_gap/(24*3600) ; \n",
    "    #print(f'{sum(not_adjacent)} (start,stop) pairs are not closer than {min_gap} s')\n",
    "\n",
    "    # make a mask, keep ends\n",
    "    mask = np.empty(2*len(start), bool)\n",
    "    mask[0] = mask[-1] = True\n",
    "    # \n",
    "\n",
    "    # insert into mask -- keep only the (stop,start) pairs  which are not adjacent\n",
    "    mask[1:-2:2] = not_adjacent\n",
    "    mask[2:-1:2] = not_adjacent\n",
    "    \n",
    "    # apply mask, split into start and stop\n",
    "    keep = ssint[mask]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-marks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59340. 59341. 59341. 59342. 59342. 59343. 59343. 59344. 59344. 59345.\n",
      " 59345. 59346. 59346. 59347.]\n",
      "[59346.01487111 59346.02841185 59346.09395583 59346.11860861\n",
      " 59346.15631779 59346.16048361 59346.22603917 59346.25069194\n",
      " 59346.36363255 59346.38272898 59346.40877065 59346.41633413\n",
      " 59346.50116727 59346.51505537 59346.54109704 59346.55631099\n",
      " 59346.62261324 59346.64720815 59346.67324981 59346.68956926\n",
      " 59346.75489333 59346.77959241 59346.80563407 59346.82195352\n",
      " 59346.88727759 59346.91162944 59346.93801833 59346.95399056]\n"
     ]
    }
   ],
   "source": [
    "exposure = e_df\n",
    "print(time_bin_edges(config, exposure, (0,0,1)) )\n",
    "print(time_bin_edges(config, exposure, (-1,0,0)))\n",
    "# print(time_bin_edges(config, exposure, (0,0,0.5)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,0,2)) )\n",
    "# print(time_bin_edges(config, exposure, (0,1,0.25)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,-4,0.25)) )   \n",
    "# print(time_bin_edges(config, exposure, (59326, 59330,0.5)) )    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep = contiguous_bins(exposure)\n",
    "# gstart, gstop = keep[0::2], keep[1::2]\n",
    "# df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "\n",
    "# # add column with duration in sec\n",
    "# df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "\n",
    "# df.hist('duration', bins=np.linspace(0,4e3,131));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def binned_exposure(config, exposure, time_edges):\n",
    "    \"\"\"Bin the exposure\n",
    "\n",
    "    - time_bins: list of edges, as an interleaved start/stop array\n",
    "       \n",
    "        \n",
    "    returns  array of exposure integrated over each time bin, times 1e-9\n",
    "    it is interleaved, client must apply [0::2] selection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get exposure calculation\n",
    "    exp   =exposure.exp.values\n",
    "    estart= exposure.start.values\n",
    "    estop = exposure.stop.values\n",
    "\n",
    "    # determine bins,\n",
    "\n",
    "    #use cumulative exposure to integrate over larger periods\n",
    "    cumexp = np.concatenate(([0],np.cumsum(exp)) )\n",
    "\n",
    "    # get index into tstop array of the bin edges\n",
    "    edge_index = np.searchsorted(estop, time_edges)\n",
    "    \n",
    "    # return the exposure integrated over the intervals\n",
    "    cum = cumexp[edge_index]\n",
    "   \n",
    "    # difference is exposure per interval: normalize it here\n",
    "    bexp = np.diff(cum) \n",
    "#     if config.verbose>1:\n",
    "#         print(f'Relative exposure per bin:\\n{pd.Series(bexp).describe(percentiles=[])}')\n",
    "    return bexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _load_from_weekly_data(config, source, week_range=None):\n",
    "    \"\"\"\n",
    "    Generate combinded DataFrames from a list of pickled files\n",
    "    Either weekly or monthly\n",
    "    \n",
    "    kwargs:\n",
    "    - week_range\n",
    "    \"\"\"\n",
    "    \n",
    "    # check weights\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    assert weight_file is not None\n",
    "    \n",
    "    data_folder = config.wtlike_data/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    iname = data_folder.name\n",
    "    \n",
    "    if config.verbose>0:\n",
    "        print(f\"\\tAssembling photon data and exposure for source {source.name} from\"\\\n",
    "              f' folder \"{data_folder}\",\\n\\t with {len(data_files)} files,'\\\n",
    "              f' last file:  {data_files[-1].name}: ', end='')\n",
    "    \n",
    "    w1,w2 = week_range or  config.week_range\n",
    "    if w1 is not None or w2 is not None:\n",
    "        if config.verbose>0:\n",
    "            print(f'\\tLoading weeks {w1}:{w2}')\n",
    "        data_files= data_files[w1:w2]\n",
    "    else:\n",
    "        if config.verbose>0: print('loading all files')\n",
    "            \n",
    "            \n",
    "\n",
    "    verbose, config.verbose=config.verbose, 0\n",
    "    # list of data framees\n",
    "    pp = []\n",
    "    ee = []\n",
    "    for f in data_files:\n",
    "        print('.', end='')\n",
    "        with open(f, 'rb') as inp:\n",
    "            week = pickle.load(inp)\n",
    "\n",
    "        photons = _get_photons_near_source(config, source, week )\n",
    "        exposure = _calculate_exposure_for_source(config, source, week )\n",
    "        if photons is not None:  pp.append(photons)\n",
    "        ee.append(exposure)\n",
    "        \n",
    "    print('');    \n",
    "    config.verbose=verbose\n",
    "    # concatenate the two lists of DataFrames\n",
    "    p_df = pd.concat(pp, ignore_index=True)\n",
    "    e_df = pd.concat(ee, ignore_index=True)\n",
    "    \n",
    "    # process exposure to find runs, and add tau column to photons\n",
    "    runs = add_exposure_to_events(config, e_df, p_df)\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        times = p_df.time.values\n",
    "        print(f'Loaded {len(p_df):,} photons from {UTC(times[0])} to  {UTC(times[-1])} ')\n",
    "        print(f'Calculated {len(e_df):,} exposure entries')\n",
    "        \n",
    "    # add weights to photon data\n",
    "    add_weights(config, p_df, source)\n",
    "        \n",
    "    return p_df, e_df, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "#photons, exposure, runs= _load_from_weekly_data(config=Config(), source=PointSource('Geminga'), week_range=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  hide\n",
    "# len(ff), ff[-132]\n",
    "# _load_from_weekly_data(config, source, week_range=(-132, -100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from wtlike.simulation import *\n",
    "class SourceData(object):\n",
    "    \"\"\" Load the photon data near the source and associated exposure. \n",
    "    \n",
    "    Either from:\n",
    "      1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
    "      2. the cache, with key `{source.name}_data`\n",
    "    \n",
    "    * source : name, PointSource, or Simulation\n",
    "    * `config` : basic configuration\n",
    "    * `source` : PointSource object if specified\n",
    "    * `clear` : if set, overwrite the cached results\n",
    "    \n",
    "    Calculate the values for\n",
    "    \n",
    "    * S, B : sums of w and 1-w\n",
    "    * exptot : total associated exposure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source, config=None,  clear=False, \n",
    "                 week_range=None, key=''):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        self.config = config if config else Config()\n",
    "        assert self.config.valid\n",
    "        self.verbose = self.config.verbose\n",
    "        self.simulated=False\n",
    "        \n",
    "        ## source is either a name, a PointSource object, or a Simulation\n",
    "        if type(source)==str:\n",
    "\n",
    "            try:\n",
    "                self.source = PointSource(source) \n",
    "            except Exception as e:\n",
    "                print(f'{e}', file=sys.stderr)\n",
    "                raise\n",
    "\n",
    "            self.source_name = self.source.name\n",
    " \n",
    "        elif isinstance(source, PointSource):\n",
    "            self.source = source # do I need this?\n",
    "            self.source_name = source.name\n",
    "            \n",
    "        elif isinstance(source, Simulation):\n",
    "            self.simulated=True\n",
    "            self.source=None \n",
    "            self.source_name = source.name\n",
    "            # can put this into cache\n",
    "            source.run()\n",
    "            self.photons = source.photons\n",
    "            self.exposure = source.exposure\n",
    "\n",
    "        key = f'{self.source_name}_data' if key=='' else key\n",
    "        #self.source.data_key = key\n",
    "        \n",
    "        if not self.simulated:\n",
    "            # either load from data, or from a chache\n",
    "            if self.config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "                raise Exception(f'Data for {self.source_name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "\n",
    "            r = self.config.cache(key, \n",
    "                            _load_from_weekly_data, self.config, self.source, week_range,\n",
    "                            overwrite=clear,\n",
    "                            description=f'SourceData: photons and exposure for {self.source_name}')\n",
    "            photons, self.exposure = r[:2]\n",
    "            self.runs = r[2] if len(r)==3 else None\n",
    "            # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "            good = np.logical_not(np.isnan(photons.weight))\n",
    "            self.photons = photons.loc[good]\n",
    "\n",
    "        else: #TODO\n",
    "            pass\n",
    "        \n",
    "        # make range of MJD or days available\n",
    "        self.start = self.exposure.start[0]\n",
    "        self.stop =  self.exposure.stop.values[-1]\n",
    "        self.exptot = self.exposure.exp.sum() \n",
    "\n",
    "        # estimates for signal and background counts in total exposure\n",
    "        w = self.photons.weight\n",
    "        self.S = np.sum(w)\n",
    "        self.B = np.sum(1-w)\n",
    "        \n",
    "        if self.verbose>0:\n",
    "            print(SourceData.__repr__(self))\n",
    "    \n",
    "    def rates(self):\n",
    "        print(f'Average rates for {self.source_name}: signal {self.S/self.exptot:.2e}/s, background {self.B/self.exptot:.2e}/s')\n",
    "\n",
    "    def __repr__(self):\n",
    "        time = self.photons.time.values\n",
    "        \n",
    "        exp = self.exposure\n",
    "        days  = np.sum(exp.stop-exp.start); secs = days*24*3600\n",
    "        exp_text = f' average rate {self.exptot/secs:.0f} cm^2 for {secs/1e6:.1f} Ms'\n",
    "        \n",
    "        if not self.simulated:\n",
    "            photon_text = f'photons from {UTC(time[0])[:10]} to {UTC(time[-1])[:10]}'\n",
    "        else:\n",
    "            photon_text = f'simulated photons over {days:.1f} days.'\n",
    "\n",
    "        r = f'SourceData: Source {self.source_name} with:'\\\n",
    "            f'\\n\\t data:     {len(self.photons):9,} {photon_text}'\\\n",
    "            f'\\n\\t exposure: {len(self.exposure):9,} intervals, {exp_text}'\n",
    " \n",
    "        src_rate, bkg_rate = self.S/self.exptot,  self.B/self.exptot\n",
    "        r+= f'\\n\\t rates:  source {src_rate:.2e}/s, background {bkg_rate:.2e}/s, S/N ratio {src_rate/bkg_rate:.2f}'\n",
    "\n",
    "        return r\n",
    "    \n",
    "    def binned_exposure(self, time_edges):\n",
    "        \"\"\"Bin the exposure\n",
    "        \n",
    "        - time_bins: list of edges.  \n",
    "        \"\"\"\n",
    "        return binned_exposure(self.config, self.exposure,  time_edges)\n",
    "    \n",
    "    def binned_cos_theta(self, time_bins=None):\n",
    "        \"\"\" Calculate average cosine of angle with respect to bore axis, per time bin\n",
    "        \"\"\"\n",
    "        if time_bins is None:\n",
    "            time_bins = get_default_bins(self.config, self.exposure)\n",
    "        df = self.exposure.copy()\n",
    "        estop =df.stop.values\n",
    "        df.loc[:,'tbin'] =np.digitize(estop, time_bins)\n",
    "        ct = df.groupby('tbin').mean()['cos_theta']\n",
    "        return ct, time_bins\n",
    "    \n",
    "    def weight_histogram(self, nbins=1000, key=''):\n",
    "        \"\"\" return a weight distribution\n",
    "        \"\"\"\n",
    "        def doit(nbins):\n",
    "            return np.histogram(self.p_df.weight.values, np.linspace(0,1,nbins+1))[0]\n",
    "\n",
    "        key = f'{self.source_name}_weight_hist' if key=='' else key\n",
    "        description = f'Weight histogram for {self.source_name}' if self.config.verbose>0 else ''\n",
    "        return self.config.cache(key, doit, nbins, description=description)\n",
    "        \n",
    "    def plot_data(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if self.simulated:\n",
    "            print(f'Simulated!')\n",
    "            fig, (ax1, ax4) = plt.subplots(1,2, figsize=(8,4))\n",
    "            ax1.hist(self.photons.time.values, 500, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');\n",
    "\n",
    "            \n",
    "        else:\n",
    "            fig, (ax1,ax2, ax3,ax4) = plt.subplots(1,4, figsize=(15,4))\n",
    "            ax1.hist(self.photons.time.values, 100, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "            ax2.hist(self.photons.radius.values**2, 100, histtype='step', log=True);\n",
    "            ax2.set(xlabel='Radius**2 (deg**2)', ylim=(100, None));\n",
    "\n",
    "            ax3.hist(self.photons.band, 32, histtype='step', log=True);\n",
    "            ax3.set(xlabel='Band index')\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-stadium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 499 photons\n",
      "SourceData: Source test_sim with:\n",
      "\t data:           499 simulated photons over 1.0 days.\n",
      "\t exposure:       288 intervals,  average rate 3000 cm^2 for 0.1 Ms\n",
      "\t rates:  source 9.90e-07/s, background 9.35e-07/s, S/N ratio 1.06\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "sim = Simulation('test_sim', src_rate=1e-6, tstart=0, tstop=1, )\n",
    "simsd = SourceData(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SourceData: photons and exposure for Geminga: Restoring from cache with key \"Geminga_data\"\n",
      "SourceData: Source Geminga with:\n",
      "\t data:     1,213,841 photons from 2008-08-04 to 2021-05-05\n",
      "\t exposure: 3,117,669 intervals,  average rate 2874 cm^2 for 93.2 Ms\n",
      "\t rates:  source 3.28e-06/s, background 1.25e-06/s, S/N ratio 2.62\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "#sd = SourceData('3C 279', week_range=(500,None), key=None)\n",
    "sd = SourceData('Geminga');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-internet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SourceData\" class=\"doc_header\"><code>class</code> <code>SourceData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SourceData</code>(**`source`**, **`config`**=*`None`*, **`clear`**=*`False`*, **`week_range`**=*`None`*, **`key`**=*`''`*)\n",
       "\n",
       "Load the photon data near the source and associated exposure. \n",
       "\n",
       "Either from:\n",
       "  1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
       "  2. the cache, with key `{source.name}_data`\n",
       "\n",
       "* source : name, PointSource, or Simulation\n",
       "* [`config`](/wtlikeconfig) : basic configuration\n",
       "* `source` : PointSource object if specified\n",
       "* `clear` : if set, overwrite the cached results\n",
       "\n",
       "Calculate the values for\n",
       "\n",
       "* S, B : sums of w and 1-w\n",
       "* exptot : total associated exposure"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SourceData.binned_exposure\" class=\"doc_header\"><code>SourceData.binned_exposure</code><a href=\"__main__.py#L114\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SourceData.binned_exposure</code>(**`time_edges`**)\n",
       "\n",
       "Bin the exposure\n",
       "\n",
       "- time_bins: list of edges.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SourceData)\n",
    "show_doc(SourceData.binned_exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90-main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Tue May 18 09:24:35 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-alignment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
