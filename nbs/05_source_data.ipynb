{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp source_data\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-adaptation",
   "metadata": {},
   "source": [
    "# Source Data management\n",
    "> Extract data for a source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-startup",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Given a point source, the class `SourceData` manages all data-oriented operations, providing all that is necessary to create a set of cells. It depends on the modules\n",
    "\n",
    "* `config` \n",
    "    This must set up the paths to the data created by `data_man`, and define paths for the effective area and weight files\n",
    "\n",
    "* effective_area\n",
    "* weights\n",
    "* exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try: # make conda-build happy?\n",
    "    import healpy\n",
    "except:\n",
    "    pass\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from wtlike.config import *\n",
    "from wtlike.data_man import *\n",
    "from wtlike.effective_area import *\n",
    "from wtlike.weights import *\n",
    "from wtlike.exposure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-conversion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters \n",
      "  verbose         : 2\n",
      "  wtlike_data     : /home/burnett/wtlike_data\n",
      "  cachepath       : /home/burnett/wtlike_cache\n",
      "  radius          : 4\n",
      "  cos_theta_max   : 0.4\n",
      "  z_max           : 100\n",
      "  week_range      : (None, None)\n",
      "  time_bins       : (0, 0, 7)\n",
      "  use_uint8       : False\n",
      "  nside           : 1024\n",
      "  nest            : True\n",
      "  bins_per_decade : 5\n",
      "  base_spectrum   : lambda E: (E/1000)**-2.1\n",
      "  energy_range    : (100.0, 1000000.0)\n",
      "  likelihood_rep  : poisson\n",
      "  errors          : []\n",
      "\n",
      "Weekly folder \"/home/burnett/wtlike_data/data_files\" contains 669 weeks,  9 to 678, last week has 0.3 days, ends at 2021-05-27 07:17\n",
      "Source \"BL Lac\" at: (l,b)=(92.590,-10.441)\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "# check the weekly files\n",
    "config = Config(wtlike_data='~/wtlike_data', verbose=2)\n",
    "if config.valid:\n",
    "    ff = get_data_files(config)\n",
    "    source = PointSource('BL Lac') #Geminga')\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _exposure(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)\n",
    "\n",
    "def _calculate_exposure_for_source(config, source, week):\n",
    "    \"\"\"\n",
    "    Calcualate the exposure for the source during the given week\n",
    "    \"\"\"\n",
    "    df = week['sc_data']\n",
    "    \n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "\n",
    "    return  pd.DataFrame( \n",
    "        dict(\n",
    "            start=df.start[mask], \n",
    "            stop=df.stop[mask], \n",
    "            exp=_exposure(config, livetime, pcosines[mask]),\n",
    "            cos_theta=pcosines[mask],\n",
    "        ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-interstate",
   "metadata": {},
   "source": [
    "### Check exposure with last data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-appearance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFound 683 S/C entries:  200 remain after zenith and theta cuts\n",
      "Calculate exposure using the energy domain 100.0-1000000.0 5 bins/decade\n",
      "       start      stop        exp  cos_theta\n",
      "38  59361.03  59361.03   33240.69       0.41\n",
      "39  59361.03  59361.03   57614.37       0.52\n",
      "40  59361.03  59361.03   78261.92       0.61\n",
      "41  59361.03  59361.03   95444.33       0.69\n",
      "42  59361.03  59361.03  111481.18       0.76\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "if config.valid:\n",
    "    filename=ff[-1]\n",
    "    week = pickle.load( open(filename,'rb') )\n",
    "    e_df = _calculate_exposure_for_source(config, source, week); \n",
    "    print(e_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _get_photons_near_source(config, source, week): #tzero, photon_df):\n",
    "    \"\"\"\n",
    "    Select the photons near a source\n",
    "    \n",
    "    - source : a PointSource object\n",
    "    - week : dict with \n",
    "        - tzero : start time for the photon\n",
    "        - photon_df : DataFrame with photon data\n",
    "    \n",
    "    Returns a DF with \n",
    "    - `band` index, \n",
    "    - `time` in MJD (added tstart and converted from MET)\n",
    "    - `pixel` index, nest indexing \n",
    "    - `radius` distance in deg from source direction\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cone(config, source, nest=True):\n",
    "        # cone geometry stuff: get corresponding pixels and center vector\n",
    "        l,b,radius = source.l, source.b, config.radius\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "        conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "        center = healpy.dir2vec(l,b, lonlat=True)\n",
    "        return center, conepix\n",
    "    \n",
    "    center, conepix = _cone(config,source)\n",
    "\n",
    "    df = week['photons']\n",
    "    tstart = week['tstart']\n",
    "    allpix = df.nest_index.values\n",
    "\n",
    "    # select by comparing high-order pixels (faster)\n",
    "    shift=11\n",
    "    a = np.right_shift(allpix, shift)\n",
    "    c = np.unique(np.right_shift(conepix, shift))\n",
    "    incone = np.isin(a,c)\n",
    "    \n",
    "    if sum(incone)<2:\n",
    "        if config.verbose>1:\n",
    "            print(f'\\nWeek at {UTC(MJD(tstart))} has 0 or 1 photons')\n",
    "        return\n",
    "    \n",
    "    if config.verbose>2:\n",
    "        a, b = sum(incone), len(allpix)\n",
    "        print(f'Select photons for source {source.name}:\\n\\tPixel cone cut: select {a} from {b} ({100*a/b:.1f}%)')\n",
    "\n",
    "    # cut df to entries in the cone\n",
    "    dfc = df[incone]\n",
    "\n",
    "    # distance from center for all accepted photons\n",
    "    ll,bb = healpy.pix2ang(config.nside, dfc.nest_index,  nest=True, lonlat=True)\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32))\n",
    "    in_cone = t2<config.radius\n",
    "\n",
    "    if config.verbose>2:\n",
    "        print(f'\\tGeometric cone cut: select {sum(in_cone)}')\n",
    "    # assume all in the GTI (should check)\n",
    "\n",
    "    # times: convert to float, add tstart, convert to MJD\n",
    "    time = MJD(np.array(dfc.time, float)+tstart)\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "        [np.array(dfc.band), time, dfc.nest_index, np.atleast_1d(t2)],\n",
    "        names='band time pixel radius'.split()))[in_cone]\n",
    "\n",
    "    # make sure times are monotonic by sorting (needed for most weeks after March 2018)\n",
    "    out_df = out_df.sort_values(by='time')\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-intermediate",
   "metadata": {},
   "source": [
    "### test photon data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  hide\n",
    "# # if config.valid:\n",
    "# for i,f in enumerate(ff[300:]):\n",
    "#     week = pickle.load(open(f,'rb')); \n",
    "#     tstart = week['tstart']\n",
    "#     print(f'{i} ', end='')\n",
    "#     #print(f'TSTART: MET {tstart}, UTC {UTC(MJD(tstart))}')\n",
    "#     p_df = _get_photons_near_source(config, source, week )\n",
    "# #     print(len(p_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export  \n",
    "\n",
    "#     gstart, gstop = keep[0::2], keep[1::2]\n",
    "#     df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "    \n",
    "#     # add column with duration in sec\n",
    "#     df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "#     return df.query(f'duration>{min_duration}')\n",
    "\n",
    "def time_bin_edges(config, exposure, tbin=None):\n",
    "    \"\"\"Return an interleaved array of start/stop values\n",
    "    \n",
    "    tbin: an array (a,b,d), default config.time_bins\n",
    "    \n",
    "    interpretation of a, b:\n",
    "\n",
    "        if > 50000, interpret as MJD\n",
    "        if <0, back from stop\n",
    "        otherwise, offset from start\n",
    "        \n",
    "    d : if positive, the day bin size\n",
    "        if 0; return contiguous bins\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # nominal total range, MJD edges\n",
    "    start = np.round(exposure.start.values[0])\n",
    "    stop =  np.round(exposure.stop.values[-1])\n",
    "\n",
    "    a, b, step = tbin if tbin is not None else config.time_bins\n",
    "    \n",
    "\n",
    "    if a>50000: start=a\n",
    "    elif a<0: start = stop+a\n",
    "    else : start += a\n",
    "\n",
    "\n",
    "    if b>50000: stop=b\n",
    "    elif b>0: stop = start+b\n",
    "    else: stop += b\n",
    "    \n",
    "    if step<=0:\n",
    "        return contiguous_bins(exposure.query(f'{start}<start<{stop}'),)\n",
    "\n",
    "    # adjust stop\n",
    "    nbins = int((stop-start)/step)\n",
    "    assert nbins>0, 'Bad binning: no bins'\n",
    "    stop = start+(nbins)*step\n",
    "    u =  np.linspace(start,stop, nbins+1 )\n",
    "    \n",
    "    # make an interleaved start/stop array\n",
    "    v = np.empty(2*nbins, float)\n",
    "    v[0::2] = u[:-1]\n",
    "    v[1::2] = u[1:]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def contiguous_bins(exposure, min_gap=20, min_duration=600):\n",
    "    \n",
    "    \"\"\" return a start/stop interleaved array for contiguous intervals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    stop = exposure.stop.values\n",
    "    start = exposure.start.values\n",
    "\n",
    "    # interleave  the starts and stops\n",
    "    ssint = np.empty(2*len(start))\n",
    "    ssint[0::2] = start\n",
    "    ssint[1::2] = stop\n",
    "\n",
    "    # Tag the (stpp,start) pairs < 10 sec as  not adjacent\n",
    "    not_adjacent = np.diff(ssint)[1::2] > min_gap/(24*3600) ; \n",
    "    #print(f'{sum(not_adjacent)} (start,stop) pairs are not closer than {min_gap} s')\n",
    "\n",
    "    # make a mask, keep ends\n",
    "    mask = np.empty(2*len(start), bool)\n",
    "    mask[0] = mask[-1] = True\n",
    "    # \n",
    "\n",
    "    # insert into mask -- keep only the (stop,start) pairs  which are not adjacent\n",
    "    mask[1:-2:2] = not_adjacent\n",
    "    mask[2:-1:2] = not_adjacent\n",
    "    \n",
    "    # apply mask, split into start and stop\n",
    "    keep = ssint[mask]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-dayton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>exp</th>\n",
       "      <th>cos_theta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>59361.03</td>\n",
       "      <td>59361.03</td>\n",
       "      <td>33240.69</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>59361.03</td>\n",
       "      <td>59361.03</td>\n",
       "      <td>57614.37</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>59361.03</td>\n",
       "      <td>59361.03</td>\n",
       "      <td>78261.92</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>59361.03</td>\n",
       "      <td>59361.03</td>\n",
       "      <td>95444.33</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>59361.03</td>\n",
       "      <td>59361.03</td>\n",
       "      <td>111481.18</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>59361.30</td>\n",
       "      <td>59361.30</td>\n",
       "      <td>124233.26</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>59361.30</td>\n",
       "      <td>59361.30</td>\n",
       "      <td>122891.24</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>59361.30</td>\n",
       "      <td>59361.30</td>\n",
       "      <td>121267.15</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>59361.30</td>\n",
       "      <td>59361.30</td>\n",
       "      <td>119792.21</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>59361.30</td>\n",
       "      <td>59361.30</td>\n",
       "      <td>87784.86</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        start      stop        exp  cos_theta\n",
       "38   59361.03  59361.03   33240.69       0.41\n",
       "39   59361.03  59361.03   57614.37       0.52\n",
       "40   59361.03  59361.03   78261.92       0.61\n",
       "41   59361.03  59361.03   95444.33       0.69\n",
       "42   59361.03  59361.03  111481.18       0.76\n",
       "..        ...       ...        ...        ...\n",
       "678  59361.30  59361.30  124233.26       0.84\n",
       "679  59361.30  59361.30  122891.24       0.83\n",
       "680  59361.30  59361.30  121267.15       0.82\n",
       "681  59361.30  59361.30  119792.21       0.82\n",
       "682  59361.30  59361.30   87784.86       0.81\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exposure = e_df; e_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(time_bin_edges(config, exposure, (0,0,1)) )\n",
    "# print(time_bin_edges(config, exposure, (-1,0,0)))\n",
    "# print(time_bin_edges(config, exposure, (0,0,0.5)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,0,2)) )\n",
    "# print(time_bin_edges(config, exposure, (0,1,0.25)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,-4,0.25)) )   \n",
    "# print(time_bin_edges(config, exposure, (59326, 59330,0.5)) )    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep = contiguous_bins(exposure)\n",
    "# gstart, gstop = keep[0::2], keep[1::2]\n",
    "# df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "\n",
    "# # add column with duration in sec\n",
    "# df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "\n",
    "# df.hist('duration', bins=np.linspace(0,4e3,131));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def binned_exposure(config, exposure, time_edges):\n",
    "    \"\"\"Bin the exposure\n",
    "\n",
    "    - time_bins: list of edges, as an interleaved start/stop array\n",
    "       \n",
    "        \n",
    "    returns  array of exposure integrated over each time bin, times 1e-9\n",
    "    it is interleaved, client must apply [0::2] selection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get exposure calculation\n",
    "    exp   =exposure.exp.values\n",
    "    estart= exposure.start.values\n",
    "    estop = exposure.stop.values\n",
    "\n",
    "    # determine bins,\n",
    "\n",
    "    #use cumulative exposure to integrate over larger periods\n",
    "    cumexp = np.concatenate(([0],np.cumsum(exp)) )\n",
    "\n",
    "    # get index into tstop array of the bin edges\n",
    "    edge_index = np.searchsorted(estop, time_edges)\n",
    "    \n",
    "    # return the exposure integrated over the intervals\n",
    "    cum = cumexp[edge_index]\n",
    "   \n",
    "    # difference is exposure per interval: normalize it here\n",
    "    bexp = np.diff(cum) \n",
    "#     if config.verbose>1:\n",
    "#         print(f'Relative exposure per bin:\\n{pd.Series(bexp).describe(percentiles=[])}')\n",
    "    return bexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _load_from_weekly_data(config, source, week_range=None):\n",
    "    \"\"\"\n",
    "    Generate combinded DataFrames from a list of pickled files\n",
    "    Either weekly or monthly\n",
    "    \n",
    "    kwargs:\n",
    "    - week_range\n",
    "    \"\"\"\n",
    "    \n",
    "    # check weights\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    assert weight_file is not None\n",
    "    \n",
    "    data_folder = config.wtlike_data/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    iname = data_folder.name\n",
    "    \n",
    "    if config.verbose>0:\n",
    "        print(f\"\\tAssembling photon data and exposure for source {source.name} from\"\\\n",
    "              f' folder \"{data_folder}\",\\n\\t with {len(data_files)} files,'\\\n",
    "              f' last file:  {data_files[-1].name}: ', end='')\n",
    "    \n",
    "    w1,w2 = week_range or  config.week_range\n",
    "    if w1 is not None or w2 is not None:\n",
    "        if config.verbose>0:\n",
    "            print(f'\\tLoading weeks {w1}:{w2}')\n",
    "        data_files= data_files[w1:w2]\n",
    "    else:\n",
    "        if config.verbose>0: print('loading all files')\n",
    " \n",
    "    verbose, config.verbose=config.verbose, 0\n",
    "    \n",
    "    # list of data framees\n",
    "    pp = []\n",
    "    ee = []\n",
    "    runs = []\n",
    "    for f in data_files:\n",
    "        print('.', end='')\n",
    "        with open(f, 'rb') as inp:\n",
    "            week = pickle.load(inp)\n",
    "\n",
    "        photons = _get_photons_near_source(config, source, week )\n",
    "        exposure = _calculate_exposure_for_source(config, source, week )\n",
    "        if photons is not None:\n",
    "            add_weights(config, photons, source)\n",
    "            pp.append(photons)\n",
    "            runs.append( add_exposure_to_events(config, exposure, photons,)\n",
    "                       )\n",
    "        ee.append(exposure)\n",
    "        \n",
    "    print('');    \n",
    "    config.verbose=verbose\n",
    "    # concatenate the two lists of DataFrames\n",
    "    p_df = pd.concat(pp, ignore_index=True)\n",
    "    e_df = pd.concat(ee, ignore_index=True)\n",
    "    \n",
    "    # process exposure to find runs, and add tau column to photons\n",
    "    ### runs = add_exposure_to_events(config, e_df, p_df)\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        times = p_df.time.values\n",
    "        print(f'Loaded {len(p_df):,} photons from {UTC(times[0])} to  {UTC(times[-1])} ')\n",
    "        print(f'Calculated {len(e_df):,} exposure entries')\n",
    "        \n",
    "    # add weights to photon data\n",
    "    ### add_weights(config, p_df, source)\n",
    "        \n",
    "    return p_df, e_df, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAssembling photon data and exposure for source 4FGL J1257.0-6339 from folder \"/home/burnett/wtlike_data/data_files\",\n",
      "\t with 669 files, last file:  week_678.pkl: \tLoading weeks 0:2\n",
      "..\n",
      "Loaded 2,120 photons from 2008-08-04 17:29 to  2008-08-14 01:08 \n",
      "Calculated 4,485 exposure entries\n"
     ]
    }
   ],
   "source": [
    "config=Config()\n",
    "config.verbose=2\n",
    "source = PointSource('4FGL J1257.0-6339')\n",
    "photons, exposure, runs= _load_from_weekly_data(config, source=source, week_range=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-balloon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band</th>\n",
       "      <th>time</th>\n",
       "      <th>radius</th>\n",
       "      <th>weight</th>\n",
       "      <th>tau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>54682.73</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.48e-03</td>\n",
       "      <td>15.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>54682.75</td>\n",
       "      <td>3.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>54682.75</td>\n",
       "      <td>3.94</td>\n",
       "      <td>2.51e-04</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>54682.75</td>\n",
       "      <td>1.61</td>\n",
       "      <td>7.14e-04</td>\n",
       "      <td>21.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>54682.75</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.69e-04</td>\n",
       "      <td>10.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115</th>\n",
       "      <td>5</td>\n",
       "      <td>54691.97</td>\n",
       "      <td>2.59</td>\n",
       "      <td>4.57e-04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>5</td>\n",
       "      <td>54691.97</td>\n",
       "      <td>2.98</td>\n",
       "      <td>8.01e-04</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>3</td>\n",
       "      <td>54692.05</td>\n",
       "      <td>3.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2118</th>\n",
       "      <td>5</td>\n",
       "      <td>54692.05</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.04e-03</td>\n",
       "      <td>23.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119</th>\n",
       "      <td>4</td>\n",
       "      <td>54692.05</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.28e-04</td>\n",
       "      <td>15.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2120 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      band      time  radius    weight    tau\n",
       "0        7  54682.73    2.52  2.48e-03  15.50\n",
       "1        3  54682.75    3.59       NaN  68.98\n",
       "2        7  54682.75    3.94  2.51e-04  11.28\n",
       "3        0  54682.75    1.61  7.14e-04  21.90\n",
       "4        0  54682.75    2.98  3.69e-04  10.59\n",
       "...    ...       ...     ...       ...    ...\n",
       "2115     5  54691.97    2.59  4.57e-04   0.00\n",
       "2116     5  54691.97    2.98  8.01e-04  13.50\n",
       "2117     3  54692.05    3.17       NaN  67.15\n",
       "2118     5  54692.05    1.71  1.04e-03  23.71\n",
       "2119     4  54692.05    3.11  2.28e-04  15.76\n",
       "\n",
       "[2120 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photons(config, source, clear=False, key=''):\n",
    "    \n",
    "    key = f'{source.name}_data' if key=='' else key\n",
    "    if config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "        raise Exception(f'Data for {self.source_name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "\n",
    "    r = config.cache(key, \n",
    "                    _load_from_weekly_data, config, source, \n",
    "                    overwrite=clear,\n",
    "                    description=f'SourceData: photons and exposure for {source.name}')\n",
    "    photons, exposure = r[:2]\n",
    "    runs = r[2] if len(r)==3 else None\n",
    "    # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "    good = np.logical_not(np.isnan(photons.weight))\n",
    "    return photons.loc[good], exposure, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-treat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SourceData: photons and exposure for Geminga: Restoring from cache with key \"Geminga_data\"\n"
     ]
    }
   ],
   "source": [
    "src, exp, runs = load_photons(Config(), PointSource('Geminga'), clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-garbage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mload_photons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      /mnt/c/Users/thbur/OneDrive/work/wtlike/nbs/<ipython-input-17-d3b310e84a74>\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_photons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-samuel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-05-21 07:04'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "UTC(runs.time.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from wtlike.simulation import *\n",
    "class SourceData(object):\n",
    "    \"\"\" Load the photon data near the source and associated exposure. \n",
    "    \n",
    "    Either from:\n",
    "      1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
    "      2. the cache, with key `{source.name}_data`\n",
    "    \n",
    "    * source : name, PointSource, or Simulation\n",
    "    * `config` : basic configuration\n",
    "    * `source` : PointSource object if specified\n",
    "    * `clear` : if set, overwrite the cached results\n",
    "    \n",
    "    Calculate the values for\n",
    "    \n",
    "    * S, B : sums of w and 1-w\n",
    "    * exptot : total associated exposure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source, config=None,  clear=False, \n",
    "                 week_range=None, key=''):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        self.config = config if config else Config()\n",
    "        assert self.config.valid\n",
    "        self.verbose = self.config.verbose\n",
    "        self.simulated=False\n",
    "        \n",
    "        ## source is either a name, a PointSource object, or a Simulation\n",
    "        if type(source)==str:\n",
    "\n",
    "            try:\n",
    "                self.source = PointSource(source) \n",
    "            except Exception as e:\n",
    "                print(f'{e}', file=sys.stderr)\n",
    "                raise\n",
    "\n",
    "            self.source_name = self.source.name\n",
    " \n",
    "        elif isinstance(source, PointSource):\n",
    "            self.source = source # do I need this?\n",
    "            self.source_name = source.name\n",
    "            \n",
    "        elif isinstance(source, Simulation):\n",
    "            self.simulated=True\n",
    "            self.source=None \n",
    "            self.source_name = source.name\n",
    "            # can put this into cache\n",
    "            source.run()\n",
    "            self.photons = source.photons\n",
    "            self.exposure = source.exposure\n",
    "\n",
    "        if self.source is not None:            \n",
    "            key = f'{self.source.filename}_data' if key=='' else key\n",
    "            self.source.data_key = key\n",
    "        else: # no cache for sim, yet\n",
    "            key=None \n",
    "        \n",
    "        \n",
    "        if not self.simulated:\n",
    "            # either load from data, or from a chache\n",
    "            if self.config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "                raise Exception(f'Data for {self.source_name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "\n",
    "            r = self.config.cache(key, \n",
    "                            _load_from_weekly_data, self.config, self.source, week_range,\n",
    "                            overwrite=clear,\n",
    "                            description=f'SourceData: photons and exposure for {self.source_name}')\n",
    "            photons, self.exposure = r[:2]\n",
    "            self.runs = r[2] if len(r)==3 else None\n",
    "            # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "            good = np.logical_not(np.isnan(photons.weight))\n",
    "            self.photons = photons.loc[good]\n",
    "\n",
    "        else: #TODO\n",
    "            pass\n",
    "        \n",
    "        # make range of MJD or days available\n",
    "        self.start = self.exposure.start[0]\n",
    "        self.stop =  self.exposure.stop.values[-1]\n",
    "        self.exptot = self.exposure.exp.sum() \n",
    "\n",
    "        # estimates for signal and background counts in total exposure\n",
    "        w = self.photons.weight\n",
    "        self.S = np.sum(w)\n",
    "        self.B = np.sum(1-w)\n",
    "        \n",
    "        if self.verbose>0:\n",
    "            print(SourceData.__repr__(self))\n",
    "    \n",
    "    def rates(self):\n",
    "        print(f'Average fluxes for {self.source_name}: signal {self.S/self.exptot:.2e}/s, background {self.B/self.exptot:.2e}/s')\n",
    "\n",
    "    def __repr__(self):\n",
    "        time = self.photons.time.values\n",
    "        \n",
    "        exp = self.exposure\n",
    "        days  = np.sum(exp.stop-exp.start); secs = days*24*3600\n",
    "        exp_text = f' average flux {self.exptot/secs:.0f} cm^2 for {secs/1e6:.1f} Ms'\n",
    "        \n",
    "        if not self.simulated:\n",
    "            photon_text = f'photons from {UTC(time[0])[:10]} to {UTC(time[-1])[:10]}'\n",
    "        else:\n",
    "            photon_text = f'simulated photons over {days:.1f} days.'\n",
    "\n",
    "        r = f'SourceData: Source {self.source_name} with:'\\\n",
    "            f'\\n\\t data:     {len(self.photons):9,} {photon_text}'\\\n",
    "            f'\\n\\t exposure: {len(self.exposure):9,} intervals, {exp_text}'\n",
    " \n",
    "        self.src_flux, self.bkg_flux = self.S/self.exptot,  self.B/self.exptot\n",
    "        r+= f'\\n\\t rates:  source {self.src_flux:.2e}/s, background {self.bkg_flux:.2e}/s,'\\\n",
    "            f' S/N ratio {self.src_flux/self.bkg_flux:.2e}'\n",
    "\n",
    "        return r\n",
    "    \n",
    "    def binned_exposure(self, time_edges):\n",
    "        \"\"\"Bin the exposure\n",
    "        \n",
    "        - time_bins: list of edges.  \n",
    "        \"\"\"\n",
    "        return binned_exposure(self.config, self.exposure,  time_edges)\n",
    "    \n",
    "    def binned_cos_theta(self, time_bins=None):\n",
    "        \"\"\" Calculate average cosine of angle with respect to bore axis, per time bin\n",
    "        \"\"\"\n",
    "        if time_bins is None:\n",
    "            time_bins = get_default_bins(self.config, self.exposure)\n",
    "        df = self.exposure.copy()\n",
    "        estop =df.stop.values\n",
    "        df.loc[:,'tbin'] =np.digitize(estop, time_bins)\n",
    "        ct = df.groupby('tbin').mean()['cos_theta']\n",
    "        return ct, time_bins\n",
    "    \n",
    "    def weight_histogram(self, nbins=1000, key=''):\n",
    "        \"\"\" return a weight distribution\n",
    "        \"\"\"\n",
    "        def doit(nbins):\n",
    "            return np.histogram(self.p_df.weight.values, np.linspace(0,1,nbins+1))[0]\n",
    "\n",
    "        key = f'{self.source_name}_weight_hist' if key=='' else key\n",
    "        description = f'Weight histogram for {self.source_name}' if self.config.verbose>0 else ''\n",
    "        return self.config.cache(key, doit, nbins, description=description)\n",
    "        \n",
    "    def plot_data(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if self.simulated:\n",
    "            print(f'Simulated!')\n",
    "            fig, (ax1, ax4) = plt.subplots(1,2, figsize=(8,4))\n",
    "            ax1.hist(self.photons.time.values, 500, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');\n",
    "\n",
    "            \n",
    "        else:\n",
    "            fig, (ax1,ax2, ax3,ax4) = plt.subplots(1,4, figsize=(15,4))\n",
    "            ax1.hist(self.photons.time.values, 100, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "            ax2.hist(self.photons.radius.values**2, 100, histtype='step', log=True);\n",
    "            ax2.set(xlabel='Radius**2 (deg**2)', ylim=(100, None));\n",
    "\n",
    "            ax3.hist(self.photons.band, 32, histtype='step', log=True);\n",
    "            ax3.set(xlabel='Band index')\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 505 photons\n",
      "SourceData: Source test_sim with:\n",
      "\t data:           505 simulated photons over 1.0 days.\n",
      "\t exposure:       288 intervals,  average flux 3000 cm^2 for 0.1 Ms\n",
      "\t rates:  source 9.75e-07/s, background 9.74e-07/s, S/N ratio 1.00e+00\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "sim = Simulation('test_sim', src_flux=1e-6, tstart=0, tstop=1, )\n",
    "simsd = SourceData(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# config=Config(); config.verbose=2\n",
    "# source = PointSource('4FGL J1257.0-6339', nickname='J1257')\n",
    "# sd = SourceData(source, config=config, week_range=None, key=None)\n",
    "#sd = SourceData('Geminga');\n",
    "\n",
    "\n",
    "# b8 = sd.photons.query('band==12')\n",
    "# plt.semilogy(b8.radius, b8.weight, '.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-auction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SourceData\" class=\"doc_header\"><code>class</code> <code>SourceData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SourceData</code>(**`source`**, **`config`**=*`None`*, **`clear`**=*`False`*, **`week_range`**=*`None`*, **`key`**=*`''`*)\n",
       "\n",
       "Load the photon data near the source and associated exposure. \n",
       "\n",
       "Either from:\n",
       "  1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
       "  2. the cache, with key `{source.name}_data`\n",
       "\n",
       "* source : name, PointSource, or Simulation\n",
       "* [`config`](/wtlikeconfig) : basic configuration\n",
       "* `source` : PointSource object if specified\n",
       "* `clear` : if set, overwrite the cached results\n",
       "\n",
       "Calculate the values for\n",
       "\n",
       "* S, B : sums of w and 1-w\n",
       "* exptot : total associated exposure"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SourceData.binned_exposure\" class=\"doc_header\"><code>SourceData.binned_exposure</code><a href=\"__main__.py#L119\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SourceData.binned_exposure</code>(**`time_edges`**)\n",
       "\n",
       "Bin the exposure\n",
       "\n",
       "- time_bins: list of edges.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SourceData)\n",
    "show_doc(SourceData.binned_exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-cross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Fri May 28 04:58:30 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-management",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
