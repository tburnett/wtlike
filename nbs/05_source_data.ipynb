{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp source_data\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-retailer",
   "metadata": {},
   "source": [
    "# Source Data management\n",
    "> Extract data for a source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-guest",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Given a point source, the class `SourceData` manages all data-oriented operations, providing all that is necessary to create a set of cells. It depends on the modules\n",
    "\n",
    "* `config` \n",
    "    This must set up the paths to the data created by `data_man`, and define paths for the effective area and weight files\n",
    "\n",
    "* effective_area\n",
    "* weights\n",
    "* exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try: # make conda-build happy?\n",
    "    import healpy\n",
    "except:\n",
    "    pass\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from wtlike.config import *\n",
    "from wtlike.data_man import *\n",
    "from wtlike.effective_area import *\n",
    "from wtlike.weights import *\n",
    "from wtlike.exposure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-signal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters \n",
      "  verbose         : 2\n",
      "  wtlike_data     : /home/burnett/wtlike_data\n",
      "  cachepath       : /home/burnett/wtlike_cache\n",
      "  radius          : 4\n",
      "  cos_theta_max   : 0.4\n",
      "  z_max           : 100\n",
      "  week_range      : (None, None)\n",
      "  time_bins       : (0, 0, 7)\n",
      "  use_uint8       : False\n",
      "  nside           : 1024\n",
      "  nest            : True\n",
      "  bins_per_decade : 5\n",
      "  base_spectrum   : lambda E: (E/1000)**-2.1\n",
      "  energy_range    : (100.0, 1000000.0)\n",
      "  likelihood_rep  : poisson\n",
      "  errors          : []\n",
      "\n",
      "Weekly folder \"/home/burnett/wtlike_data/data_files\" contains 668 weeks,  9 to 677, last week has 1.3 days, ends at 2021-05-21 08:27\n",
      "Source \"BL Lac\" at: (l,b)=(92.590,-10.441)\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "# check the weekly files\n",
    "config = Config(wtlike_data='~/wtlike_data', verbose=2)\n",
    "if config.valid:\n",
    "    ff = get_data_files(config)\n",
    "    source = PointSource('BL Lac') #Geminga')\n",
    "    print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _exposure(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)\n",
    "\n",
    "def _calculate_exposure_for_source(config, source, week):\n",
    "    \"\"\"\n",
    "    Calcualate the exposure for the source during the given week\n",
    "    \"\"\"\n",
    "    df = week['sc_data']\n",
    "    \n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "\n",
    "    return  pd.DataFrame( \n",
    "        dict(\n",
    "            start=df.start[mask], \n",
    "            stop=df.stop[mask], \n",
    "            exp=_exposure(config, livetime, pcosines[mask]),\n",
    "            cos_theta=pcosines[mask],\n",
    "        ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-fight",
   "metadata": {},
   "source": [
    "### Check exposure with last data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFound 3,163 S/C entries:  953 remain after zenith and theta cuts\n",
      "Calculate exposure using the energy domain 100.0-1000000.0 5 bins/decade\n",
      "       start      stop        exp  cos_theta\n",
      "33  59354.03  59354.03   49990.39       0.48\n",
      "34  59354.03  59354.03   71704.66       0.58\n",
      "35  59354.03  59354.03   90833.62       0.66\n",
      "36  59354.03  59354.03  107517.35       0.74\n",
      "37  59354.03  59354.03  122104.60       0.81\n"
     ]
    }
   ],
   "source": [
    "#  hide\n",
    "if config.valid:\n",
    "    filename=ff[-1]\n",
    "    week = pickle.load( open(filename,'rb') )\n",
    "    e_df = _calculate_exposure_for_source(config, source, week); \n",
    "    print(e_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _get_photons_near_source(config, source, week): #tzero, photon_df):\n",
    "    \"\"\"\n",
    "    Select the photons near a source\n",
    "    \n",
    "    - source : a PointSource object\n",
    "    - week : dict with \n",
    "        - tzero : start time for the photon\n",
    "        - photon_df : DataFrame with photon data\n",
    "    \n",
    "    Returns a DF with \n",
    "    - `band` index, \n",
    "    - `time` in MJD (added tstart and converted from MET)\n",
    "    - `pixel` index, nest indexing \n",
    "    - `radius` distance in deg from source direction\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cone(config, source, nest=True):\n",
    "        # cone geometry stuff: get corresponding pixels and center vector\n",
    "        l,b,radius = source.l, source.b, config.radius\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "        conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "        center = healpy.dir2vec(l,b, lonlat=True)\n",
    "        return center, conepix\n",
    "    \n",
    "    center, conepix = _cone(config,source)\n",
    "\n",
    "    df = week['photons']\n",
    "    tstart = week['tstart']\n",
    "    allpix = df.nest_index.values\n",
    "\n",
    "    # select by comparing high-order pixels (faster)\n",
    "    shift=11\n",
    "    a = np.right_shift(allpix, shift)\n",
    "    c = np.unique(np.right_shift(conepix, shift))\n",
    "    incone = np.isin(a,c)\n",
    "    \n",
    "    if sum(incone)<2:\n",
    "        if config.verbose>1:\n",
    "            print(f'\\nWeek at {UTC(MJD(tstart))} has 0 or 1 photons')\n",
    "        return\n",
    "    \n",
    "    if config.verbose>2:\n",
    "        a, b = sum(incone), len(allpix)\n",
    "        print(f'Select photons for source {source.name}:\\n\\tPixel cone cut: select {a} from {b} ({100*a/b:.1f}%)')\n",
    "\n",
    "    # cut df to entries in the cone\n",
    "    dfc = df[incone]\n",
    "\n",
    "    # distance from center for all accepted photons\n",
    "    ll,bb = healpy.pix2ang(config.nside, dfc.nest_index,  nest=True, lonlat=True)\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32))\n",
    "    in_cone = t2<config.radius\n",
    "\n",
    "    if config.verbose>2:\n",
    "        print(f'\\tGeometric cone cut: select {sum(in_cone)}')\n",
    "    # assume all in the GTI (should check)\n",
    "\n",
    "    # times: convert to float, add tstart, convert to MJD\n",
    "    time = MJD(np.array(dfc.time, float)+tstart)\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "        [np.array(dfc.band), time, dfc.nest_index, np.atleast_1d(t2)],\n",
    "        names='band time pixel radius'.split()))[in_cone]\n",
    "\n",
    "    # make sure times are monotonic by sorting (needed for most weeks after March 2018)\n",
    "    out_df = out_df.sort_values(by='time')\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-extraction",
   "metadata": {},
   "source": [
    "### test photon data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  hide\n",
    "# # if config.valid:\n",
    "# for i,f in enumerate(ff[300:]):\n",
    "#     week = pickle.load(open(f,'rb')); \n",
    "#     tstart = week['tstart']\n",
    "#     print(f'{i} ', end='')\n",
    "#     #print(f'TSTART: MET {tstart}, UTC {UTC(MJD(tstart))}')\n",
    "#     p_df = _get_photons_near_source(config, source, week )\n",
    "# #     print(len(p_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export  \n",
    "\n",
    "#     gstart, gstop = keep[0::2], keep[1::2]\n",
    "#     df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "    \n",
    "#     # add column with duration in sec\n",
    "#     df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "#     return df.query(f'duration>{min_duration}')\n",
    "\n",
    "def time_bin_edges(config, exposure, tbin=None):\n",
    "    \"\"\"Return an interleaved array of start/stop values\n",
    "    \n",
    "    tbin: an array (a,b,d), default config.time_bins\n",
    "    \n",
    "    interpretation of a, b:\n",
    "\n",
    "        if > 50000, interpret as MJD\n",
    "        if <0, back from stop\n",
    "        otherwise, offset from start\n",
    "        \n",
    "    d : if positive, the day bin size\n",
    "        if 0; return contiguous bins\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # nominal total range, MJD edges\n",
    "    start = np.round(exposure.start.values[0])\n",
    "    stop =  np.round(exposure.stop.values[-1])\n",
    "\n",
    "    a, b, step = tbin if tbin is not None else config.time_bins\n",
    "    \n",
    "\n",
    "    if a>50000: start=a\n",
    "    elif a<0: start = stop+a\n",
    "    else : start += a\n",
    "\n",
    "\n",
    "    if b>50000: stop=b\n",
    "    elif b>0: stop = start+b\n",
    "    else: stop += b\n",
    "    \n",
    "    if step<=0:\n",
    "        return contiguous_bins(exposure.query(f'{start}<start<{stop}'),)\n",
    "\n",
    "    # adjust stop\n",
    "    nbins = int((stop-start)/step)\n",
    "    assert nbins>0, 'Bad binning: no bins'\n",
    "    stop = start+(nbins)*step\n",
    "    u =  np.linspace(start,stop, nbins+1 )\n",
    "    \n",
    "    # make an interleaved start/stop array\n",
    "    v = np.empty(2*nbins, float)\n",
    "    v[0::2] = u[:-1]\n",
    "    v[1::2] = u[1:]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def contiguous_bins(exposure, min_gap=20, min_duration=600):\n",
    "    \n",
    "    \"\"\" return a start/stop interleaved array for contiguous intervals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    stop = exposure.stop.values\n",
    "    start = exposure.start.values\n",
    "\n",
    "    # interleave  the starts and stops\n",
    "    ssint = np.empty(2*len(start))\n",
    "    ssint[0::2] = start\n",
    "    ssint[1::2] = stop\n",
    "\n",
    "    # Tag the (stpp,start) pairs < 10 sec as  not adjacent\n",
    "    not_adjacent = np.diff(ssint)[1::2] > min_gap/(24*3600) ; \n",
    "    #print(f'{sum(not_adjacent)} (start,stop) pairs are not closer than {min_gap} s')\n",
    "\n",
    "    # make a mask, keep ends\n",
    "    mask = np.empty(2*len(start), bool)\n",
    "    mask[0] = mask[-1] = True\n",
    "    # \n",
    "\n",
    "    # insert into mask -- keep only the (stop,start) pairs  which are not adjacent\n",
    "    mask[1:-2:2] = not_adjacent\n",
    "    mask[2:-1:2] = not_adjacent\n",
    "    \n",
    "    # apply mask, split into start and stop\n",
    "    keep = ssint[mask]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-angle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>exp</th>\n",
       "      <th>cos_theta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>59354.03</td>\n",
       "      <td>59354.03</td>\n",
       "      <td>49990.39</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>59354.03</td>\n",
       "      <td>59354.03</td>\n",
       "      <td>71704.66</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>59354.03</td>\n",
       "      <td>59354.03</td>\n",
       "      <td>90833.62</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>59354.03</td>\n",
       "      <td>59354.03</td>\n",
       "      <td>107517.35</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>59354.03</td>\n",
       "      <td>59354.03</td>\n",
       "      <td>122104.60</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158</th>\n",
       "      <td>59355.35</td>\n",
       "      <td>59355.35</td>\n",
       "      <td>139768.19</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>59355.35</td>\n",
       "      <td>59355.35</td>\n",
       "      <td>140303.71</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>59355.35</td>\n",
       "      <td>59355.35</td>\n",
       "      <td>139789.75</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3161</th>\n",
       "      <td>59355.35</td>\n",
       "      <td>59355.35</td>\n",
       "      <td>139497.31</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>59355.35</td>\n",
       "      <td>59355.35</td>\n",
       "      <td>7206.64</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>953 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         start      stop        exp  cos_theta\n",
       "33    59354.03  59354.03   49990.39       0.48\n",
       "34    59354.03  59354.03   71704.66       0.58\n",
       "35    59354.03  59354.03   90833.62       0.66\n",
       "36    59354.03  59354.03  107517.35       0.74\n",
       "37    59354.03  59354.03  122104.60       0.81\n",
       "...        ...       ...        ...        ...\n",
       "3158  59355.35  59355.35  139768.19       0.95\n",
       "3159  59355.35  59355.35  140303.71       0.96\n",
       "3160  59355.35  59355.35  139789.75       0.95\n",
       "3161  59355.35  59355.35  139497.31       0.95\n",
       "3162  59355.35  59355.35    7206.64       0.95\n",
       "\n",
       "[953 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exposure = e_df; e_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59354. 59355.]\n",
      "[59354.02536787 59354.04709241 59354.08307695 59354.09314565\n",
      " 59354.1562012  59354.17911787 59354.22090107 59354.22541417\n",
      " 59354.28974364 59354.31127065 59354.34252065 59354.34411191\n",
      " 59354.42092416 59354.44349287 59354.47474287 59354.49015954\n",
      " 59354.55300676 59354.5756225  59354.60721972 59354.62219657\n",
      " 59354.6850438  59354.70765954 59354.73925676 59354.75458083\n",
      " 59354.81728917 59354.83951139 59354.87166417 59354.88694194\n",
      " 59354.94969657 59354.9719188 ]\n"
     ]
    }
   ],
   "source": [
    "print(time_bin_edges(config, exposure, (0,0,1)) )\n",
    "print(time_bin_edges(config, exposure, (-1,0,0)))\n",
    "# print(time_bin_edges(config, exposure, (0,0,0.5)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,0,2)) )\n",
    "# print(time_bin_edges(config, exposure, (0,1,0.25)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,-4,0.25)) )   \n",
    "# print(time_bin_edges(config, exposure, (59326, 59330,0.5)) )    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep = contiguous_bins(exposure)\n",
    "# gstart, gstop = keep[0::2], keep[1::2]\n",
    "# df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "\n",
    "# # add column with duration in sec\n",
    "# df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "\n",
    "# df.hist('duration', bins=np.linspace(0,4e3,131));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def binned_exposure(config, exposure, time_edges):\n",
    "    \"\"\"Bin the exposure\n",
    "\n",
    "    - time_bins: list of edges, as an interleaved start/stop array\n",
    "       \n",
    "        \n",
    "    returns  array of exposure integrated over each time bin, times 1e-9\n",
    "    it is interleaved, client must apply [0::2] selection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get exposure calculation\n",
    "    exp   =exposure.exp.values\n",
    "    estart= exposure.start.values\n",
    "    estop = exposure.stop.values\n",
    "\n",
    "    # determine bins,\n",
    "\n",
    "    #use cumulative exposure to integrate over larger periods\n",
    "    cumexp = np.concatenate(([0],np.cumsum(exp)) )\n",
    "\n",
    "    # get index into tstop array of the bin edges\n",
    "    edge_index = np.searchsorted(estop, time_edges)\n",
    "    \n",
    "    # return the exposure integrated over the intervals\n",
    "    cum = cumexp[edge_index]\n",
    "   \n",
    "    # difference is exposure per interval: normalize it here\n",
    "    bexp = np.diff(cum) \n",
    "#     if config.verbose>1:\n",
    "#         print(f'Relative exposure per bin:\\n{pd.Series(bexp).describe(percentiles=[])}')\n",
    "    return bexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _load_from_weekly_data(config, source, week_range=None):\n",
    "    \"\"\"\n",
    "    Generate combinded DataFrames from a list of pickled files\n",
    "    Either weekly or monthly\n",
    "    \n",
    "    kwargs:\n",
    "    - week_range\n",
    "    \"\"\"\n",
    "    \n",
    "    # check weights\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    assert weight_file is not None\n",
    "    \n",
    "    data_folder = config.wtlike_data/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    iname = data_folder.name\n",
    "    \n",
    "    if config.verbose>0:\n",
    "        print(f\"\\tAssembling photon data and exposure for source {source.name} from\"\\\n",
    "              f' folder \"{data_folder}\",\\n\\t with {len(data_files)} files,'\\\n",
    "              f' last file:  {data_files[-1].name}: ', end='')\n",
    "    \n",
    "    w1,w2 = week_range or  config.week_range\n",
    "    if w1 is not None or w2 is not None:\n",
    "        if config.verbose>0:\n",
    "            print(f'\\tLoading weeks {w1}:{w2}')\n",
    "        data_files= data_files[w1:w2]\n",
    "    else:\n",
    "        if config.verbose>0: print('loading all files')\n",
    " \n",
    "    verbose, config.verbose=config.verbose, 0\n",
    "    \n",
    "    # list of data framees\n",
    "    pp = []\n",
    "    ee = []\n",
    "    runs = []\n",
    "    for f in data_files:\n",
    "        print('.', end='')\n",
    "        with open(f, 'rb') as inp:\n",
    "            week = pickle.load(inp)\n",
    "\n",
    "        photons = _get_photons_near_source(config, source, week )\n",
    "        exposure = _calculate_exposure_for_source(config, source, week )\n",
    "        if photons is not None:\n",
    "            add_weights(config, photons, source)\n",
    "            pp.append(photons)\n",
    "            runs.append( add_exposure_to_events(config, exposure, photons,)\n",
    "                       )\n",
    "        ee.append(exposure)\n",
    "        \n",
    "    print('');    \n",
    "    config.verbose=verbose\n",
    "    # concatenate the two lists of DataFrames\n",
    "    p_df = pd.concat(pp, ignore_index=True)\n",
    "    e_df = pd.concat(ee, ignore_index=True)\n",
    "    \n",
    "    # process exposure to find runs, and add tau column to photons\n",
    "    ### runs = add_exposure_to_events(config, e_df, p_df)\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        times = p_df.time.values\n",
    "        print(f'Loaded {len(p_df):,} photons from {UTC(times[0])} to  {UTC(times[-1])} ')\n",
    "        print(f'Calculated {len(e_df):,} exposure entries')\n",
    "        \n",
    "    # add weights to photon data\n",
    "    ### add_weights(config, p_df, source)\n",
    "        \n",
    "    return p_df, e_df, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "#photons, exposure, runs= _load_from_weekly_data(config=Config(), source=PointSource('Geminga'), week_range=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photons(config, source, clear=False, key=''):\n",
    "    \n",
    "    key = f'{source.name}_data' if key=='' else key\n",
    "    if config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "        raise Exception(f'Data for {self.source_name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "\n",
    "    r = config.cache(key, \n",
    "                    _load_from_weekly_data, config, source, \n",
    "                    overwrite=clear,\n",
    "                    description=f'SourceData: photons and exposure for {source.name}')\n",
    "    photons, exposure = r[:2]\n",
    "    runs = r[2] if len(r)==3 else None\n",
    "    # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "    good = np.logical_not(np.isnan(photons.weight))\n",
    "    return photons.loc[good], exposure, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SourceData: photons and exposure for Geminga: Restoring from cache with key \"Geminga_data\"\n"
     ]
    }
   ],
   "source": [
    "src, exp, runs = load_photons(Config(), PointSource('Geminga'), clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-05-21 07:04'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTC(runs.time.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from wtlike.simulation import *\n",
    "class SourceData(object):\n",
    "    \"\"\" Load the photon data near the source and associated exposure. \n",
    "    \n",
    "    Either from:\n",
    "      1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
    "      2. the cache, with key `{source.name}_data`\n",
    "    \n",
    "    * source : name, PointSource, or Simulation\n",
    "    * `config` : basic configuration\n",
    "    * `source` : PointSource object if specified\n",
    "    * `clear` : if set, overwrite the cached results\n",
    "    \n",
    "    Calculate the values for\n",
    "    \n",
    "    * S, B : sums of w and 1-w\n",
    "    * exptot : total associated exposure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source, config=None,  clear=False, \n",
    "                 week_range=None, key=''):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        self.config = config if config else Config()\n",
    "        assert self.config.valid\n",
    "        self.verbose = self.config.verbose\n",
    "        self.simulated=False\n",
    "        \n",
    "        ## source is either a name, a PointSource object, or a Simulation\n",
    "        if type(source)==str:\n",
    "\n",
    "            try:\n",
    "                self.source = PointSource(source) \n",
    "            except Exception as e:\n",
    "                print(f'{e}', file=sys.stderr)\n",
    "                raise\n",
    "\n",
    "            self.source_name = self.source.name\n",
    " \n",
    "        elif isinstance(source, PointSource):\n",
    "            self.source = source # do I need this?\n",
    "            self.source_name = source.name\n",
    "            \n",
    "        elif isinstance(source, Simulation):\n",
    "            self.simulated=True\n",
    "            self.source=None \n",
    "            self.source_name = source.name\n",
    "            # can put this into cache\n",
    "            source.run()\n",
    "            self.photons = source.photons\n",
    "            self.exposure = source.exposure\n",
    "\n",
    "        key = f'{self.source_name}_data' if key=='' else key\n",
    "        #self.source.data_key = key\n",
    "        \n",
    "        if not self.simulated:\n",
    "            # either load from data, or from a chache\n",
    "            if self.config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "                raise Exception(f'Data for {self.source_name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "\n",
    "            r = self.config.cache(key, \n",
    "                            _load_from_weekly_data, self.config, self.source, week_range,\n",
    "                            overwrite=clear,\n",
    "                            description=f'SourceData: photons and exposure for {self.source_name}')\n",
    "            photons, self.exposure = r[:2]\n",
    "            self.runs = r[2] if len(r)==3 else None\n",
    "            # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "            good = np.logical_not(np.isnan(photons.weight))\n",
    "            self.photons = photons.loc[good]\n",
    "\n",
    "        else: #TODO\n",
    "            pass\n",
    "        \n",
    "        # make range of MJD or days available\n",
    "        self.start = self.exposure.start[0]\n",
    "        self.stop =  self.exposure.stop.values[-1]\n",
    "        self.exptot = self.exposure.exp.sum() \n",
    "\n",
    "        # estimates for signal and background counts in total exposure\n",
    "        w = self.photons.weight\n",
    "        self.S = np.sum(w)\n",
    "        self.B = np.sum(1-w)\n",
    "        \n",
    "        if self.verbose>0:\n",
    "            print(SourceData.__repr__(self))\n",
    "    \n",
    "    def rates(self):\n",
    "        print(f'Average fluxes for {self.source_name}: signal {self.S/self.exptot:.2e}/s, background {self.B/self.exptot:.2e}/s')\n",
    "\n",
    "    def __repr__(self):\n",
    "        time = self.photons.time.values\n",
    "        \n",
    "        exp = self.exposure\n",
    "        days  = np.sum(exp.stop-exp.start); secs = days*24*3600\n",
    "        exp_text = f' average flux {self.exptot/secs:.0f} cm^2 for {secs/1e6:.1f} Ms'\n",
    "        \n",
    "        if not self.simulated:\n",
    "            photon_text = f'photons from {UTC(time[0])[:10]} to {UTC(time[-1])[:10]}'\n",
    "        else:\n",
    "            photon_text = f'simulated photons over {days:.1f} days.'\n",
    "\n",
    "        r = f'SourceData: Source {self.source_name} with:'\\\n",
    "            f'\\n\\t data:     {len(self.photons):9,} {photon_text}'\\\n",
    "            f'\\n\\t exposure: {len(self.exposure):9,} intervals, {exp_text}'\n",
    " \n",
    "        self.src_flux, self.bkg_flux = self.S/self.exptot,  self.B/self.exptot\n",
    "        r+= f'\\n\\t rates:  source {self.src_flux:.2e}/s, background {self.bkg_flux:.2e}/s,'\\\n",
    "            f' S/N ratio {self.src_flux/self.bkg_flux:.2e}'\n",
    "\n",
    "        return r\n",
    "    \n",
    "    def binned_exposure(self, time_edges):\n",
    "        \"\"\"Bin the exposure\n",
    "        \n",
    "        - time_bins: list of edges.  \n",
    "        \"\"\"\n",
    "        return binned_exposure(self.config, self.exposure,  time_edges)\n",
    "    \n",
    "    def binned_cos_theta(self, time_bins=None):\n",
    "        \"\"\" Calculate average cosine of angle with respect to bore axis, per time bin\n",
    "        \"\"\"\n",
    "        if time_bins is None:\n",
    "            time_bins = get_default_bins(self.config, self.exposure)\n",
    "        df = self.exposure.copy()\n",
    "        estop =df.stop.values\n",
    "        df.loc[:,'tbin'] =np.digitize(estop, time_bins)\n",
    "        ct = df.groupby('tbin').mean()['cos_theta']\n",
    "        return ct, time_bins\n",
    "    \n",
    "    def weight_histogram(self, nbins=1000, key=''):\n",
    "        \"\"\" return a weight distribution\n",
    "        \"\"\"\n",
    "        def doit(nbins):\n",
    "            return np.histogram(self.p_df.weight.values, np.linspace(0,1,nbins+1))[0]\n",
    "\n",
    "        key = f'{self.source_name}_weight_hist' if key=='' else key\n",
    "        description = f'Weight histogram for {self.source_name}' if self.config.verbose>0 else ''\n",
    "        return self.config.cache(key, doit, nbins, description=description)\n",
    "        \n",
    "    def plot_data(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if self.simulated:\n",
    "            print(f'Simulated!')\n",
    "            fig, (ax1, ax4) = plt.subplots(1,2, figsize=(8,4))\n",
    "            ax1.hist(self.photons.time.values, 500, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');\n",
    "\n",
    "            \n",
    "        else:\n",
    "            fig, (ax1,ax2, ax3,ax4) = plt.subplots(1,4, figsize=(15,4))\n",
    "            ax1.hist(self.photons.time.values, 100, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "            ax2.hist(self.photons.radius.values**2, 100, histtype='step', log=True);\n",
    "            ax2.set(xlabel='Radius**2 (deg**2)', ylim=(100, None));\n",
    "\n",
    "            ax3.hist(self.photons.band, 32, histtype='step', log=True);\n",
    "            ax3.set(xlabel='Band index')\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-rough",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'src_flux'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c813e4c66d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#hide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_sim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_flux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msimsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSourceData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'src_flux'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "sim = Simulation('test_sim', src_flux=1e-6, tstart=0, tstop=1, )\n",
    "simsd = SourceData(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#sd = SourceData('3C 279', week_range=(500,None), key=None)\n",
    "sd = SourceData('Geminga');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SourceData)\n",
    "show_doc(SourceData.binned_exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-london",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
