{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *\n",
    "# default_exp source_data\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-aspect",
   "metadata": {},
   "source": [
    "# Source Data management\n",
    "> Extract data for a source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-commitment",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Given a point source, the class `SourceData` manages all data-oriented operations, providing all that is necessary to create a set of cells. It depends on the modules\n",
    "\n",
    "* `config` \n",
    "    This must set up the paths to the data created by `data_man`, and define paths for the effective area and weight files\n",
    "\n",
    "* effective_area\n",
    "* weights\n",
    "* exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try: # make conda-build happy?\n",
    "    import healpy\n",
    "except:\n",
    "    pass\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from wtlike.config import *\n",
    "from wtlike.data_man import *\n",
    "from wtlike.effective_area import *\n",
    "from wtlike.weights import *\n",
    "from wtlike.exposure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _exposure(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effectivee area function from\n",
    "    ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "\n",
    "    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "    return (aeff*livetime)\n",
    "\n",
    "def _calculate_exposure_for_source(config, source, week):\n",
    "    \"\"\"\n",
    "    Calcualate the exposure for the source during the given week\n",
    "    \"\"\"\n",
    "    df = week['sc_data']\n",
    "    \n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "\n",
    "    return  pd.DataFrame( \n",
    "        dict(\n",
    "            start=df.start[mask], \n",
    "            stop=df.stop[mask], \n",
    "            exp=_exposure(config, livetime, pcosines[mask]),\n",
    "            cos_theta=pcosines[mask],\n",
    "        ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-plenty",
   "metadata": {},
   "source": [
    "### Check exposure with last data file, and our weak source\n",
    "\n",
    "Adapt to determine exposure record for each band."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-scholar",
   "metadata": {},
   "source": [
    "### Load the weight info for the source, get the \n",
    "* energies\n",
    "* spectral function\n",
    "\n",
    "And for each band:\n",
    "The id is 2 x energy index + front(0), back(1)\n",
    "* npred, \n",
    "* flux (which should be func(energy))\n",
    "\n",
    "For each s/c entry, typically 30-s intervals\n",
    "* livetime\n",
    "* pcosine\n",
    "\n",
    "The effective area function, ` ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')`\n",
    "is evaluated `ea(energy, pcosine)`, and returns an effective area (front, back) tuple in $\\mathrm{cm^2}$. This is then multiplied by the livetime for that interval.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-north",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters \n",
      "  verbose         : 2\n",
      "  wtlike_data     : /home/burnett/wtlike_data\n",
      "  cachepath       : /home/burnett/wtlike_cache\n",
      "  radius          : 4\n",
      "  cos_theta_max   : 0.4\n",
      "  z_max           : 100\n",
      "  week_range      : (None, None)\n",
      "  time_bins       : (0, 0, 7)\n",
      "  use_uint8       : False\n",
      "  nside           : 1024\n",
      "  nest            : True\n",
      "  bins_per_decade : 5\n",
      "  base_spectrum   : lambda E: (E/1000)**-2.1\n",
      "  energy_range    : (100.0, 1000000.0)\n",
      "  likelihood_rep  : poisson\n",
      "  errors          : []\n",
      "\n",
      "Weekly folder \"/home/burnett/wtlike_data/data_files\" contains 669 weeks,  9 to 678, last week has 3.2 days, ends at 2021-05-30 04:54\n",
      "Source \"4FGL J1257.0-6339\" at: (l,b)=(303.549,-0.756)\n",
      "WeightMan: file from source \"{source.filename}\"_weights.pkl : new format, 16 bamds with nsides 64 to 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.3290538257419684e-12, 6.754116888198375e-09, 0.023808757766803926)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  hide\n",
    "# check the weekly files\n",
    "config = Config(wtlike_data='~/wtlike_data', verbose=2)\n",
    "if config.valid:\n",
    "    ff = get_data_files(config)\n",
    "    source = PointSource('4FGL J1257.0-6339') #'BL Lac') #Geminga')\n",
    "    print(source)\n",
    "\n",
    "wm = WeightMan(config, source=source)\n",
    "\n",
    "[ (k,round(b['npred'],1), np.float32(b['flux'])) for k,b in wm.wt_dict.items()]\n",
    "\n",
    "energies = np.sqrt(wm.energy_bins[1:]*wm.energy_bins[:-1]).astype(np.float32); energies\n",
    "from scipy.integrate import quad\n",
    "class FluxModel():\n",
    "\n",
    "    emin, emax = 1e2, 1e5\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def count_flux(self):\n",
    "        return quad(self, self.emin, self.emax)[0]\n",
    "    \n",
    "    def energy_flux(self):\n",
    "        func = lambda e: self(e) * e**2\n",
    "        return quad(func, self.emin, self.emax)[0]\n",
    "    \n",
    "class LogParabola(FluxModel):\n",
    "    def __init__(self, pars):\n",
    "        self.pars=pars\n",
    "    def __call__(self, e):\n",
    "        n0,alpha,beta,e_break=self.pars\n",
    "        x = np.log(e_break/e)\n",
    "        y = (alpha - beta*x)*x\n",
    "        return n0*np.exp(y)\n",
    "    \n",
    "lp = LogParabola(wm.fitinfo['pars'])\n",
    "lp(1333), lp.count_flux(), lp.energy_flux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# examine energy dependence\n",
    "def test_exp(week):\n",
    "    df = week['sc_data']\n",
    "    \n",
    "    # calculate cosines with respect to sky direction\n",
    "    sc = source\n",
    "    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)\n",
    "    sdec, cdec = np.sin(dec_r), np.cos(dec_r)\n",
    "\n",
    "    def cosines( ra2, dec2):\n",
    "        ra2_r =  np.radians(ra2.values)\n",
    "        dec2_r = np.radians(dec2.values)\n",
    "        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec\n",
    "\n",
    "    pcosines = cosines(df.ra_scz,    df.dec_scz)\n",
    "    zcosines = cosines(df.ra_zenith, df.dec_zenith)\n",
    "    # mask out entries too close to zenith, or too far away from ROI center\n",
    "    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))\n",
    "    if config.verbose>1:\n",
    "        print(f'\\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')\n",
    "    dfm = df.loc[mask,:]\n",
    "    livetime = dfm.livetime.values\n",
    "    return livetime, pcosines[mask]\n",
    "\n",
    "def test_exp2(config,  livetime, pcosine):\n",
    "    \"\"\"return exposure calculated for each pair in livetime and cosines arrays\n",
    "\n",
    "    uses effective area\n",
    "    \"\"\"\n",
    "    from scipy.integrate import simps\n",
    "    assert len(livetime)==len(pcosine), 'expect equal-length arrays'\n",
    "\n",
    "    # get a set of energies and associated weights from a trial spectrum\n",
    "\n",
    "    emin,emax = config.energy_range\n",
    "    loge1=np.log10(emin); loge2=np.log10(emax)\n",
    "\n",
    "    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))\n",
    "    if config.verbose>1:\n",
    "        print(f'Calculate exposure using the energy domain'\\\n",
    "              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )\n",
    "    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1\n",
    "    assert base_spectrum(1000)==1.\n",
    "    wts = base_spectrum(edom)\n",
    "\n",
    "    # effective area function from\n",
    "    ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')\n",
    "\n",
    "    # a table of the weighted for each pair in livetime and pcosine arrays\n",
    "    rvals = np.empty([len(wts),len(pcosine)])\n",
    "    for i,(en,wt) in enumerate(zip(edom,wts)):\n",
    "        faeff,baeff = ea([en],pcosine)\n",
    "        rvals[i] = (faeff+baeff)*wt\n",
    "    return dict(rvals=rvals, wts=wts, edom=edom)\n",
    "    \n",
    "#     aeff = simps(rvals,edom,axis=0)/simps(wts,edom)\n",
    "#     return (aeff*livetime)\n",
    "\n",
    "lt, pc = test_exp(week)\n",
    "\n",
    "# effective area function from\n",
    "ea = EffectiveArea(file_path=config.wtlike_data/'aeff_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  hide\n",
    "# if config.valid:\n",
    "#     filename=ff[-1]\n",
    "#     week = pickle.load( open(filename,'rb') )\n",
    "#     e_df = _calculate_exposure_for_source(config, source, week); \n",
    "#     print(e_df.head())\n",
    "\n",
    "# f,b = ea(energies[0], pc[:50])\n",
    "# f, lt[:50]\n",
    "\n",
    "# plt.rc('font', size=12)\n",
    "# fig, (ax1,ax2,) = plt.subplots(1,2, figsize=(12,4))\n",
    "# plt.subplots_adjust(wspace=0.3)\n",
    "# ax1.plot(lt.clip(24.5,50), pc,'.')\n",
    "# ax1.set(xlabel='livetime (s)', ylabel='cosine')\n",
    "\n",
    "\n",
    "# ax2.loglog( d['edom'],d['wts'] ,'.')\n",
    "# ax2.grid()\n",
    "# ax2.set(xlabel='energy',xlim=(100, 1e5), ylabel='spectral weight');\n",
    "# fig2,ax3 = plt.subplots(figsize=(15,4))\n",
    "# rvals = d['rvals']\n",
    "# ax3.imshow(np.log10(rvals[:,:250]), cmap='hot', interpolation='nearest')\n",
    "# ax3.set(xlabel='30-s bin', ylabel='band', title='Exposure')\n",
    "# rvals[:,:25].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _get_photons_near_source(config, source, week): #tzero, photon_df):\n",
    "    \"\"\"\n",
    "    Select the photons near a source\n",
    "    \n",
    "    - source : a PointSource object\n",
    "    - week : dict with \n",
    "        - tzero : start time for the photon\n",
    "        - photon_df : DataFrame with photon data\n",
    "    \n",
    "    Returns a DF with \n",
    "    - `band` index, \n",
    "    - `time` in MJD (added tstart and converted from MET)\n",
    "    - `pixel` index, nest indexing \n",
    "    - `radius` distance in deg from source direction\n",
    "    \"\"\"\n",
    "    \n",
    "    def _cone(config, source, nest=True):\n",
    "        # cone geometry stuff: get corresponding pixels and center vector\n",
    "        l,b,radius = source.l, source.b, config.radius\n",
    "        cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "        conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "        center = healpy.dir2vec(l,b, lonlat=True)\n",
    "        return center, conepix\n",
    "    \n",
    "    center, conepix = _cone(config,source)\n",
    "\n",
    "    df = week['photons']\n",
    "    tstart = week['tstart']\n",
    "    allpix = df.nest_index.values\n",
    "\n",
    "    # select by comparing high-order pixels (faster)\n",
    "    shift=11\n",
    "    a = np.right_shift(allpix, shift)\n",
    "    c = np.unique(np.right_shift(conepix, shift))\n",
    "    incone = np.isin(a,c)\n",
    "    \n",
    "    if sum(incone)<2:\n",
    "        if config.verbose>1:\n",
    "            print(f'\\nWeek at {UTC(MJD(tstart))} has 0 or 1 photons')\n",
    "        return\n",
    "    \n",
    "    if config.verbose>2:\n",
    "        a, b = sum(incone), len(allpix)\n",
    "        print(f'Select photons for source {source.name}:\\n\\tPixel cone cut: select {a} from {b} ({100*a/b:.1f}%)')\n",
    "\n",
    "    # cut df to entries in the cone\n",
    "    dfc = df[incone]\n",
    "\n",
    "    # distance from center for all accepted photons\n",
    "    ll,bb = healpy.pix2ang(config.nside, dfc.nest_index,  nest=True, lonlat=True)\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32))\n",
    "    in_cone = t2<config.radius\n",
    "\n",
    "    if config.verbose>2:\n",
    "        print(f'\\tGeometric cone cut: select {sum(in_cone)}')\n",
    "    # assume all in the GTI (should check)\n",
    "\n",
    "    # times: convert to float, add tstart, convert to MJD\n",
    "    time = MJD(np.array(dfc.time, float)+tstart)\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "        [np.array(dfc.band), time, dfc.nest_index, np.atleast_1d(t2)],\n",
    "        names='band time pixel radius'.split()))[in_cone]\n",
    "\n",
    "    # make sure times are monotonic by sorting (needed for most weeks after March 2018)\n",
    "    out_df = out_df.sort_values(by='time')\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-edgar",
   "metadata": {},
   "source": [
    "### test photon data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  hide\n",
    "# # if config.valid:\n",
    "# for i,f in enumerate(ff[300:]):\n",
    "#     week = pickle.load(open(f,'rb')); \n",
    "#     tstart = week['tstart']\n",
    "#     print(f'{i} ', end='')\n",
    "#     #print(f'TSTART: MET {tstart}, UTC {UTC(MJD(tstart))}')\n",
    "#     p_df = _get_photons_near_source(config, source, week )\n",
    "# #     print(len(p_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-suspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     gstart, gstop = keep[0::2], keep[1::2]\n",
    "#     df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "    \n",
    "#     # add column with duration in sec\n",
    "#     df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "#     return df.query(f'duration>{min_duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export  \n",
    "def time_bin_edges(config, exposure, tbin=None):\n",
    "    \"\"\"Return an interleaved array of start/stop values\n",
    "    \n",
    "    tbin: an array (a,b,d), default config.time_bins\n",
    "    \n",
    "    interpretation of a, b:\n",
    "\n",
    "        if > 50000, interpret as MJD\n",
    "        if <0, back from stop\n",
    "        otherwise, offset from start\n",
    "        \n",
    "    d : if positive, the day bin size\n",
    "        if 0; return contiguous bins\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    # nominal total range, MJD edges\n",
    "    start = np.round(exposure.start.values[0])\n",
    "    stop =  np.round(exposure.stop.values[-1])\n",
    "\n",
    "    a, b, step = tbin if tbin is not None else config.time_bins\n",
    "    \n",
    "\n",
    "    if a>50000: start=a\n",
    "    elif a<0: start = stop+a\n",
    "    else : start += a\n",
    "\n",
    "\n",
    "    if b>50000: stop=b\n",
    "    elif b>0: stop = start+b\n",
    "    else: stop += b\n",
    "    \n",
    "    if step<=0:\n",
    "        return contiguous_bins(exposure.query(f'{start}<start<{stop}'),)\n",
    "\n",
    "    # adjust stop\n",
    "    nbins = int((stop-start)/step)\n",
    "    assert nbins>0, 'Bad binning: no bins'\n",
    "    stop = start+(nbins)*step\n",
    "    u =  np.linspace(start,stop, nbins+1 )\n",
    "    \n",
    "    # make an interleaved start/stop array\n",
    "    v = np.empty(2*nbins, float)\n",
    "    v[0::2] = u[:-1]\n",
    "    v[1::2] = u[1:]\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def contiguous_bins(exposure, min_gap=20, min_duration=600):\n",
    "    \n",
    "    \"\"\" return a start/stop interleaved array for contiguous intervals\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    stop = exposure.stop.values\n",
    "    start = exposure.start.values\n",
    "\n",
    "    # interleave  the starts and stops\n",
    "    ssint = np.empty(2*len(start))\n",
    "    ssint[0::2] = start\n",
    "    ssint[1::2] = stop\n",
    "\n",
    "    # Tag the (stpp,start) pairs < 10 sec as  not adjacent\n",
    "    not_adjacent = np.diff(ssint)[1::2] > min_gap/(24*3600) ; \n",
    "    #print(f'{sum(not_adjacent)} (start,stop) pairs are not closer than {min_gap} s')\n",
    "\n",
    "    # make a mask, keep ends\n",
    "    mask = np.empty(2*len(start), bool)\n",
    "    mask[0] = mask[-1] = True\n",
    "    # \n",
    "\n",
    "    # insert into mask -- keep only the (stop,start) pairs  which are not adjacent\n",
    "    mask[1:-2:2] = not_adjacent\n",
    "    mask[2:-1:2] = not_adjacent\n",
    "    \n",
    "    # apply mask, split into start and stop\n",
    "    keep = ssint[mask]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exposure = e_df; e_df\n",
    "\n",
    "# print(time_bin_edges(config, exposure, (0,0,1)) )\n",
    "# print(time_bin_edges(config, exposure, (-1,0,0)))\n",
    "# print(time_bin_edges(config, exposure, (0,0,0.5)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,0,2)) )\n",
    "# print(time_bin_edges(config, exposure, (0,1,0.25)) )\n",
    "# print(time_bin_edges(config, exposure, (-5,-4,0.25)) )   \n",
    "# print(time_bin_edges(config, exposure, (59326, 59330,0.5)) )    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep = contiguous_bins(exposure)\n",
    "# gstart, gstop = keep[0::2], keep[1::2]\n",
    "# df =  pd.DataFrame.from_dict(dict(start=gstart, stop=gstop))\n",
    "\n",
    "# # add column with duration in sec\n",
    "# df.loc[:,'duration'] = (df.stop-df.start)*24*3600\n",
    "\n",
    "# df.hist('duration', bins=np.linspace(0,4e3,131));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def binned_exposure(config, exposure, time_edges):\n",
    "    \"\"\"Bin the exposure\n",
    "\n",
    "    - time_bins: list of edges, as an interleaved start/stop array\n",
    "       \n",
    "        \n",
    "    returns  array of exposure integrated over each time bin, times 1e-9\n",
    "    it is interleaved, client must apply [0::2] selection.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get exposure calculation\n",
    "    exp   =exposure.exp.values\n",
    "    estart= exposure.start.values\n",
    "    estop = exposure.stop.values\n",
    "\n",
    "    # determine bins,\n",
    "\n",
    "    #use cumulative exposure to integrate over larger periods\n",
    "    cumexp = np.concatenate(([0],np.cumsum(exp)) )\n",
    "\n",
    "    # get index into tstop array of the bin edges\n",
    "    edge_index = np.searchsorted(estop, time_edges)\n",
    "    \n",
    "    # return the exposure integrated over the intervals\n",
    "    cum = cumexp[edge_index]\n",
    "   \n",
    "    # difference is exposure per interval: normalize it here\n",
    "    bexp = np.diff(cum) \n",
    "#     if config.verbose>1:\n",
    "#         print(f'Relative exposure per bin:\\n{pd.Series(bexp).describe(percentiles=[])}')\n",
    "    return bexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _load_from_weekly_data(config, source, appendto=None, week_range=None):\n",
    "    \"\"\"\n",
    "    Generate combinded DataFrames from a list of weekly pickled files\n",
    "    * config\n",
    "    * source -- a PointSource object\n",
    "    * appendto [None] -- previous data,  a tuple (photons, exposure, runs) to append to\n",
    "    * week_range [None] -- Slice tuple (start, stop, step) selects week indeces, \n",
    "            so (-1,None) is last.\n",
    "    \"\"\"\n",
    "    \n",
    "    # check weights\n",
    "    weight_file =  check_weights(config,  source)\n",
    "    assert weight_file is not None\n",
    "    \n",
    "    data_folder = config.wtlike_data/'data_files'\n",
    "    data_files = sorted(list(data_folder.glob('*.pkl')))\n",
    "    iname = data_folder.name\n",
    "    \n",
    "    if config.verbose>0:\n",
    "        print(f\"\\tAssembling photon data and exposure for source {source.name}\\n\\tfrom\"\\\n",
    "              f' folder \"{data_folder}\", with {len(data_files)} files,'\\\n",
    "              f'\\n\\tWeights from file {source.filename}_weights.pkl')\n",
    "\n",
    "    \n",
    "    weeks = week_range or  config.week_range\n",
    "    if week_range is not None:\n",
    "        slc = slice(*week_range)\n",
    "        if config.verbose>0:\n",
    "            print(f'\\tLoading weeks {slc}', end='')\n",
    "        data_files= data_files[slc]\n",
    "    else:\n",
    "        if config.verbose>0: print('loading all files', end='')\n",
    " \n",
    "    verbose, config.verbose=config.verbose, 0\n",
    "    \n",
    "    # list of data framees\n",
    "    pp = []\n",
    "    ee = []\n",
    "    runs = []\n",
    "    for f in data_files:\n",
    "        print('.', end='')\n",
    "        with open(f, 'rb') as inp:\n",
    "            week = pickle.load(inp)\n",
    "\n",
    "        photons = _get_photons_near_source(config, source, week )\n",
    "        exposure = _calculate_exposure_for_source(config, source, week )\n",
    "        if photons is not None and len(photons)>2:\n",
    "            add_weights(config, photons, source)\n",
    "            pp.append(photons)\n",
    "            runs.append( add_exposure_to_events(config, exposure, photons,)\n",
    "                       )\n",
    "        if len(exposure)>0:\n",
    "            ee.append(exposure)\n",
    "        \n",
    "    print('');    \n",
    "    config.verbose=verbose\n",
    "    # concatenate the two lists of DataFrames\n",
    "    p_df = pd.concat(pp, ignore_index=True)\n",
    "    e_df = pd.concat(ee, ignore_index=True)\n",
    "    \n",
    "    # process exposure to find runs, and add tau column to photons\n",
    "    ### runs = add_exposure_to_events(config, e_df, p_df)\n",
    "    \n",
    "    if config.verbose>1:\n",
    "        times = p_df.time.values\n",
    "        print(f'Loaded {len(p_df):,} photons from {UTC(times[0])} to  {UTC(times[-1])} ')\n",
    "        print(f'Calculated {len(e_df):,} exposure entries')\n",
    "        \n",
    "    # add weights to photon data\n",
    "    ### add_weights(config, p_df, source)\n",
    "        \n",
    "    return p_df, e_df, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_photons(config, source, clear=False, key=''):\n",
    "    \n",
    "    key = f'{source.name}_data' if key=='' else key\n",
    "    if config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "        raise Exception(f'Data for {self.source_name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "\n",
    "    r = config.cache(key, \n",
    "                    _load_from_weekly_data, config, source, \n",
    "                    overwrite=clear,\n",
    "                    description=f'SourceData: photons and exposure for {source.name}')\n",
    "    photons, exposure = r[:2]\n",
    "    runs = r[2] if len(r)==3 else None\n",
    "    # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "    good = np.logical_not(np.isnan(photons.weight))\n",
    "    return photons.loc[good], exposure, runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_cache(self, week_range=(-1,None), save=True):\n",
    "    \n",
    "    config=self.config\n",
    "    source = self.source\n",
    "    \n",
    "    config.verbose, vsave=(2, config.verbose)\n",
    "    key = f'{source.filename}_data'\n",
    "    \n",
    "    print(f'Retrieve photon data from cache {key}')\n",
    "    cur_ph, cur_exp, cur_runs = config.cache.get(key)\n",
    "    #cur_ph, cur_exp, cur_runs = current =  load_photons(config, source, clear=False)\n",
    "    \n",
    "    new_ph, new_exp, new_runs = tomerge = _load_from_weekly_data(config, source, week_range=(-1,None))\n",
    "    \n",
    "    tlast = cur_ph.iloc[-1].time \n",
    "    ph_to_append = new_ph[new_ph.time>tlast]\n",
    "    \n",
    "    if len(ph_to_append)==0:\n",
    "        print('Up to date')\n",
    "        config.verbose=vsave\n",
    "        return\n",
    "    print(f'photons:  {tlast:.3f} -> {to_append.iloc[0].time:.3f}')\n",
    "    p_df = pd.concat([cur_ph, ph_to_append])\n",
    "    \n",
    "    tlast = cur_exp.iloc[-1].stop\n",
    "    exp_to_append = new_exp[new_exp.start>tlast]\n",
    "    print(f'exposure: {tlast:.3f} -> {exp_to_append.iloc[0].start:.3f}')\n",
    "    e_df  = pd.concat([cur_exp, exp_to_append])\n",
    "    \n",
    "    tlast = cur_runs[-1].iloc[-1].time\n",
    "    print(f'runs:     {tlast:.3f} -> {new_runs[0].iloc[0].time:.3f}')\n",
    "    runs = cur_runs.append(new_runs)\n",
    "    \n",
    "    if save:\n",
    "        print('Saving back')\n",
    "        config.cache.add(key, (p_df, e_df, runs), exist_ok=True)\n",
    "    else: print('not saving')\n",
    "    config.vebose=vsave\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=Config()\n",
    "config.verbose=2\n",
    "source = PointSource('PSR B1259-63') #'4FGL J1257.0-6339')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-transmission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve photon data from cache PSR_B1259-63_data\n",
      "\tAssembling photon data and exposure for source PSR B1259-63\n",
      "\tfrom folder \"/home/burnett/wtlike_data/data_files\",\n",
      "\t with 669 files,\n",
      "\tWeights from file PSR_B1259-63_weights.pkl\n",
      "\tLoading weeks slice(-1, None, None).\n",
      "Loaded 965 photons from 2021-05-27 00:27 to  2021-05-30 04:41 \n",
      "Calculated 1,846 exposure entries\n",
      "Up to date\n"
     ]
    }
   ],
   "source": [
    "update_cache(config, source, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-tennessee",
   "metadata": {},
   "source": [
    "key = f'{source.name}_data'\n",
    "cur_ph, cur_exp, cur_runs = config.cache.get(key)\n",
    "\n",
    "tlast = cur_ph.iloc[-1].time \n",
    "to_append = new_ph[new_ph.time>tlast]\n",
    "print(tlast, to_append.iloc[0].time)\n",
    "\n",
    "tlast = cur_exp.iloc[-1].stop\n",
    "exp_to_append = new_exp[new_exp.start>tlast]\n",
    "print(tlast, exp_to_append.iloc[0].start)\n",
    "\n",
    "exp.info(), src.info();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-valentine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from wtlike.simulation import *\n",
    "class SourceData(object):\n",
    "    \"\"\" Load the photon data near the source and associated exposure. \n",
    "    \n",
    "    Either from:\n",
    "      1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
    "      2. the cache, with key `{source.name}_data`\n",
    "    \n",
    "    * source : name, PointSource, or Simulation\n",
    "    * `config` : basic configuration\n",
    "    * `source` : PointSource object if specified\n",
    "    * `clear` : if set, overwrite the cached results\n",
    "    \n",
    "    Calculate the values for\n",
    "    \n",
    "    * S, B : sums of w and 1-w\n",
    "    * exptot : total associated exposure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source, config=None,  clear=False, \n",
    "                 week_range=None, key=''):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "            \n",
    "        self.config = config if config else Config()\n",
    "        assert self.config.valid\n",
    "        self.verbose = self.config.verbose\n",
    "        self.simulated=False\n",
    "        \n",
    "        ## source is either a name, a PointSource object, or a Simulation\n",
    "        if type(source)==str:\n",
    "\n",
    "            try:\n",
    "                self.source = PointSource(source) \n",
    "            except Exception as e:\n",
    "                print(f'{e}', file=sys.stderr)\n",
    "                raise\n",
    "\n",
    "            self.source_name = self.source.name\n",
    " \n",
    "        elif isinstance(source, PointSource):\n",
    "            self.source = source # do I need this?\n",
    "            self.source_name = source.name\n",
    "            \n",
    "        elif isinstance(source, Simulation):\n",
    "            self.simulated=True\n",
    "            self.source=None \n",
    "            self.source_name = source.name\n",
    "            # can put this into cache\n",
    "            source.run()\n",
    "            self.photons = source.photons\n",
    "            self.exposure = source.exposure\n",
    "\n",
    "        if self.source is not None:            \n",
    "            key = f'{self.source.filename}_data' if key=='' else key\n",
    "            self.source.data_key = key\n",
    "        else: # no cache for sim, yet\n",
    "            key=None \n",
    "        \n",
    "        \n",
    "        if not self.simulated:\n",
    "            # either load from data, or from a chache\n",
    "            if self.config.wtlike_data/'data_files' is None and key not in config.cache:\n",
    "                raise Exception(f'Data for {self.source_name} is not cached, and config.wtlike_data/\"data_files\" is not set')\n",
    "\n",
    "            r = self.config.cache(key, \n",
    "                            _load_from_weekly_data, self.config, self.source, week_range,\n",
    "                            overwrite=clear,\n",
    "                            description=f'SourceData: photons and exposure for {self.source_name}')\n",
    "            photons, self.exposure = r[:2]\n",
    "            self.runs = r[2] if len(r)==3 else None\n",
    "            # get the photon data with good weights, not NaN (maybe remove small weigts, too)\n",
    "            good = np.logical_not(np.isnan(photons.weight))\n",
    "            self.photons = photons.loc[good]\n",
    "\n",
    "        else: #TODO\n",
    "            pass\n",
    "        \n",
    "        # make range of MJD or days available\n",
    "        self.start = self.exposure.start[0]\n",
    "        self.stop =  self.exposure.stop.values[-1]\n",
    "        self.exptot = self.exposure.exp.sum() \n",
    "\n",
    "        # estimates for signal and background counts in total exposure\n",
    "        w = self.photons.weight\n",
    "        self.S = np.sum(w)\n",
    "        self.B = np.sum(1-w)\n",
    "        \n",
    "        if self.verbose>0:\n",
    "            print(SourceData.__repr__(self))\n",
    "    \n",
    "    def rates(self):\n",
    "        print(f'Average fluxes for {self.source_name}: signal {self.S/self.exptot:.2e}/s, background {self.B/self.exptot:.2e}/s')\n",
    "\n",
    "    def __repr__(self):\n",
    "        time = self.photons.time.values\n",
    "        \n",
    "        exp = self.exposure\n",
    "        days  = np.sum(exp.stop-exp.start); secs = days*24*3600\n",
    "        exp_text = f' average flux {self.exptot/secs:.0f} cm^2 for {secs/1e6:.1f} Ms'\n",
    "        \n",
    "        if not self.simulated:\n",
    "            photon_text = f'photons from {UTC(time[0])[:10]} to {UTC(time[-1])[:10]}'\n",
    "        else:\n",
    "            photon_text = f'simulated photons over {days:.1f} days.'\n",
    "\n",
    "        r = f'SourceData: Source {self.source_name} with:'\\\n",
    "            f'\\n\\t data:     {len(self.photons):9,} {photon_text}'\\\n",
    "            f'\\n\\t exposure: {len(self.exposure):9,} intervals, {exp_text}'\n",
    " \n",
    "        self.src_flux, self.bkg_flux = self.S/self.exptot,  self.B/self.exptot\n",
    "        r+= f'\\n\\t rates:  source {self.src_flux:.2e}/s, background {self.bkg_flux:.2e}/s,'\\\n",
    "            f' S/N ratio {self.src_flux/self.bkg_flux:.2e}'\n",
    "\n",
    "        return r\n",
    "    \n",
    "    def binned_exposure(self, time_edges):\n",
    "        \"\"\"Bin the exposure\n",
    "        \n",
    "        - time_bins: list of edges.  \n",
    "        \"\"\"\n",
    "        return binned_exposure(self.config, self.exposure,  time_edges)\n",
    "    \n",
    "    def binned_cos_theta(self, time_bins=None):\n",
    "        \"\"\" Calculate average cosine of angle with respect to bore axis, per time bin\n",
    "        \"\"\"\n",
    "        if time_bins is None:\n",
    "            time_bins = get_default_bins(self.config, self.exposure)\n",
    "        df = self.exposure.copy()\n",
    "        estop =df.stop.values\n",
    "        df.loc[:,'tbin'] =np.digitize(estop, time_bins)\n",
    "        ct = df.groupby('tbin').mean()['cos_theta']\n",
    "        return ct, time_bins\n",
    "    \n",
    "    def weight_histogram(self, nbins=1000, key=''):\n",
    "        \"\"\" return a weight distribution\n",
    "        \"\"\"\n",
    "        def doit(nbins):\n",
    "            return np.histogram(self.p_df.weight.values, np.linspace(0,1,nbins+1))[0]\n",
    "\n",
    "        key = f'{self.source_name}_weight_hist' if key=='' else key\n",
    "        description = f'Weight histogram for {self.source_name}' if self.config.verbose>0 else ''\n",
    "        return self.config.cache(key, doit, nbins, description=description)\n",
    "        \n",
    "    def plot_data(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        if self.simulated:\n",
    "            print(f'Simulated!')\n",
    "            fig, (ax1, ax4) = plt.subplots(1,2, figsize=(8,4))\n",
    "            ax1.hist(self.photons.time.values, 500, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');\n",
    "\n",
    "            \n",
    "        else:\n",
    "            fig, (ax1,ax2, ax3,ax4) = plt.subplots(1,4, figsize=(15,4))\n",
    "            ax1.hist(self.photons.time.values, 100, histtype='step');\n",
    "            ax1.set(xlabel='Time (MJD)')\n",
    "            ax2.hist(self.photons.radius.values**2, 100, histtype='step', log=True);\n",
    "            ax2.set(xlabel='Radius**2 (deg**2)', ylim=(100, None));\n",
    "\n",
    "            ax3.hist(self.photons.band, 32, histtype='step', log=True);\n",
    "            ax3.set(xlabel='Band index')\n",
    "            ax4.hist(self.photons.weight, 100, histtype='step')\n",
    "            ax4.set(xlabel='weight');\n",
    "            \n",
    "    def update_cache(self, **kwargs): #week_range=(-1,None), save=True):\n",
    "        return update_cache(self, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated 492 photons\n",
      "SourceData: Source test_sim with:\n",
      "\t data:           492 simulated photons over 1.0 days.\n",
      "\t exposure:       288 intervals,  average flux 3000 cm^2 for 0.1 Ms\n",
      "\t rates:  source 8.96e-07/s, background 1.00e-06/s, S/N ratio 8.94e-01\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "sim = Simulation('test_sim', src_flux=1e-6, tstart=0, tstop=1, )\n",
    "simsd = SourceData(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# config=Config(); config.verbose=2\n",
    "# source = PointSource('4FGL J1257.0-6339', nickname='J1257')\n",
    "# sd = SourceData(source, config=config, week_range=None, key=None)\n",
    "#sd = SourceData('Geminga');\n",
    "\n",
    "\n",
    "# b8 = sd.photons.query('band==12')\n",
    "# plt.semilogy(b8.radius, b8.weight, '.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-palestine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"SourceData\" class=\"doc_header\"><code>class</code> <code>SourceData</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>SourceData</code>(**`source`**, **`config`**=*`None`*, **`clear`**=*`False`*, **`week_range`**=*`None`*, **`key`**=*`''`*)\n",
       "\n",
       "Load the photon data near the source and associated exposure. \n",
       "\n",
       "Either from:\n",
       "  1. `config.wtlike_data/'data_files'`, the Path to folder with list of pickle files\n",
       "  2. the cache, with key `{source.name}_data`\n",
       "\n",
       "* source : name, PointSource, or Simulation\n",
       "* [`config`](/wtlikeconfig) : basic configuration\n",
       "* `source` : PointSource object if specified\n",
       "* `clear` : if set, overwrite the cached results\n",
       "\n",
       "Calculate the values for\n",
       "\n",
       "* S, B : sums of w and 1-w\n",
       "* exptot : total associated exposure"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SourceData.binned_exposure\" class=\"doc_header\"><code>SourceData.binned_exposure</code><a href=\"__main__.py#L119\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SourceData.binned_exposure</code>(**`time_edges`**)\n",
       "\n",
       "Bin the exposure\n",
       "\n",
       "- time_bins: list of edges.  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SourceData)\n",
    "show_doc(SourceData.binned_exposure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-better",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_data_man.ipynb.\n",
      "Converted 02_effective_area.ipynb.\n",
      "Converted 03_weights.ipynb.\n",
      "Converted 04_exposure.ipynb.\n",
      "Converted 04_simulation.ipynb.\n",
      "Converted 05_source_data.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_loglike.ipynb.\n",
      "Converted 08_cell_data.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 14_bayesian.ipynb.\n",
      "Converted 90_main.ipynb.\n",
      "Converted 99_tutorial.ipynb.\n",
      "Converted index.ipynb.\n",
      "Sun May 30 11:33:40 PDT 2021\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-pursuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
