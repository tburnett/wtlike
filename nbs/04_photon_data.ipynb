{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp photon_data\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photon Data\n",
    "\n",
    "> Load timed photon data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the photon data around a source, subject to a GTI.\n",
    "\n",
    "The function `get_photon_data` loads a data dataset of photons in a cone about a specified source, with time, position and energy info, the latter two binned.\n",
    "\n",
    "These are retrieved from a set of pickled dicts, one per week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os, sys\n",
    "import healpy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from wtlike.config import *\n",
    "from wtlike.load_gti import get_gti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _load_photon_data(config, table, tstart, \n",
    "                      conepix, gti, center, band_limits, radius, \n",
    "                      nest=True):\n",
    "    \"\"\"For a given month table, select photons in cone, add tstart to times,\n",
    "    return DataFrame with band, time, pixel, radius\n",
    "    \"\"\"\n",
    "    allpix = np.array(table.column('nest_index'))\n",
    "\n",
    "    def cone_select(allpix, conepix, shift=None):\n",
    "        \"\"\"Fast cone selection using NEST and shift\n",
    "        \"\"\"\n",
    "        if shift is None:\n",
    "            return np.isin(allpix, conepix)\n",
    "        assert nest, 'Expect pixels to use NEST indexing'\n",
    "        a = np.right_shift(allpix, shift)\n",
    "        c = np.unique(np.right_shift(conepix, shift))\n",
    "        return np.isin(a,c)\n",
    "\n",
    "    # a selection of all those in an outer cone\n",
    "    incone = cone_select(allpix, conepix, 13)\n",
    "\n",
    "    # times: convert to double, add to start, convert to MJD\n",
    "    time = MJD(np.array(table['time'],float)[incone]+tstart)\n",
    "    in_gti = gti(time)\n",
    "    if np.sum(in_gti)==0:\n",
    "        print(f'WARNING: no photons in GTI for month {month}!', file=sys.stderr)\n",
    "\n",
    "    pixincone = allpix[incone][in_gti]\n",
    "\n",
    "    # distance from center for all accepted photons\n",
    "    ll,bb = healpy.pix2ang(config.nside, pixincone,  nest=nest, lonlat=True)\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    t2 = np.degrees(np.array(np.sqrt((1.-np.dot(center, cart(ll,bb)))*2), np.float32))\n",
    "\n",
    "    # assemble the DataFrame, remove those outside the radius\n",
    "    out_df = pd.DataFrame(np.rec.fromarrays(\n",
    "        [np.array(table['band'])[incone][in_gti], time[in_gti], pixincone, t2],\n",
    "        names='band time pixel radius'.split()))\n",
    "\n",
    "    # apply final selection for radius and energy range\n",
    "\n",
    "    if band_limits is None: return out_df.query(f'radius<{radius}')\n",
    "\n",
    "    return out_df.query(f'radius<{radius} & {band_limits[0]} < band < {band_limits[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_photons(config, source, nest=True):\n",
    "    # check GTI\n",
    "    gti = get_gti(config)\n",
    "\n",
    "    # cone geometry stuff: get corresponding pixels and center vector\n",
    "    l,b,radius = source.l, source.b, config.radius\n",
    "    cart = lambda l,b: healpy.dir2vec(l,b, lonlat=True)\n",
    "    conepix = healpy.query_disc(config.nside, cart(l,b), np.radians(radius), nest=nest)\n",
    "    center = healpy.dir2vec(l,b, lonlat=True)\n",
    "\n",
    "    ebins = config.energy_edges\n",
    "    ecenters = np.sqrt(ebins[:-1]*ebins[1:]);\n",
    "    band_limits = 2*np.searchsorted(ecenters, config.energy_range) if config.energy_range is not None else None\n",
    "\n",
    "\n",
    "    # get the monthly-partitioned dataset and tstart values\n",
    "    datapath = config.files.data\n",
    "    dataset = datapath/'dataset'\n",
    "    tstart_dict= pickle.load(open(datapath/'tstart.pkl', 'rb'))\n",
    "    months = tstart_dict.keys()\n",
    "\n",
    "    if config.verbose>0:\n",
    "        print(f'Loading  {len(months)} months from Arrow dataset {dataset}\\n', end='')\n",
    "\n",
    "    dflist=[]\n",
    "    for month, tstart in tstart_dict.items(): #months:\n",
    "        table= pq.read_table(dataset, filters=[f'month == {month}'.split()])\n",
    "\n",
    "        d = _load_photon_data(config, table, tstart, \n",
    "                              conepix, gti, center, band_limits, radius,  \n",
    "                              nest)\n",
    "        if d is not None:\n",
    "            dflist.append(d)\n",
    "            if config.verbose>1: print('.', end='')\n",
    "        else:\n",
    "            if config.verbose>1: print('x', end='')\n",
    "            continue\n",
    "\n",
    "    assert len(dflist)>0, '\\nNo photon data found?'\n",
    "    df = pd.concat(dflist, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_photon_data(config: 'configuration data',\n",
    "                    source: 'Source data',\n",
    "                    key='',\n",
    "                    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    \n",
    "    - `source` -- `PointSource` object\n",
    "    - `key` [''] cache key -- default, use \"photons_name\", set to None to ignore cache \n",
    "    \n",
    "    Steps:\n",
    "    -  Read photon data from a Parquet dataset, \n",
    "    -  select cone around the source\n",
    "    -  use exposure to add exposures\n",
    "    -  return DataFrame with columns `band time pixel radius`\n",
    "    \"\"\"\n",
    "        \n",
    "    key = f'photons_{source.name}' if key=='' else key\n",
    "    \n",
    "    if config.verbose>0 and key is not None: \n",
    "        print(f'Photon data: {\"Saving to\" if key not in config.cache else \"Restoring from\"} cache with key \"{key}\"')\n",
    "\n",
    "    df = config.cache(key, _get_photons, config, source)\n",
    "    \n",
    "    if config.verbose>0:\n",
    "        emin,emax = config.energy_range or (config.energy_edges[0],config.energy_edges[-1])\n",
    "        print(f'\\n\\tSelected {len(df):,} photons within {config.radius}'\\\n",
    "              f' deg of  ({source.l:.2f},{source.b:.2f})')\n",
    "        print(f'\\tEnergies: {emin:.1f}-{emax:.0f} MeV')\n",
    "        ta,tb = df.iloc[0].time, df.iloc[-1].time\n",
    "        print(f'\\tDates:    {UTC(ta):16} - {UTC(tb)}'\n",
    "            f'\\n\\tMJD  :    {ta:<16.1f} - {tb:<16.1f}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_photon_data\" class=\"doc_header\"><code>get_photon_data</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_photon_data</code>(**`config`**:`configuration data`, **`source`**:`Source data`, **`key`**=*`''`*)\n",
       "\n",
       "Parameters:\n",
       "\n",
       "- `source` -- [`PointSource`](/wtlike/config.html#PointSource) object\n",
       "- `key` [''] cache key -- default, use \"photons_name\", set to None to ignore cache \n",
       "\n",
       "Steps:\n",
       "-  Read photon data from a Parquet dataset, \n",
       "-  select cone around the source\n",
       "-  use exposure to add exposures\n",
       "-  return DataFrame with columns `band time pixel radius`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_photon_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test reading from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loading a photon data set, for source Geminga\n",
      "Loading  132 months from Arrow dataset /home/burnett/data/dataset\n",
      "....................................................................................................................................\n",
      "\tSelected 1,313,726 photons within 5 deg of  (195.13,4.27)\n",
      "\tEnergies: 100.0-1000000 MeV\n",
      "\tDates:    2008-08-04 15:46 - 2019-08-03 01:17\n",
      "\tMJD  :    54682.7          - 58698.1         \n",
      "Head of the table, length 1,313,726:\n",
      "   band          time    pixel    radius\n",
      "0     6  54682.657022  6738278  0.698381\n",
      "1     3  54682.657934  6761152  2.498099\n",
      "2     4  54682.658637  6739138  0.290310\n",
      "3     1  54682.658760  6714890  3.276757\n",
      "4    11  54682.659099  6736721  4.899003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = Config(verbose=3)\n",
    "if config.valid:\n",
    "    source = PointSource('Geminga')\n",
    "    print(f'Test loading a photon data set, for source {source.name}')\n",
    "    photon_data = get_photon_data(config,  source, key=None)\n",
    "    print(f'Head of the table, length {len(photon_data):,}:\\n{photon_data.head()}')\n",
    "else:\n",
    "    print('Not testing since no files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet conversion code\n",
    "\n",
    "Copied here, needs integration. \n",
    "Don't know how to make more of those \"time_info\" filesm, but the data come from the binned FITS files. \n",
    "Need to modify this to read from the FITS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hide\n",
    "# #####################################################################################\n",
    "# #    Parquet code -- TODO just copied, need to test.\n",
    "# #           from notebooks/code development/parquet_writer.ipynb\n",
    "\n",
    "# class ParquetConversion(object):\n",
    "#     import glob, pickle;\n",
    "#     import healpy\n",
    "#     import pyarrow as pa\n",
    "#     import pyarrow.parquet as pq\n",
    "    \n",
    "#     def __init__(self, \n",
    "#                  data_file_pattern ='$FERMI/data/P8_P305/time_info/month_*.pkl',\n",
    "#             dataset = '/nfs/farm/g/glast/u/burnett/analysis/lat_timing/data/photon_dataset'):\n",
    "\n",
    "#         self.files = sorted(glob.glob(os.path.expandvars(data_file_pattern)));\n",
    "#         print(f'Found {len(self.files)} monthly files with pattern {data_file_pattern}'\\\n",
    "#              f'\\nWill store parquet files here: {dataset}')\n",
    "#         if os.path.exists(dataset):\n",
    "#             print(f'Dataset folder {dataset} exists')\n",
    "#         else:\n",
    "#             os.makedirs(dataset)\n",
    "#         self.dataset=dataset\n",
    "            \n",
    "#     def convert_all(self):\n",
    "#         files=self.files\n",
    "#         dataset=self.dataset\n",
    "#         nside=1024\n",
    "    \n",
    "#         def convert(month):\n",
    "\n",
    "#             infile = files[month-1]\n",
    "#             print(month, end=',')\n",
    "#             #print(f'Reading file {os.path.split(infile)[-1]} size {os.path.getsize(infile):,}' )   \n",
    "\n",
    "#             with open(infile, 'rb') as inp:\n",
    "#                 t = pickle.load(inp,encoding='latin1')\n",
    "\n",
    "#             # convert to DataFrame, add month index as new column for partition, then make a Table\n",
    "#             df = pd.DataFrame(t['timerec'])\n",
    "#             tstart = t['tstart']\n",
    "#             df['month']= np.uint8(month)\n",
    "#             # add a columb with nest indexing -- makes the ring redundant, may remove later\n",
    "#             df['nest_index'] = healpy.ring2nest(nside, df.hpindex).astype(np.int32)\n",
    "#             table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "#             # add to partitioned dataset\n",
    "#             pq.write_to_dataset(table, root_path=dataset, partition_cols=['month'] )\n",
    "\n",
    "#         for i in range(len(files)):\n",
    "#             convert(month=i+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_config.ipynb.\n",
      "Converted 01_effective_area.ipynb.\n",
      "Converted 02_gti.ipynb.\n",
      "Converted 03_exposure.ipynb.\n",
      "Converted 04_photon_data.ipynb.\n",
      "Converted 05_weights.ipynb.\n",
      "Converted 06_poisson.ipynb.\n",
      "Converted 07_cells.ipynb.\n",
      "Converted 08_loglike.ipynb.\n",
      "Converted 09_lightcurve.ipynb.\n",
      "Converted 10_simulation.ipynb.\n",
      "Sat Dec 26 14:37:51 PST 2020\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
