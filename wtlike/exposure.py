# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_exposure.ipynb (unless otherwise specified).

__all__ = ['get_default_bins', 'get_exposure', 'get_contiguous_exposure', 'get_binned_exposure']

# Cell
from astropy.io import fits
import numpy as np
import pandas as pd
from scipy.integrate import simps

from light_curves.config import *
from light_curves.load_gti import get_gti
from light_curves.effective_area import EffectiveArea

# Cell
def get_default_bins(config, exposure):
    """set up default bins from exposure; and config.mjd_range if set.

    adjust stop to come out even,    round to whole day
    """

    start = np.round(exposure.start.values[0])
    stop =  np.round(exposure.stop.values[-1])
    if config.mjd_range is None:
        config.mjd_range = (start,stop)

    step = config.time_interval
    nbins = int(round((stop-start)/step))
    tb =time_bins = np.linspace(start, stop, nbins+1)
    if config.verbose>0:
        print(f'Time bins: {nbins} intervals of {step} days, '\
              f'in range ({time_bins[0]:.1f}, {time_bins[-1]:.1f})')
    return time_bins

# Cell
def _process_ft2(config, source, ft2_file_path, gti, effective_area):
    """Process a set of FT2 files, with S/C history data
    Parameters:
        - config -- verbose, cos_theta_max, z_max
        - source -- l,b for position
        - ft2_files -- list of spacecraft files
        - gti -- GTI object with allowed intervals
        - effective_area -- function of energy and angle with respect to zenity
    Generate a dataset with fields:
        - start, stop : start  and stop times in MJD
        - exposure    : calculated exposure using effective area

     """
    # combine the files into a DataFrame with following fields besides START and STOP (lower case for column)
    fields    = ['LIVETIME','RA_SCZ','DEC_SCZ', 'RA_ZENITH','DEC_ZENITH']
    ft2_files = list(ft2_file_path.glob('*.fits'))
    if config.verbose>1:
        print(f'Processing {len(ft2_files)} S/C history (FT2) files')
        print(f'  applying cuts cos(theta) < {config.cos_theta_max},  z < {config.z_max}')
    sc_data=[]
    for filename in ft2_files:
        with fits.open(filename) as hdu:
            scdata = hdu['SC_DATA'].data
            # get times to check against MJD limits and GTI
            start, stop = [MJD(np.array(scdata.START, float)),
                           MJD(np.array(scdata.STOP, float))]
            if config.mjd_range is not None:
                a,b=  config.mjd_range
                if stop[-1]<a:
                    print(f'\tfile {filename}: skip, before selected range' )
                    continue
                elif start[0]>b:
                    print(f'\tfile {filename}: quit, beyond range')
                    break
            # apply GTI to bin center (avoid edge effects?)
            in_gti = gti(0.5*(start+stop))
            if config.verbose>2:
                print(f'\tfile {filename}: {len(start)} entries, {sum(in_gti)} in GTI')
            t = [('start', start[in_gti]), ('stop',stop[in_gti])]+\
                [(field.lower(), np.array(scdata[field][in_gti],np.float32)) for field in fields ]
            sc_data.append( pd.DataFrame(dict(t) ) )
    df = pd.concat(sc_data, ignore_index=True)

    # calculate cosines with respect to sky direction
    sc = source
    ra_r,dec_r = np.radians(sc.ra), np.radians(sc.dec)
    sdec, cdec = np.sin(dec_r), np.cos(dec_r)

    def cosines( ra2, dec2):
        ra2_r =  np.radians(ra2.values)
        dec2_r = np.radians(dec2.values)
        return np.cos(dec2_r)*cdec*np.cos(ra_r-ra2_r) + np.sin(dec2_r)*sdec

    pcosines = cosines(df.ra_scz,    df.dec_scz)
    zcosines = cosines(df.ra_zenith, df.dec_zenith)

    # mask out entries too close to zenith, or too far away from ROI center
    mask =   (pcosines >= config.cos_theta_max) & (zcosines>=np.cos(np.radians(config.z_max)))
    if config.verbose>1:
        print(f'\tFound {len(mask):,} S/C entries:  {sum(mask):,} remain after zenith and theta cuts')
    dfm = df.loc[mask,:]
    livetime = dfm.livetime.values
    config.dfm = dfm ##############debug
    # apply MJD range if present. note times in MJD
    start, stop = dfm.start,dfm.stop
    lims = slice(None)
    if config.mjd_range is not None:
#         a, b = config._get_limits(start)
        a, b = np.searchsorted(start, config.mjd_range)
        if a>0 or b<len(start):
            if config.verbose>1:
                print(f'\tcut from {len(start):,} to {a} - {b}, or {b-a:,} entries after MJD range selection')
            dfm = dfm.iloc[a:b]
            lims = slice(a,b)


    expose = _exposure(config, effective_area, livetime[lims], pcosines[mask][lims])
    return pd.DataFrame(dict(start=start[lims],stop=stop[lims], exposure=expose))

# Cell
def _exposure(config, effective_area, livetime, pcosine):
    """return exposure calculated for each pair in livetime and cosines arrays

    uses effective area
    """
    assert len(livetime)==len(pcosine), 'expect equal-length arrays'

    # get a set of energies and associated weights from a trial spectrum

    emin,emax = config.energy_range
    loge1=np.log10(emin); loge2=np.log10(emax)

    edom=np.logspace(loge1, loge2, int((loge2-loge1)*config.bins_per_decade+1))
    if config.verbose>1:
        print(f'Calculate exposure using the energy domain'\
              f' {emin}-{emax} {config.bins_per_decade} bins/decade' )
    base_spectrum = eval(config.base_spectrum) #lambda E: (E/1000)**-2.1
    assert base_spectrum(1000)==1.
    wts = base_spectrum(edom)

    # effectivee area function from
    ea = effective_area

    # a table of the weighted for each pair in livetime and pcosine arrays
    rvals = np.empty([len(wts),len(pcosine)])
    for i,(en,wt) in enumerate(zip(edom,wts)):
        faeff,baeff = ea([en],pcosine)
        rvals[i] = (faeff+baeff)*wt

    aeff = simps(rvals,edom,axis=0)/simps(wts,edom)
    return (aeff*livetime)

# Cell
def get_exposure(config,  source, gti_key='gti'):
    """Return the exposure for the source

    """





    def getit():
        files = config.files

        gti = get_gti(config, gti_key)
        aeff = EffectiveArea(file_path = files.aeff)
        exposure =  _process_ft2(config, source, files.ft2, gti, aeff)
        if config.verbose>1:
            print(f'{len(exposure)} entries, MJD {exposure.iloc[0].start:.0f}'
                  f' - {exposure.iloc[-1].stop:.0f}')
        return exposure

    key = f'exposure_{source.name}'
    description = f'exposure for {source.name}' if config.verbose>1 else ''
    return config.cache(key, getit, description=description)


# Cell
def get_contiguous_exposure(exposure, max_interval=10, verbose=0 ):
    """Combine exposure intervals from an exposure dataframe
    return array of (start, stop) pairs
    """

    t0s = exposure.start.values
    t1s = exposure.stop.values
    break_mask = (t0s[1:]-t1s[:-1])>max_interval/day
    break_starts = t1s[:-1][break_mask]
    break_stops = t0s[1:][break_mask]
    # now assumble the complement
    good_starts = np.empty(len(break_starts)+1)
    good_stops = np.empty_like(good_starts)
    good_starts[0] = t0s[0]
    good_starts[1:] = break_stops
    good_stops[-1] = t1s[-1]
    good_stops[:-1] = break_starts
    if verbose>1:
        print(f'Generate list of contiguous exposures:\n'\
              f'  WIth max interval {max_interval} s, combine {len(t0s):,} exposure entries to {len(good_stops):,} ')
    return np.array([good_starts, good_stops]).T

# Cell
def get_binned_exposure(config, source, time_bins=None, key='', gti_key='gti'):
    """

    Calculate integrated exposures per bin

    - `source` -- a PointSource object

    - `time_bins` -- a list of bins, or None<br> If None, generate the default

    - `key` -- use cache unless set to None.

    Return two arrays: a list of integrated exposures per bin, normalized to the average per bin,
    and the time_bin array.
    """

    def doit(time_bins):
        # get exposure calculation
        exposure = get_exposure(config, source, gti_key)
        exp   = exposure.exposure.values
        estart= exposure.start.values
        estop = exposure.stop.values

        #use cumulative exposure to integrate over larger periods
        cumexp = np.concatenate(([0],np.cumsum(exp)) )

        # default bins depends on exposure
        if time_bins is None:
            time_bins = get_default_bins(config, exposure)

        # get index into tstop array of the bin edges
        edge_index = np.searchsorted(estop, time_bins)
        # return the exposure integrated over the intervals
        cum = cumexp[edge_index]

        return np.diff(cum)/(cum[-1]-cum[0]) * (len(time_bins)-1) , time_bins

    key = f'binned_exposure_{source.name}' if key=='' else key
    description= f'binned exposure for source {source.name}'

    return config.cache(key, doit, time_bins, description=description)
