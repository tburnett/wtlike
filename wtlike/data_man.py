# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_data_man.ipynb (unless otherwise specified).

__all__ = ['GTI', 'get_ft1_data', 'get_ft2_info', 'FermiData', 'check_data', 'update_data', 'DataView']

# Cell
import os, sys, pickle
import dateutil, datetime
from astropy.io import fits
from ftplib import FTP_TLS as FTP

import healpy
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt

from .config import Config, Timer, UTC, MJD, mjd_range, mission_week

# Cell

class GTI(object):
    """ Define a functor that returns acceptability of times

    gti_file: a FITS file name with columns START,STOP, MET times in HDU[1]
    """
    def __init__(self,
                 gti_file:'Name of file containing a GTI',

                 config:'Config | None'=None):

        from astropy.io import fits

        config = config or Config()
        self.gti_file = gti_file
        filepath = gti_file if Path(gti_file).is_file() else Path(config.datapath)/gti_file
        assert filepath.is_file(), f'Input filename "{gti_file}" is not a file.'
        with  fits.open(filepath.absolute()) as hdus:
            gti_data = hdus['GTI'].data

        # make an interleaved (start,stop) array describing the "bad" intervals
        a,b = sorted(gti_data.START), sorted(gti_data.STOP)
        self.times = MJD(np.ravel(np.column_stack((a,b))))[1:-1] #remove first, last

        assert np.all(np.diff(self.times)>0), f'Bad input data from {gti_file} expect ascending time values.'

    def plot(self, ax=None, **kwargs):
        """ plot exclusion time and duration
        """
        fstart = self.times[0::2]
        delta = np.diff(self.times)[::2]
        fig, ax = plt.subplots(figsize=(8,2)) if ax is None else (ax.figure, ax)
        ax.semilogy(fstart, delta, '+');
        ax.grid(alpha=0.5)
        kw= dict(xlabel='MJD', xlim=(54500, 60000),ylabel='Duration (d)',
                 title=f'{self.gti_file}: {len(fstart)} exclusions')
        kw.update(kwargs)
        ax.set(**kw)

    def add(self, other):
        """Combine with another GTI.
        Require that there is no overlap of the excluded regions.
        """
        assert np.all(self(other.times)), 'Can only combine if no overlaps of excluded regions'
        self.times = np.sort(np.hstack([self.times, other.times]))
        self.gti_file += ('+ '+other.gti_file)

    def __repr__(self):
        return f'GTI {self.gti_file} has {len(self.times)//2}'\
                f' excluded intervals, total {np.sum(np.diff(self.times)[0::2]):.3f} d.'
    def __str__(self): return self.__repr__()

    def __call__(self, times:'array of MJD times'):
        """
        return array of bool for accepability of times
        """
        # searchsorted index must be even
        return np.searchsorted(self.times, times) & 1 ==0


# Cell
def get_ft1_data( config, ft1_file):

    """
    Read in a photon data (FT1) file, bin in energy and position to convert to a compact table

    - `ft1_file` -- A weekly file from GSFC

    Depends on config items
    - `theta_cut, z_cut` -- selection criteria
    - `ebins, etypes` -- define band index
    - `nside, nest` -- define HEALPix binning

    Returns a dict with keys

    - `tstart`, the start MET time from the FT1 header

    - `photons`: a dict with keys and entries for each selected photon
       - `band` (uint8):    energy band index*2 + 0,1 for Front/Back
       - `nest_index`  if nest else `ring_index` (uint32): HEALPIx index for the nside
       - `run_ref` (uint8) reference to the run number, in the array `runs`
       - `trun` (unit32): time since the run id in 2 $\\mu s$ units

    - `gti_times` -- GTI times as an interleaved start, stop array.
    - `runs` -- a list of the run numbers, each a MET time. Expect ~109 per week

    For the selected events above 100 MeV, this represents 10 bytes per photon, vs. 27 in the FT1 data
    """

    delta_t = config.offset_size
    ebins =   config.energy_edges
    etypes=   config.etypes
    nside =   config.nside
    nest  =   config.nest
    z_cut =   config.z_max
    theta_cut = np.degrees(np.arccos(config.cos_theta_max))
    verbose = config.verbose

    with  fits.open(ft1_file) as ft1:
        tstart = ft1[1].header['TSTART']

        ## GTI - setup raveled array function to make cut
        gti_data= ft1['GTI'].data
        # extract arrays for values of interest
        data =ft1['EVENTS'].data

    a,b = sorted(gti_data.START), sorted(gti_data.STOP)

    gti_times = np.ravel(np.column_stack((a,b)))
    if np.any(np.diff(gti_times)<0):
        raise Exception(f'Non-monatonic GTI found in file {ft1_file}')


    # apply  selections

    sel =  ((data['ENERGY'] > ebins[0]) &
            (data['ZENITH_ANGLE'] < z_cut) &
            (data['THETA'] < theta_cut))

    dsel = data[sel]

    # get the columns for output
    glon, glat, energy, et, z, theta, time, ec =\
         [dsel[x] for x in 'L B ENERGY EVENT_TYPE ZENITH_ANGLE THETA TIME EVENT_CLASS'.split()]

    # generate event_type masks
    et_mask={}
    for ie in etypes:
        et_mask[ie]= et[:,-1-ie]


    if verbose>2:
        total = sum(b)-sum(a)
        fraction = total/(b[-1]-a[0])

        print(  f'FT1: {ft1_file.name}, GTI range {a[0]:.1f}-{b[-1]:.1f}:  {len(data):,} photons'\
                f'\n\tSelection E > {ebins[0]:.0f} MeV. theta<{theta_cut:.1f} and z<{z_cut} remove:'\
                f' {100.- 100*len(dsel)/float(len(data)):.2f}%'
             )

    # pixelate direction
    hpindex = healpy.ang2pix(nside, glon, glat, nest=nest, lonlat=True).astype(np.uint32)
    hpname = 'nest_index' if nest else 'ring_index'

    # digitize energy and create band index incluing (front/back)
    band_index = (2*(np.digitize(energy, ebins, )-1) + et_mask[1]).astype(np.uint8)

    #
    run_id = dsel['RUN_ID'].astype(np.uint32)

    # save this for for run reference
    runlist = np.unique(run_id)

    # a dict with all photon info, which requires runlist
    runs = np.unique(run_id)
    photons = {'band' : band_index,
              hpname : hpindex,
              'run_ref': np.searchsorted(runs, run_id).astype(np.uint8),
              'trun'  : ((time-run_id)/delta_t).astype(np.uint32),
             }

    if verbose>1:
        print(f'FT1 data from file {ft1_file.name}: tstart={tstart:.0f} (UTC {UTC(MJD(tstart))[:-6]})'
              f' selected {len(dsel):,}/{len(data):,} photons, in {len(runlist)} runs.')

    return  dict(tstart=tstart,
                 photons=photons,
                 gti_times=gti_times,
                 runlist=runlist)

# Cell
def get_ft2_info(config, filename,
                 gti = lambda x: np.ones_like(x).astype(bool)):
    """Process a FT2 file, with S/C history data, and return a summary dict

    Parameters:

    * config -- verbose, cos_theta_max, z_max
    * filename -- spacecraft (FT2) file
    * gti -- GTI object that checkes for allowed intervals, in MJD units

    Returns: A dict with fields consistent with GTI if specified

    * start, stop -- interval in MJD units
    * livetime -- sec
    * ra_scz, dec_scz --spaceraft direction
    * ra_zenith, dec_zenith -- local zenith
    """
    # combine the files into a dict  with following fields besides START and STOP (lower case for column)
    fields    = ['LIVETIME','RA_SCZ','DEC_SCZ', 'RA_ZENITH','DEC_ZENITH']
    with fits.open(filename) as hdu:
        scdata = hdu['SC_DATA'].data
        tstart, tstop = [float(hdu[0].header[field]) for field in  ('TSTART','TSTOP') ]

    if config.verbose>1:
        print(f'FT2: {filename.name}, MET range {tstart:.1f}-{tstop:.1f},', end='')# {"not" if gti is None else ""} applying GTI')

    # get times to check against MJD limits and GTI
    start, stop = [MJD(np.array(scdata.START, float)),
                   MJD(np.array(scdata.STOP, float))]

    # apply GTI to bin center (avoid edge effects?)
    in_gti = np.array(gti(0.5*(start+stop)))
    if config.verbose>1:
        s = sum(in_gti)
        print(f' {len(start)} entries, {s} ({100*s/len(start):.1f}%) in GTI')

    t = [('start', start[in_gti]), ('stop',stop[in_gti])]+\
        [(field.lower(), np.array(scdata[field][in_gti],np.float32)) for field in fields ]

    sc_data = dict(t)
    return sc_data

# Internal Cell
def filepaths(week):
    """Returns: A tuple with two elements for the week number, each with two triplets with:
        ftp folder, ftp filename, local simple filename
    """
    urls = []
    for ftype, alias in  [('spacecraft','ft2'), ('photon','ft1')]:
         urls.append((
             f'{ftype}',
             f'lat_{ftype}_weekly_w{week:03d}_p{"305" if ftype=="photon" else "310" }_v001.fits',
             f'week{week:03d}_{alias}.fits',
            ))
    return urls

# Internal Cell
class GSFCweekly(dict):

    ftp_site = 'heasarc.gsfc.nasa.gov'
    ftp_path = 'fermi/data/lat/weekly'
    local_path  = '/tmp/from_gsfc'

    def __init__(self, config=None):
        """ Obtain lists of the weekly FT1 and FT2 files at GSFC, Set up as a dict, with
        keys= week numbers, values=mofification date strings
        """
        self.config = config or Config()
        # self.wtlike_data_file_path = Path(self.config.datapath/'data_files')
        # assert self.wtlike_data_file_path.is_dir(), 'Data path invalid'
        # os.makedirs(self.local_path, exist_ok=True)
        try:
            with FTP(self.ftp_site) as ftp:
                ftp.login()
                ftp.prot_p()
                ftp.cwd(self.ftp_path+'/photon') # or spacecraft
                # aet modification time and type for all files in folder

                parse_week = lambda fn:  int(fn.split('_')[3][1:])
                flist = ftp.mlsd(facts=['modify', 'type'])
                self.fileinfo = sorted([(parse_week(name), fact['modify']) for name,fact in flist
                                 if fact['type']=='file' and name.startswith('lat') ])
        except Exception as msg:
            print(f'FTP login to or download from {self.ftp_site} failed:\n\t--> {msg}',file=sys.stderr)
            self.valid=False
            return
        self.update(self.fileinfo)
        self.valid=True

    def download(self, week):
        """ Download the given week's files to the local folder

        week -- the mission week number, starting at 9. If negative, get recent one

        return the ft1, ft2 local filenames
        """

        if week<0:
            week = list(self.keys())[week]
        assert week in self, f'week {week} not found at FTP site'
        files = []
        with FTP(self.ftp_site) as ftp:
            ftp.login()
            ftp.prot_p()
            for ftp_folder, ftp_filename, local_filename in filepaths(week):
                ftp.cwd('/'+self.ftp_path+'/'+ftp_folder)
                if self.config.verbose>0:
                    print(f'GSFCweekly: {ftp_folder}/{ftp_filename} --> {local_filename}', flush=True)
                with open(f'{self.local_path}/{local_filename}', 'wb') as  localfile:
                    ftp.retrbinary('RETR ' + ftp_filename, localfile.write)
                files.append(local_filename)
        return files

    def week_number(self, met):
        return (met-233711940)//(7*24*3600)

    def load_week(self, week):
        """Load a pickled week summary """
        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'
        assert filename.exists(), f'File {filename} does not exist'
        with open(filename, 'rb') as imp:
            ret = pickle.load(imp)
        return ret

    def check_week(self, week):
        """Returns True if the local week needs updating"""
        data = self.load_week(week)
        if 'file_date' not in data:
            return True
        # check that file date agrees
        return data['file_date'] != self[week]

# Cell
class FermiData(GSFCweekly):
    """ Manage the full data set in weekly chunks
    * Checking the current set of files at GSFC
    * downloading a week at a time to a local tmp
    * Converting to condensed format and saving to pickled dicts in wtlike_data
    """

    def __init__(self, config=None):
        super().__init__(config)
        self.wtlike_data_file_path = Path(self.config.datapath/'data_files')
        assert self.wtlike_data_file_path.is_dir(), 'Data path is invalid'
        os.makedirs(self.local_path, exist_ok=True)

    @property
    def local_filedate(self):
        """ the datetime object representing the last file date in local storage"""
        from dateutil.parser import parse
        weekly_folder = self.config.datapath/'data_files'
        ff = sorted(list(weekly_folder.glob('*.pkl')))
        if len(ff)==0:
            print(f'FermiData: No weekly summary files found in {weekly_folder}', file=sys.stderr)
            return None

        wk = list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))
        lastweek = pickle.load(open(ff[-1],'rb'))
        return dateutil.parser.parse(lastweek['file_date'])

    @property
    def gsfc_filedate(self):
        return dateutil.parser.parse(list(self.values())[-1])

    def __str__(self):
        return f'FermiData: {len(self.fileinfo)} week files at GSFC, from {self.fileinfo[0]} to {self.fileinfo[-1]}'

    def in_temp(self):
        """return list of GSFC copied files in the local_path folder"""
        names = [f.name for f in Path(self.local_path).glob('*')]
        return names

    def __call__(self, week, test=False, tries_left=2):
        """ Download and convert the given week:
        * download FT1 and FT2 from GSFC to scratch space
        * convert each
        * save pickled dict summary
        * remove files

        """
        assert week in self, f'week {week} not found at FTP site'
        ff = filepaths(week)
        ft1_file = Path(self.local_path)/ff[1][2]
        ft2_file = Path(self.local_path)/ff[0][2]

        if self.config.verbose>1:
            print(f'FermiData: converting week {week}')

        while tries_left>0:
            try:
                if not (ft1_file.exists() and ft2_file.exists()):
                    self.download(week)
                # convert photon data to compact form, as a dice
                week_summary = get_ft1_data(self.config, ft1_file)
                break
            except Exception as e:
                print(f'*** ERROR *** Failed to convert {ft1_file}: {e} download it again)',
                      file=sys.stderr)
                tries_left -=1
                if tries_left==0:
                    print(f'Failed to convert week file {ft1_file}: quitting', file=sys.stderr)
                    return None
                else:
                    os.unlink(ft1_file)

        def apply_gti(time): # note MJD
            x = np.digitize(time, MJD(week_summary['gti_times']))
            return np.bitwise_and(x,1).astype(bool)
        sc_data = get_ft2_info(self.config, ft2_file, apply_gti)

        # finished with copies of FT1 and FT2 files: delete them
        for file in (ft1_file,ft2_file):
            os.unlink(file)

        # add file date and space craft summary
        week_summary['file_date'] = self[week]
        week_summary['sc_data'] = sc_data
        filename = self.wtlike_data_file_path/f'week_{week:03d}.pkl'

        if not test:
            with open(filename, 'wb') as out:
                pickle.dump(week_summary, out)

            if self.config.verbose>0:
                print(f'FermiData: {"replaced" if filename.exists() else "saved to" } {filename.name}', flush=True)
        else:
            print(f'testing... no change to {filename.name}')

    def download_and_convert(self, week_range,  processes=None):
        """Download FT1 and FT2 files from GSFC and create summary files for the weeks

        * week_range: a (first, last+1) tumple, or a iterable

        """
        from multiprocessing import Pool

        processes = processes or self.config.pool_size
        txt = f', using {processes} processes ' if processes>1 else ''

        if self.config.verbose>0:
            print(f'\tDownloading {len(week_range)} week files{txt}\n', end='', flush=True)

        if processes>1:
            with Pool(processes=processes) as pool:
                pool.map(self, week_range)
        else:
            list(map(self,  week_range))

    def needs_update(self):
        """ Compare files on disk with the GSFC list and compile list that need to be downloaded

        Check the file date of the last one on disk and update it as well if it has a different filedate
        """
        gg =self.wtlike_data_file_path.glob('*.pkl')
        file_weeks= map(lambda n: int(n.name[5:8]), gg)
        ondisk = np.array(list(file_weeks))

        missing =  set(self.keys()).difference(set(ondisk))
        if len(ondisk)==0:
            return list(missing)

        last = sorted(ondisk)[-1]
        if self.check_week(last):
             missing.add(last)
        return sorted(list(missing))

    def check_data(self):
        """
        Return: sorted list of summary files, last week number, number of days in last week"""
        config = self.config
        if config.valid:
            weekly_folder = config.datapath/'data_files'
            ff = sorted(list(weekly_folder.glob('*.pkl')))
            if len(ff)==0:
                print(f'No .pkl files found in {weekly_folder}', file=sys.stderr)
                return
            getwk = lambda f: int(os.path.splitext(f)[0][-3:])
            wk = [getwk(f) for f in ff] #list(map(lambda f: int(os.path.splitext(f)[0][-3:]), ff))
            lastweek = pickle.load(open(ff[-1],'rb'))

            file_date = lastweek['file_date']
            gti = lastweek['gti_times'];
            days = (gti[-1]-gti[0])/(24*3600)
            if config.verbose>0:
                print(f'Weekly folder "{weekly_folder}" contains {len(wk)} weeks.'\
                      f'\n\t Last week in local dataset, #{wk[-1]}, has {days:.3f} days, ends at UTC {UTC(MJD(gti[-1]))}, filedate {file_date}' )
            #return ff, wk[-1], days
        else:
            print(f'Config not valid, {config.errors}', file=sys.stderr)
            return None

    def update_data(self):
        """Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.
        If the last one is the current week, check to see if needs updating, by comparing file date, in days,
        from the last update with the current one at GSFC.
        """
        self.check_data()
        needs = self.needs_update()
        if len(needs)==0:
            print('--> Up to date!')
            return
        return self.download_and_convert(needs)

    def get_run_times(self, week):
        r = self.load_week(week)
        pdict = r['photons']
        if 'run_id' in pdict:
            runs = np.unique(pdict['run_id'])
        elif 'run_ref' in pdict:
            runs = r['runlist'][pdict['run_ref']]
        else:
            assert False
        return MJD(runs)

# Cell
def check_data(config=None, update=False):
    """
    Print current status, and update if requested
    """
    config = config or Config()
    if not config.valid:
        print('Config datapath is not valid', file=sys.stderr)
        return
    ff = FermiData(config)
    ff.check_data()
    print(f'Weeks needing download: {ff.needs_update()}')
    if update:
        ff.update_data()

def update_data( config=None):
    """Bring all of the local week data summaries up to date, downloading the missing ones from GSFC.
    If the last one is the current week, check to see if needs updating, by comparing file date, in days,
    from the last update with the current one at GSFC.
    """
    ff = FermiData(config)
    return ff.update_data()

# Internal Cell
def get_week_files(config, week_range=None):
    """Return list of week files

    - week_range [None] -- tuple with inclusive range. If None, get all
    """
    import pandas as pd
    data_folder = config.datapath/'data_files'
    data_files = sorted(list(data_folder.glob('*.pkl')))
    weeks = week_range or  config.week_range
    if week_range is not None:

        slc = slice(*week_range) if type(week_range)==tuple else slice(week_range,week_range)
        wk_table = pd.Series(data=[df for df in data_files],
                     index= [ int(df.name[-7:-4]) for df in  data_files],
                    )
        data_files = wk_table.loc[slc].values

        if config.verbose>1:
            q = lambda x: x if x is not None else ""
            print(f'LoadData: Loading weeks[{q(slc.start)}:{q(slc.stop)}:{q(slc.step)}]', end='' if config.verbose<2 else '\n')
    else:
        if config.verbose>1: print(f'LoadData: loading all {len(data_files)} weekly files')

    if len(data_files)==0:
        msg =  f'Specified week_range {week_range} produced no output. Note that week numbers are 9-'
        raise Exception(msg)

    return data_files

# Cell
class DataView(object):
    """
    Manage various views of the data set

    Constructor selects a time range

        - interval : a (start,stop) tuple (MJD). If (0,0), access all data
        - gti : GTI
        - nside [128] default HEALPix nside to use for maps
        - bmin [0] minimum energy band index

    """
    def __init__(self,  interval:tuple=(0,0),
                 config:'Config  | None'=None,
                 gti:'GTI  | None'=None,
                 nside:int=128,
                 bmin:int=0):
        """
        Constructor selects a time range

        - interval : a (start,stop) tuple (MJD). If (0,0), access all data
        - gti : external GTI object or None
        - nside [128] default HEALPix nside to use for maps
        - bmin [0] minimum energy band index

        """

        # convert interval to MJD, then week range
        self.config = config or Config()
        assert self.config.valid, f'Config not valid:\n{self.config.error_msg}'
        self.nside=nside
        self.bmin=bmin
        self.gti = gti

        # extract MJD range
        self.time_range = mjd_range(*interval)

        # get corresponding week range, and list of all week files
        start, stop = [mission_week(x) for x in self.time_range]
        self.week_files = get_week_files(self.config, (start, stop))

        # the perhaps compound GTI
        simple_cut = lambda time: (time>=self.time_range[0]) & (time<self.time_range[1])
        if gti is None:
            self.time_filter = simple_cut
        else:
            self.time_filter = lambda t: simple_cut(t) & gti(t)

    def __repr__(self):
        r= f'DataView({self.time_range}), {len(self)} weekly files)'
        if self.gti is not None:
            r += f'\n  with  {self.gti}'
        return r
    def __str__(self):
        return self.__repr__()

    def __len__(self):
        return len(self.week_files)

    def __getitem__(self, i):
        """ access to the week data"""
        file = self.week_files[i]
        with open(file, 'rb') as inp:
            return pickle.load(inp)

    def _photon_week(self, week_dict):
        """
        Return a dict with photon info for the week, with a 'time' field, in MJD reconstructed from
        the run_ref and trun fields. The latter removed.

        - week_dict -- standard photon info for the week
        - config -- used for config.offset_size

        """
        delta_t = self.config.offset_size
        phd = week_dict['photons'].copy()
        rl = week_dict['runlist']
        phd['time'] = MJD(phd['trun']* delta_t + rl[phd['run_ref']]) #- week_dict['tstart']
        phd.pop('run_ref')
        phd.pop('trun')
        return phd

    def count_map(self, nside=None, bmin=None): #, gti:'GTI object| None'=None):
        """ all-sky count map

        - nside [None] Set to override class.
            Project map to this value, from data's 1024
        - bmin [0] minimum band index (8 for 1 GeV cut)
        - gti [None] : if present, filter times

        Return a HEALPix counts map, in RING ordering
        """

        def get_map(file):

            with open(file, 'rb') as inp:
                u = pickle.load(inp)
            phd = self._photon_week( u)
            time = phd['time']

            # mask the photons based on time and/or band index
            mask = self.time_filter(time)
            if bmin>0:
                mask = mask & (phd['band']>=self.bmin)

            # extract the list of healpix indices
            ni = phd['nest_index'][mask]

            # to convert from data nside (1024 normally)
            shift = int(np.log2(self.config.nside//self.nside))
            n = 12*nside**2
            pmap,_ = np.histogram( np.right_shift(ni,2*shift), np.linspace(0,n,n+1))
            return pmap

        nside = nside or self.nside
        bmin = bmin or self.bmin
        pmap = get_map(self.week_files[0],)

        if len(self.week_files)>1:
            for week in self.week_files[1:]:
                pmap += get_map(week)
        # finally reorder
        return healpy.reorder(pmap, n2r=True)

    def livetime_map(self, nside=None,  sigma=0):
        """ all-sky pointing livetime map

        - nside [None]
        - sigma: Gaussian smooting parameter (degrees)

        Return a HEALPix map, RING ordering, of the integrated livetime per pixel
        """
        import pandas as pd
        from astropy.coordinates import SkyCoord
        nside = nside or self.nside
        N = 12*nside**2

        def get_ltmap(file):
            with open(file, 'rb') as inp:
                u =pickle.load(inp)
            sc_data = u['sc_data']
            mask = self.time_filter(sc_data['start'])
            #print('filter:', sum(filter), '/', len(filter))
            df = pd.DataFrame(sc_data)[mask]

            coords =  SkyCoord(df.ra_scz, df.dec_scz, unit='deg', frame='fk5').galactic
            indx= healpy.ang2pix(nside, coords.l.deg, coords.b.deg, lonlat=True)
            ltmap, _ = np.histogram( indx, bins=np.arange(N+1),  weights=df.livetime)
            return ltmap

        ltmap = get_ltmap(self.week_files[0],)
        if len(self.week_files)>1:
            for week in self.week_files[1:]:
                ltmap += get_ltmap(week)
        if sigma>0:
            ltmap = healpy.smoothing(ltmap, np.radians(sigma))
        return ltmap

    def exposure_map(self, beam_window, nside=None,):
        """ ### all-sky exposure map

        - beam_window --a list of coefficients of a Legendre polynomial expansion
        - nside [None]

        Return a HEALPix map of the weighted exposure
        """
        nside = nside or self.nside

        ltmap = self.livetime_map(nside=nside)

        # do a spherical convolution of the live-time map with a aeff beam window
        ### NEED TO ALLOW for BMIN!
        return healpy.alm2map(
                healpy.smoothalm(
                    healpy.map2alm(ltmap),
                    beam_window=beam_window,
                    ),
                nside=nside,
                )
    def flux_map(self, beam_window=None):
        """
        Return a flux map, the ratio of counts/exposure / sr

        where the counts are now divided by the pixel solid angle.
        So units are now: counts cm-2 s-1 sr-1
        """
        return self.count_map()/self.exposure_map(beam_window) * 12*self.nside**2/(4*np.pi)